{
  "hash": "057f88ce27f4c02f90117512220c5799",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Supervised learning: regularization\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n# Background\n\n## The big picture\n\nRegularized/shrinkage/penalized regression\n\n* Involves fitting a model containing all $p$ predictors\n\n. . .\n\n* However, the estimated coefficients are shrunken towards zero relative to the least squares estimates of vanilla linear regression\n\n. . .\n\n* This \"shrinkage\" (also known as regularization) has the effect of reducing variance\n\n. . .\n\n* Depending on what type of regularization is performed, some coefficient estimates can be exactly zero\n\n. . .\n\n* Thus, regularization methods can perform variable selection\n\n## Recap: linear regression\n\n__Linear regression__ estimates coefficients by minimizing\n\n $$\\text{RSS} = \\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\hat{\\beta}_j X_{ij}\\big)^2$$\n\n\n<center>\n\n<!-- ![](/images/rss.png){width=\"700\"} -->\n\n<img src= \"/images/rss.png\" style=\"border: 1px solid grey;\" width=\"600\">\n\n</center>\n\n## Shrinkage methods: ridge regression\n\n__Linear regression__ estimates coefficients by minimizing\n\n $$\\text{RSS} = \\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\hat{\\beta}_j X_{ij}\\big)^2$$\n\n. . .\n\n__Ridge regression__ introduces a __shrinkage penalty__ $\\lambda \\geq 0$ by minimizing\n\n$$\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j X_{ij}\\big)^2 + \\lambda \\sum_j^p \\beta_j^2 = \\text{RSS} + \\lambda \\sum_j^p \\beta_j^2$$\n\n. . .\n\n- as $\\lambda$ increases, flexibility of models decreases\n\n  - __increases bias, but decreases variance__\n \n\n. . .\n\n- for fixed value of $\\lambda$, ridge regression fits only a single model\n\n  - need to use cross-validation to __tune__ $\\lambda$\n\n\n\n## Shrinkage methods: ridge regression\n\nFor example: note how the magnitude of the coefficient for `Income` trends as $\\lambda \\rightarrow \\infty$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/Ridge.png){fig-align='center' width=20%}\n:::\n:::\n\n\n\nThe coefficient __shrinks towards zero__, but never actually reaches it\n\n- `Income` is always a variable in the learned model, regardless of the value of $\\lambda$\n\n\n## Shrinkage methods: lasso regression\n\nRidge regression __keeps all variables__, but we may believe there is a __sparse__ solution\n\n\n. . .\n\n__Lasso__ (\"Least Absolute Shrinkage and Selection Operator\") enables variable selection with $\\lambda$ by minimizing:\n\n$$\\sum_i^n \\big(Y_i - \\beta_0 -  \\sum_j^p \\beta_j X_{ij}\\big)^2 + \\lambda \\sum_j^p\\vert  \\beta_j \\vert = \\text{RSS} + \\lambda \\sum_j^p \\vert \\beta_j \\vert$$\n\n- Lasso uses an $\\ell_1$ (\"ell 1\") penalty\n\n\n. . .\n\n- as $\\lambda$ increases, flexibility of models decreases\n\n  - __increases bias, but decreases variance__\n  \n. . .\n\n- Can handle the $p > n$ case, i.e. more variables than observations!\n \n## Shrinkage methods: lasso regression\n\nLasso regression __performs variable selection__ yielding __sparse__ models (i.e. models that involve only a subset of the variables)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/Lasso.png){fig-align='center' width=40%}\n:::\n:::\n\n\n\nThe coefficient shrinks towards and __eventually equals zero__\n\n## Be lasso \n\n<center>\n\n![](/images/lasso-costume.jpeg){width=\"500\"}\n\n</center>\n\n\n## Lasso and ridge as optimization problems\n\nWhy does the lasso, unlike ridge, result in coefficient estimates that are exactly 0?\n\n* Lasso\n\n$$\n\\underset{\\beta}{\\text{minimize}} \\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\sum_{j=1}^p\\beta_j X_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^p|\\beta_j| \\le s\n$$\n\n* Ridge\n\n$$\n\\underset{\\beta}{\\text{minimize}} \\sum_{i=1}^n \\left(Y_i - \\beta_0 - \\sum_{j=1}^p\\beta_j X_{ij} \\right)^2 \\quad \\text{subject to} \\quad \\sum_{j=1}^p \\beta_j ^2 \\le s\n$$\n\n## Lasso and ridge in picture\n\n<center>\n\n![](/images/lasso-ridge.png){width=\"820\"}\n\n</center>\n\n## Best of both worlds: elastic net\n\n$$\\sum_{i}^{n}\\left(Y_{i}-\\beta_{0}-\\sum_{j=1}^{p} \\beta_{j} X_{i j}\\right)^{2}+\\lambda\\left(\\frac{1-\\alpha}{2}\\|\\beta\\|_{2}^{2}+\\alpha\\|\\beta\\|_{1} \\right)$$\n\n- $\\Vert \\beta \\Vert_1$: $\\ell_1$ norm: $\\Vert \\beta \\Vert_1 = \\sum_{j=1}^p \\vert \\beta_j \\vert$\n\n- $\\Vert \\beta \\Vert_2$: $\\ell_2$ (Euclidean) norm: $\\Vert \\beta \\Vert_2 = \\sqrt{\\sum_{j=1}^p \\beta_j^2}$\n\n. . .\n\n- Ridge penalty: $\\lambda \\cdot (1 - \\alpha) / 2$\n\n- Lasso penalty: $\\lambda \\cdot \\alpha$\n\n- $\\alpha$ controls the __mixing__ between the two types, ranges from 0 to 1\n\n  - $\\alpha = 1$ returns lasso, $\\alpha = 0$ return ridge\n\n- $\\lambda$ and $\\alpha$ are __tuning parameters__\n\n- Choose appropriate values based on out-of-sample performance/cross-validation\n\n- Use `cv.glmnet()` function in `glmnet` to perform cross-validation\n\n## Scaling of predictors\n\n- In linear regression, the (least squares) coefficient estimates are scale equivariant\n\n  -   multiplying $X_j$ by a constant c simply leads to a scaling of the least squares coefficient estimates by a\nfactor of $1/c$\n\n  -   i.e., regardless of how the $j$th predictor is scaled, $X_j \\hat \\beta_j$ will remain the same\n\n- For either ridge, lasso, or elastic net: __you should standardize your data__\n\n  -  The coefficient estimates can change substantially when multiplying a given predictor by a constant, due to the sum of squared coefficients term in the penalty\n\n- Common convention: for each predictor column, subtract off the mean, and divide by the standard deviation $\\qquad \\displaystyle \\tilde{x}_{ij} = \\frac{x_{ij} - \\bar{x}_j}{s_{x,j}}$\n\n- [`glmnet`](https://glmnet.stanford.edu/articles/glmnet.html) package does this by default and reports coefficients on the original scale\n\n# Examples\n\n## Data: prostate cancer\n\nExamine the level of a prostate specific antigen and a number of clinical measures in\nmen who were about to receive a radical prostatectomy\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\n# more info: https://rdrr.io/cran/ElemStatLearn/man/prostate.html\nprostate <- read_tsv(\"https://hastie.su.domains/ElemStatLearn/datasets/prostate.data\")\nglimpse(prostate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 97\nColumns: 11\n$ ...1    <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ lcavol  <dbl> -0.5798185, -0.9942523, -0.5108256, -1.2039728, 0.7514161, -1.…\n$ lweight <dbl> 2.769459, 3.319626, 2.691243, 3.282789, 3.432373, 3.228826, 3.…\n$ age     <dbl> 50, 58, 74, 58, 62, 50, 64, 58, 47, 63, 65, 63, 63, 67, 57, 66…\n$ lbph    <dbl> -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…\n$ svi     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ lcp     <dbl> -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1.3862944, -1…\n$ gleason <dbl> 6, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 6, 7, 6, 6, 6, 6,…\n$ pgg45   <dbl> 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 5, 5, 0, 30, 0, 0, 0,…\n$ lpsa    <dbl> -0.4307829, -0.1625189, -0.1625189, -0.1625189, 0.3715636, 0.7…\n$ train   <lgl> TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, FALSE, FALSE,…\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Introduction to `glmnet`\n\nFit ridge, lasso, and elastic net models with `glmnet`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(glmnet)\nprostate <- prostate |> \n  select(lcavol:lpsa)\n```\n:::\n\n\n\nCreate predictor matrix and response vector\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# predictors\n# model_x <- model.matrix(lpsa ~ ., prostate)\nmodel_x <- prostate |> \n  select(lcavol:pgg45) |> \n  as.matrix()\n\n# response\n# model_y <- prostate$lpsa\nmodel_y <- prostate |> \n  pull(lpsa)\n```\n:::\n\n\n\n## Vanilla linear regression model\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n- What do the initial regression coefficients look like?\n\n- Use [`broom`](https://broom.tidymodels.org/reference/tidy.cv.glmnet.html) to tidy model output for plotting\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprostate_lm <- lm(lpsa ~ ., data = prostate)\nlibrary(broom)\nprostate_lm |> \n  tidy() |> \n  mutate(term = fct_reorder(term, estimate)) |> \n  ggplot(aes(x = estimate, y = term, \n             fill = estimate > 0)) +\n  geom_col(color = \"white\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"darkred\", \"darkblue\"))\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Ridge regression\n\nPerform ridge regression using `glmnet` with `alpha = 0`\n\nBy default, predictors are standardized and models are fitted across a range of $\\lambda$ values\n  \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprostate_ridge <- glmnet(model_x, model_y, alpha = 0)\nplot(prostate_ridge, xvar = \"lambda\")\n```\n\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/init-ridge-ex-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n## Ridge regression\n\nUse cross-validation to select $\\lambda$ with `cv.glmnet()` which uses 10-folds by default\n\nSpecify ridge regression with `alpha = 0`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprostate_ridge_cv <- cv.glmnet(model_x, model_y, alpha = 0)\nplot(prostate_ridge_cv)\n```\n\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n## Tidy ridge regression\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# str(prostate_ridge_cv)\ntidy_ridge_coef <- tidy(prostate_ridge_cv$glmnet.fit)\ntidy_ridge_coef |> \n  ggplot(aes(x = lambda, y = estimate, group = term)) +\n  scale_x_log10() +\n  geom_line(alpha = 0.75) +\n  geom_vline(xintercept = prostate_ridge_cv$lambda.min) +\n  geom_vline(xintercept = prostate_ridge_cv$lambda.1se, \n             linetype = \"dashed\", color = \"red\")\n```\n:::\n\n\n\n- Could easily add color with legend for variables...\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n## Tidy ridge regression\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy_ridge_cv <- tidy(prostate_ridge_cv)\ntidy_ridge_cv |> \n  ggplot(aes(x = lambda, y = estimate)) +\n  geom_line() + \n  scale_x_log10() +\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2) +\n  geom_vline(xintercept = prostate_ridge_cv$lambda.min) +\n  geom_vline(xintercept = prostate_ridge_cv$lambda.1se,\n             linetype = \"dashed\", color = \"red\")\n```\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n## Ridge regression coefficients\n\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nCoefficients using the __one-standard-error rule__\n\n(select the model with estimated test error within one standard error of the minimum test error)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ridge_final <- glmnet(\n#   model_x, model_y, alpha = 0,\n#   lambda = prostate_ridge_cv$lambda.1se,\n# )\n# library(vip)\n# ridge_final |> \n#   vip()\n\ntidy_ridge_coef |>\n  filter(lambda == prostate_ridge_cv$lambda.1se) |>\n  mutate(term = fct_reorder(term, estimate)) |>\n  ggplot(aes(x = estimate, y = term, \n             fill = estimate > 0)) +\n  geom_col(color = \"white\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"darkred\", \"darkblue\"))\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n## Lasso regression example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nSimilar syntax to ridge but specify `alpha = 1`:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprostate_lasso_cv <- cv.glmnet(model_x, model_y, \n                               alpha = 1)\ntidy_lasso_coef <- tidy(prostate_lasso_cv$glmnet.fit)\ntidy_lasso_coef |> \n  ggplot(aes(x = lambda, y = estimate, group = term)) +\n  scale_x_log10() +\n  geom_line(alpha = 0.75) +\n  geom_vline(xintercept = prostate_lasso_cv$lambda.min) +\n  geom_vline(xintercept = prostate_lasso_cv$lambda.1se, \n             linetype = \"dashed\", color = \"red\")\n```\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n## Lasso regression example\n\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nNumber of non-zero predictors by $\\lambda$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy_lasso_cv <- tidy(prostate_lasso_cv)\ntidy_lasso_cv |>\n  ggplot(aes(x = lambda, y = nzero)) +\n  geom_line() +\n  geom_vline(xintercept = prostate_lasso_cv$lambda.min) +\n  geom_vline(xintercept = prostate_lasso_cv$lambda.1se, \n             linetype = \"dashed\", color = \"red\") +\n  scale_x_log10()\n```\n:::\n\n\n\nReduction in variables using __one-standard-error rule__\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n## Lasso regression example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nCoefficients using the __one-standard-error rule__\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# this will only print out non-zero coefficient estimates\n# tidy_lasso_coef |>\n#   filter(lambda == prostate_lasso_cv$lambda.1se)\n\nlasso_final <- glmnet(\n  model_x, model_y, \n  alpha = 1,\n  lambda = prostate_lasso_cv$lambda.1se,\n)\nlibrary(vip)\nlasso_final |> \n  vi() |> \n  mutate(Variable = fct_reorder(Variable, Importance)) |>\n  ggplot(aes(x = Importance, y = Variable, \n             fill = Importance > 0)) +\n  geom_col(color = \"white\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"darkred\", \"darkblue\")) +\n  labs(x = \"estimate\", y = NULL)\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Elastic net example\n\nNeed to tune both $\\lambda$ and $\\alpha$ - can do so manually with our own folds\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(100)\nfold_id <- sample(rep(1:10, length.out = nrow(model_x)))\n```\n:::\n\n\n\nThen use cross-validation with these folds for different candidate `alpha` values:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_enet_25 <- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 0.25)\ncv_enet_50 <- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 0.5)\ncv_ridge <- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 0)\ncv_lasso <- cv.glmnet(model_x, model_y, foldid = fold_id, alpha = 1)\n```\n:::\n\n\n\nCan see which one had the lowest CV error among its candidate $\\lambda$ values:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwhich.min(c(min(cv_enet_25$cvm), min(cv_enet_50$cvm), min(cv_ridge$cvm), min(cv_lasso$cvm)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n\n\n\n## Elastic net example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nCan view same type of summary\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_enet_50 |> \n  tidy() |> \n  ggplot(aes(x = lambda, y = nzero)) +\n  geom_line() +\n  geom_vline(xintercept = cv_enet_50$lambda.min) +\n  geom_vline(xintercept = cv_enet_50$lambda.1se, \n             linetype = \"dashed\", \n             color = \"red\") +\n  scale_x_log10()\n```\n:::\n\n\n\n- More relaxed than lasso for variable entry\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n## Comparison of models based on holdout performance\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(101)\nk <- 5\nprostate <- prostate |>\n  mutate(test_fold = sample(rep(1:k, length.out = n())))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_test_pred <- function(k) {\n  test_data <- prostate |> filter(test_fold == k)                     # get test and training data\n  train_data <- prostate |> filter(test_fold != k)\n  test_x <- as.matrix(select(test_data, -lpsa))                       # get test and training matrices\n  train_x <- as.matrix(select(train_data, -lpsa))\n  \n  lm_fit <- lm(lpsa ~ ., data = train_data)                           # fit models to training data\n  ridge_fit <- cv.glmnet(train_x, train_data$lpsa, alpha = 0)\n  lasso_fit <- cv.glmnet(train_x, train_data$lpsa, alpha = 1)\n  enet_fit <- cv.glmnet(train_x, train_data$lpsa, alpha = 0.5)\n  \n  tibble(lm_pred = predict(lm_fit, newdata = test_data),              # return test results\n         ridge_pred = as.numeric(predict(ridge_fit, newx = test_x)),\n         lasso_pred = as.numeric(predict(lasso_fit, newx = test_x)),\n         enet_pred = as.numeric(predict(enet_fit, newx = test_x)),\n         test_actual = test_data$lpsa,\n         test_fold = k)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_pred_all <- map(1:k, get_test_pred) |> \n  bind_rows()\n```\n:::\n\n\n\n## Comparison of models based on holdout performance\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nCompute RMSE across folds with standard error intervals\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_pred_all |>\n  pivot_longer(lm_pred:enet_pred, \n               names_to = \"type\", \n               values_to = \"test_pred\") |>\n  group_by(type, test_fold) |>\n  summarize(\n    rmse = sqrt(mean((test_actual - test_pred)^2))\n  ) |> \n  ggplot(aes(x = type, y = rmse)) + \n  geom_point(size = 4) +\n  stat_summary(fun = mean, geom = \"point\", \n               color = \"red\", size = 4) + \n  stat_summary(fun.data = mean_se, geom = \"errorbar\", \n               color = \"red\", width = 0.2)\n```\n:::\n\n\n\nLinear regression actually performs \"better\" than regularization, but within intervals\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](13-regularization_files/figure-revealjs/unnamed-chunk-23-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n:::\n",
    "supporting": [
      "13-regularization_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}