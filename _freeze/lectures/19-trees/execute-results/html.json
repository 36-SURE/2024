{
  "hash": "fa9baee7017a48a7135c445d77ee2faa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Supervised learning: decision trees\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io](https://36-sure.github.io)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n# Background\n\n## Tree-based methods\n\n* Can be applied to both regression and classification problems\n\n. . .\n\n* A single decision tree is simple and useful for interpretation\n\n    *   Main idea: stratify/segment the predictor space into a number of simple regions\n  \n    *   The set of splitting rules used to segment the predictor space can be summarized in a tree\n    \n. . .    \n\n* Bagging, random forests, and boosting: grow multiple trees which are then combined to yield a single consensus prediction\n\n* Combining a large number of trees can result in great improvements in prediction accuracy, at the expense of some loss interpretation\n\n## Predictor space\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/data_geometry.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n\n- Two predictor variables with binary response variable\n\n- __Left__: Linear boundaries that form rectangles will perform well in predicting response\n\n- __Right__: Circular boundaries will perform better\n\n## Regression trees\n\nExample: baseball salary\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Regression trees\n\nFit a **regression tree** to predict the salary of a baseball player using\n\n* Number of years they played in the major leagues\n\n* Number of hits they made in the previous year\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Regression trees\n\n* At each **node** the label (e.g., $X_j < t_k$ ) indicates that the _left_ branch that comes from that split. The _right_ branch is the opposite, e.g. $X_j \\geq t_k$.\n\n* The first **internal node** indicates that those to the left have less than 4.5 years in the major league, on the right have $\\geq$ 4.5 years.\n\n* The number on the _top_ of the **nodes** indicates the predicted Salary, for example before doing _any_ splitting, the average Salary for the whole dataset is 536 thousand dollars.\n\n* This tree has **two internal nodes** and **three termninal nodes**\n\n## Plotting a regression trees\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Regression trees: partitioning the feature space\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Regression trees: partitioning the feature space\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Regression trees: partitioning the feature space\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Terminology\n\n* The final regions (1), (2) and (3) are called **terminal nodes**\n\n* View the trees from _upside down_, the **leaves** are at the bottom\n\n* The splits are called **internal nodes**\n\n\n## Interpretation of results\n\n* `Years` is the most important factor in determining `Salary`\n\n  *   Players with less experience earn lower salaries\n  \n. . .  \n\n* Given that a player is less experienced, the number of `Hits` seems to play little role in the `Salary`\n\n. . .\n\n* Among players who have been in the major leagues for 4.5 years or more, the number of `Hits` made in the previous year **does** affect `Salary`\n\n  *   Players with more `Hits` tend to have higher salaries\n  \n. . .  \n\n* This is probably an oversimplification, but it is very easy to interpret\n\n## Decision tree: a more complex example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Decision tree structure\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://bradleyboehmke.github.io/HOML/images/decision-tree-terminology.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## Decision tree structure\n\nPredict the response value for an observation by __following its path along the tree__\n\n(See previous baseball salary example)\n\n. . .\n\n- Decision trees are __very easy to explain__ to non-statisticians.\n\n- Easy to visualize and thus easy to interpret __without assuming a parametric form__\n\n\n## Tree-building process: the big picture\n\n*   Divide the training data into distinct and non-overlapping regions\n\n    *   The regions are found __recursively using recursive binary splitting__  (i.e. asking a series of yes/no questions about the predictors)\n\n    *   Stop splitting the tree once a __stopping criteria__ has been reached (e.g. maximum depth allowed)\n\n. . .\n\n*   For a given region, make the same prediction for all observations in that region\n\n    *   Regression tree: __the average of the response values__ in the node\n\n    *   Classification tree: __the most popular class__ in the node\n\n. . .\n\n* Most popular algorithm: Classification and Regression Tree (CART)\n\n## Tree-building process: more details\n\n*   Divide the predictor space into high-dimensional rectangles (or boxes) - for simplicity and ease of interpretation\n\n. . .\n\n*   Goal: find regions $R_1, \\dots, R_J$ that minimize the RSS\n\n$$\n\\sum_{j=1}^J \\sum_{i \\in R_j} (y_i - \\hat y_{R_j})^2\n$$\n\n\nwhere $\\hat y_{R_j}$ is the mean response for the training observations within the $j$th region\n\n. . .\n\n*   Challenge: it is computationally infeasible to consider every possible partition of the feature space into $J$ regions\n\n## Tree-building process: more details\n\nSolution: recursive binary splitting (a top-down, greedy approach)\n\n. . .\n\n*   top-down \n\n    *   begin at the top of\nthe tree\n\n    *   then successively split the predictor space\n    \n    *   each split is indicated via two new branches further down\non the tree\n\n. . .\n\n*   greedy\n\n    *   at each step of the tree-building process, the best split is made at that particular step\n    \n    *   rather than looking ahead and picking a split that will lead to a better tree in some future step\n    \n## Tree-building process: more details\n\n*   First, select predictor $X_j$ and cutpoint $s$ such that\n\n    *   splitting the predictor space into the regions $\\{X \\mid X_j < s\\}$ and $\\{X \\mid X_j \\ge s\\}$ will yield the greatest possible reduction in RSS\n    \n. . .\n\n*   Next, repeat the process\n\n    *   looking for the best predictor and best cutpoint in order to split the data further\n    \n    *   so as to minimize the RSS within each of the resulting regions\n    \n    *   BUT...split one of the two previously identified region (instead of the entire predictor space)\n    \n. . .    \n    \n*   Continue the process until a stopping criterion is reached (i.e. how complex should the tree be?)\n\n    *   maximum tree depth\n    \n    *   minimum node size\n    \n## Pruning a tree\n\n*   The process described above may produce good training performance but poor test set performance (i.e. overfitting)\n\n. . .\n\n*   Solution: tree pruning\n\n    *   Grow a very large complicated tree\n    \n    *   Then prune back to an optimal subtree\n    \n. . .    \n    \n*   Tuning parameter: cost complexity parameter $\\alpha$\n\n    *   Minimize $$\\text{RSS} + \\alpha | T|$$ where $| T|$ is the number of terminal nodes of the tree $T$\n    \n    *   Controls a trade-off between the subtree’s complexity and its fit to the training data\n    \n    *   How do we select the optimal value?\n    \n## Pruning a tree\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://bradleyboehmke.github.io/HOML/07-decision-trees_files/figure-html/pruned-tree-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n## Pruning a tree\n\n<center>\n\n![](/images/prune.png){width=\"420\"}\n\n</center>\n\n## Classification trees\n\n*   Predict that each observation belongs to the most commonly occurring class in the region to which it belongs\n\n. . .\n\n*   Just like regression trees, use recursive binary splitting to grow a classification tree\n\n. . .\n\n*   Instead of RSS, use the Gini index $$G = \\sum_{k=1}^K \\hat p_{jk} (1 - \\hat p_{jk})$$ where $\\hat p_{jk}$ is proportion of observations in the $j$th region that are from the $k$th class\n\n    *   A measure of total variance across the $K$ classes\n    \n    *   A measure of node purity (or node impurity)\n    \n        *   small value: a node contains mostly observations from a single class\n\n# Examples\n\n## Predicting MLB home run probability\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nbatted_balls <- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/batted_balls.csv\")\nglimpse(batted_balls)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 19,852\nColumns: 12\n$ is_hr         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ launch_angle  <dbl> 14, 6, -31, 25, 5, 33, 42, 4, -10, 49, -22, -69, 45, 12,…\n$ launch_speed  <dbl> 73.9, 104.6, 87.7, 92.9, 90.7, 102.0, 85.7, 111.9, 102.1…\n$ bat_speed     <dbl> 69.25944, 69.95896, 75.73321, 73.28100, 67.64741, 73.959…\n$ swing_length  <dbl> 7.66206, 7.63497, 6.75095, 7.58858, 6.61407, 7.15180, 7.…\n$ plate_x       <dbl> -0.81, -0.11, -0.22, -0.30, -0.30, -0.28, 0.82, -0.37, 0…\n$ plate_z       <dbl> 1.96, 3.03, 2.23, 2.92, 2.49, 2.23, 2.44, 2.39, 1.51, 2.…\n$ inning        <dbl> 9, 9, 9, 9, 8, 8, 8, 8, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 5,…\n$ balls         <dbl> 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 1, 0, 1, 1, 0, 1, 0, 2,…\n$ strikes       <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 2, 1, 1, 1, 1, 1, 0, 0, 2, 0, 2,…\n$ is_stand_left <dbl> 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,…\n$ is_throw_left <dbl> 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,…\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(123)\ntrain <- batted_balls |> \n  slice_sample(prop = 0.5)\ntest <- batted_balls |> \n  anti_join(train)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n## Model training with `caret`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(caret)\nhr_tree <- train(as.factor(is_hr) ~ ., method = \"rpart\", tuneLength = 20,\n                 trControl = trainControl(method = \"cv\", number = 10),\n                 data = train)\n# str(hr_tree)\nggplot(hr_tree)\n```\n\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Display the final tree model\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\nhr_tree |> \n  pluck(\"finalModel\") |> \n  rpart.plot()\n```\n\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n\n## Evaluate predictions\n\n*   In-sample evaluation\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain |> \n  mutate(pred = predict(hr_tree, newdata = train)) |> \n  summarize(correct = mean(is_hr == pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  correct\n    <dbl>\n1   0.977\n```\n\n\n:::\n:::\n\n\n\n\n*   Out-of-sample evaluation\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest |> \n  mutate(pred = predict(hr_tree, newdata = test)) |> \n  summarize(correct = mean(is_hr == pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  correct\n    <dbl>\n1   0.972\n```\n\n\n:::\n:::\n\n\n\n\n## Variable importance\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(vip)\nhr_tree |> \n  vip()\n```\n\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Partial dependence plot\n\nPartial dependence of home run outcome on launch speed and launch angle (individually)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(pdp)\nhr_tree |> \n  partial(pred.var = \"launch_speed\", \n          which.class = 2, \n          prob = TRUE) |> \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_tree |> \n  partial(pred.var = \"launch_angle\", \n          which.class = 2, \n          prob = TRUE) |> \n  autoplot()\n```\n\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n## Partial dependence plot\n\nPartial dependence of home run outcome on launch speed and launch angle (jointly)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_tree |>\n  partial(pred.var = c(\"launch_speed\", \"launch_angle\"), which.class = 2, prob = TRUE) |>\n  autoplot(contour = TRUE)\n```\n\n::: {.cell-output-display}\n![](19-trees_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Appendix: code to build dataset\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsavant <- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/savant.csv\")\nbatted_balls <- savant |> \n  filter(type == \"X\") |> \n  mutate(is_hr = as.numeric(events == \"home_run\"),\n         is_stand_left = as.numeric(stand == \"L\"),\n         is_throw_left = as.numeric(p_throws == \"L\")) |> \n  filter(!is.na(launch_angle), !is.na(launch_speed), !is.na(is_hr)) |> \n  select(is_hr, launch_angle, launch_speed, bat_speed, swing_length, \n          plate_x, plate_z, inning, balls, strikes, is_stand_left, is_throw_left) |> \n  drop_na()\n```\n:::\n",
    "supporting": [
      "19-trees_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}