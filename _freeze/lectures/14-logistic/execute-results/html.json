{
  "hash": "4f624ba269c748cc527f86139e77a358",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Supervised learning: logistic regression\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n# Background\n\n## Classification\n\n* Categorical variables take values in an unordered set with different categories\n\n* Classification: predicting a categorical response $Y$ for an observation (since it involves assigning the observation to a category/class)\n\n* Often, we are more interested in estimating the probability for each category of the response $Y$\n\n* Classification is different from clustering, since we know the true label $Y$\n\n* We will focus on binary categorical response (categorical response variables with only two categories)\n\n    *   Examples: whether it will rain or not tomorrow, whether the kicker makes or misses a field goal attempt\n\n## Logistic regression\n\n* Review: in linear regression, the response variable $Y$ is continuous\n\n* Logistic regression is used to model binary outcomes (i.e. $Y \\in \\{0, 1\\}$)\n\n* Similar concepts from linear regression\n\n  *   tests for coefficients\n  *   indicator variables\n  *   model selection techniques\n  *   regularization\n\n* Other concepts are different\n  \n  *   interpretation of coefficients\n  *   residuals are not normally distributed\n  *   model evaluation\n\n## Why not linear regression for categorical data?\n\nFrom ISLR:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/Figure_4.2.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n. . .\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nLinear regression\n\npredicted probabilities might be outside the $[0, 1]$ interval\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nLogistic regression\n\nall probabilities lie between 0 and 1\n\n:::\n:::\n\n\n## Logistic regression\n\n* Logistic regression models the probability of success $p(x)$ of a binary response variable $Y \\in \\{0, 1\\}$\n\n* To limit the regression between $[0, 1]$, we use the __logit__ function, or the __log-odds ratio__\n\n$$\n\\log \\left( \\frac{p(x)}{1 - p(x)} \\right) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n$$\n\n. . .\n\nor equivalently,\n\n$$\np(x) =  \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}\n$$\n\n. . .\n\n\n*   Note: $p(x)= P(Y = 1 \\mid X = x) = \\mathbb E[Y \\mid X = x]$\n\n\n## Maximum likelihood estimation (MLE)\n\n*   Likelihood function: how likely to observe data for given parameter values $$L(\\theta) = \\prod_{i=1}^n f(y_i; \\theta)$$\n\n. . .\n\n*   Goal: find values for parameters that maximize likelihood function (i.e., MLEs)\n\n. . .\n\n*   Why: likelihood methods can handle variety of data types and assumptions\n\n. . .\n\n*   How: typically numerical approaches and optimization techniques\n\n    *   Easier to work with log-likelihood (sums compared to products)\n    \n## Example: linear regression MLE\n\nReminder: probability density function (pdf) for a normal distribution $$f(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{\\frac{-(y - \\mu) ^ 2}{2 \\sigma^2}}$$\n\n. . .\n\nConsider a simple linear regression model (with an intercept and one predictor) $$y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)$$\n\n. . .\n\nThen $\\qquad \\displaystyle L(\\beta_0, \\beta_1, \\sigma) = \\prod_{i=1}^n f(y_i \\mid x_i; \\beta_0, \\beta_1, \\sigma) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{\\frac{-(y_i - \\beta_0 - \\beta_1 x_i) ^ 2}{2 \\sigma^2}}$\n\n. . .\n\nAnd $\\qquad \\displaystyle \\log L(\\beta_0, \\beta_1, \\sigma) =  -\\frac{n}{2} \\log 2\\pi - \\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1x_1)^2$\n\n. . .\n\nWe can then use calculus to find the optimal parameter values. For more details, see [here](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf)\n\n## Major difference between linear and logistic regression\n\nLogistic regression __involves numerical optimization__\n\n- $y_i$ is observed response for $n$ observations, either 0 or 1\n\n\n. . .\n\n- We need to use an iterative algorithm to find $\\beta$'s that maximize the __likelihood__ $$\\prod_{i=1}^{n} p\\left(x_{i}\\right)^{y_{i}}\\left(1-p\\left(x_{i}\\right)\\right)^{1-y_{i}}$$\n\n\n. . .\n\n- __Newton's method__: start with initial guess, calculate gradient of log-likelihood, add amount proportional to the gradient to parameters, moving up log-likelihood surface\n\n\n. . .\n\n- This means logistic regression runs more slowly than linear regression\n\n. . .\n\n- if you're interested: iteratively re-weighted least squares, see [here](http://www.stat.cmu.edu/~cshalizi/uADA/15/lectures/12.pdf)\n\n\n\n## Inference with logistic regression\n\n* __Major motivation__ for logistic regression (and all GLMs) is __inference__\n\n    *   How does the response change when we change a predictor by one unit?\n\n. . .\n\n* For linear regression, the answer is straightforward \n\n$$\\mathbb{E}[Y \\mid x] = \\beta_0 + \\beta_1 x_1 + + \\cdots + \\beta_p x_p$$\n\n. . .\n\n* For logistic regression, it is a little _less_ straightforward \n\n$$\\mathbb E[Y \\mid x] = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}$$\n\nNote: \n\n* The predicted response varies __non-linearly__ with the predictor variable values\n\n*   Interpretation on the __odds__ scale\n\n\n\n\n## The odds interpretation\n\nThe odds of an event are the probability of the event divided by the probability of the event not occurring\n\n* $\\displaystyle \\text{odds} = \\frac{\\text{probability}}{1 - \\text{probability}}$\n\n* $\\displaystyle \\text{probability} = \\frac{\\text{odds}}{1 + \\text{odds}}$\n\nExample: Rolling a fair six-sided die\n\n*   The probability of rolling a 2 is 1/6\n\n*   The odds of rolling a 2 is 1 to 5 (or 1:5)\n\n## The odds interpretation\n\nSuppose we fit a simple logistic regression model (with one predictor), and say the predicted probability is 0.8 for a particular predictor value\n\n*   This means that if we were to repeatedly sample response values given that predictor variable value: __we expect class 1 to appear 4 times as often as class 0__ $$\\text{odds} = \\frac{\\mathbb{E}[Y \\mid x]}{1-\\mathbb{E}[Y \\mid x]} = \\frac{0.8}{1-0.8} = 4 = e^{\\beta_0+\\beta_1x}$$\n\n*   Thus we say that for the given predictor variable value, the odds are 4 (or 4:1) in favor of class 1\n\n. . .\n\nHow does the odds change if we change the value of a predictor variable by one unit?\n\n. . .\n\n$$\\text{odds}_{\\rm new} = e^{\\beta_0+\\beta_1(x+1)} = e^{\\beta_0+\\beta_1x}e^{\\beta_1} = e^{\\beta_1}\\text{odds}_{\\rm old}$$\n\nFor every unit change in $x$, the odds change by a __factor__ $e^{\\beta_1}$\n\n# Example\n\n\n## Data: diabetes in the Pima\n\n* Each observation represents one Pima woman, at least 21 years old, who was tested for diabetes as part of the study. \n\n* `type`: if they had diabetes according to the diagnostic criteria\n\n* `npreg`: number of pregnancies\n\n* `bp`: blood pressure\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\npima <- as_tibble(MASS::Pima.tr)\npima <- pima |> \n  mutate(pregnancy = ifelse(npreg > 0, \"Yes\", \"No\")) # whether the patient has had any pregnancies\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n## Fitting a logistic regression model\n\n- Use the `glm` function, but specify the family is `binomial`\n- Get coefficients table using the `tidy()` function in the `broom` package\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npima_logit <- glm(type ~ pregnancy + bp, data = pima, family = binomial)\nlibrary(broom)\n# summary(pima_logit)\ntidy(pima_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   -3.16      1.08       -2.94 0.00329\n2 pregnancyYes  -0.468     0.427      -1.10 0.273  \n3 bp             0.0403    0.0140      2.89 0.00391\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n- Eponentiate the coefficient estimates (log-odds) to back-transform and get the odds\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(pima_logit, conf.int = TRUE, exponentiate = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 7\n  term         estimate std.error statistic p.value conf.low conf.high\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1 (Intercept)    0.0422    1.08       -2.94 0.00329  0.00472     0.329\n2 pregnancyYes   0.626     0.427      -1.10 0.273    0.272       1.47 \n3 bp             1.04      0.0140      2.89 0.00391  1.01        1.07 \n```\n\n\n:::\n:::\n\n\n\n\n\n## Interpreting a logistic regression model\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(pima_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   -3.16      1.08       -2.94 0.00329\n2 pregnancyYes  -0.468     0.427      -1.10 0.273  \n3 bp             0.0403    0.0140      2.89 0.00391\n```\n\n\n:::\n:::\n\n\n\n\n* Intercept: the estimated log-odds of diabetes for a Pima woman who has never been pregnant and has a diastolic blood pressure of 0 (hmm...) are -3.16, so the odds are exp(-3.16) = 0.0422\n    \n## Interpreting a logistic regression model\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(pima_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   -3.16      1.08       -2.94 0.00329\n2 pregnancyYes  -0.468     0.427      -1.10 0.273  \n3 bp             0.0403    0.0140      2.89 0.00391\n```\n\n\n:::\n:::\n\n\n\n\n* Pregnancy: pregnancy changes the log-odds of diabetes, relative to the baseline value, by -0.468. The odds ratio is exp(-0.47) = 0.626\n\n\n## Interpreting a logistic regression model\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(pima_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)   -3.16      1.08       -2.94 0.00329\n2 pregnancyYes  -0.468     0.427      -1.10 0.273  \n3 bp             0.0403    0.0140      2.89 0.00391\n```\n\n\n:::\n:::\n\n\n\n\n*   Blood pressure: each unit of increase in blood pressure (in mmHg) is associated with an increase in the log-odds of diabetes of 0.0403, or an increase in the odds of diabetes by a multiplicative factor of exp(0.0403) = 1.04\n\n    *   Every extra mmHg in blood pressure multiplies the odds of diabetes by 1.04\n\n## View predicted probability relationship\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npima |> \n  mutate(pred_prob = fitted(pima_logit),\n         i_type = as.numeric(type == \"Yes\")) |> \n  ggplot(aes(bp)) +\n  geom_line(aes(y = pred_prob, color = pregnancy), linewidth = 2) +\n  geom_point(aes(y = i_type), alpha = 0.3, color = \"darkorange\", size = 4)\n```\n\n::: {.cell-output-display}\n![](14-logistic_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Effects plot\n\n* Nonlinear relationship between blood pressure and probability of diabetes\n\n* The gap between women who were or were not previously pregnant is different across the range\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggeffects)\npima_logit |> \n  ggeffect(terms = c(\"bp\", \"pregnancy\")) |>\n  plot()\n```\n\n::: {.cell-output-display}\n![](14-logistic_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## [Deviance](https://en.wikipedia.org/wiki/Deviance_(statistics))\n\nFor model of interest $\\mathcal{M}$ the total deviance is\n\n$$D_{\\mathcal{M}}= -2 \\log \\frac{\\mathcal{L}_{\\mathcal{M}}}{\\mathcal{L}_{\\mathcal{S}}} = 2\\left(\\log  \\mathcal{L}_{\\mathcal{S}}-\\log  \\mathcal{L}_{\\mathcal{M}}\\right)$$\n\n- $\\mathcal{L}_{\\mathcal{M}}$ is the likelihood for model $\\mathcal{M}$\n\n. . .\n\n- $\\mathcal{L}_{\\mathcal{S}}$ is the likelihood for the __saturated__ model, with $n$ parameters \n\n  -   i.e. the most complex model possible, with separate parameter for each observation and provides a perfect fit to the data\n\n. . .\n\n__Deviance is a measure of goodness-of-fit__: the smaller the deviance, the better the fit\n\n- Generalization of RSS in linear regression to any distribution family\n\nFor more, see [here](https://bookdown.org/egarpor/PM-UC3M/glm-deviance.html)\n\n## Deviance residuals\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npima_logit |> \n  residuals(type = \"deviance\") |> \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-1.5293 -0.9156 -0.7463 -0.0990  1.3205  1.9326 \n```\n\n\n:::\n:::\n\n\n\n\n* Remember Pearson residual?\n\n* A deviance residual is an alternative residual for logistic regression based on the discrepancy between the observed values and those estimated using the likelihood $$d_i = \\mbox{sign}(y_i-\\hat{p}_i) \\sqrt{-2[y_i \\log \\hat{p}_i + (1-y_i) \\log (1 - \\hat{p}_i)]}$$ where $y_i$ is the $i$-th observed response and $\\hat p_i$ is the estimated probability of success\n\nNote: The use of the sign of the difference ensures that the deviance residuals are positive/negative when the observation is larger/smaller than predicted.\n\n* The sum of the individual deviance residuals is the total deviance\n\n## Model measures\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(pima_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n1          256.     199  -123.  252.  262.     246.         197   200\n```\n\n\n:::\n:::\n\n\n\n\n* `deviance` = `2 * logLik`\n\n* `null.deviance` corresponds to intercept-only model\n\n* `AIC` = `2 * df.residual - 2 * logLik`\n\n* We will consider these to be less important than out-of-sample/test set performances\n\n## Predictions\n\n- The default output of `predict()` is on __the log-odds scale__\n\n- To get predicted probabilities, specify `type = \"response`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npima_pred_prob <- predict(pima_logit, type = \"response\")\n```\n:::\n\n\n\n\n- How do we predict the class (diabetes or not)?\n\n- Typically if predicted probability is > 0.5 then we predict `Yes`, else `No`\n\n  -   Could also be encoded as `1` and `0`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npima_pred_class <- ifelse(pima_pred_prob > 0.5, \"Yes\", \"No\")\npima_pred_binary <- ifelse(pima_pred_prob > 0.5, 1, 0)\n```\n:::\n\n\n\n\n## Model assessment\n\n* Confusion matrix\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(\"Predicted\" = pima_pred_class, \"Observed\" = pima$type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Observed\nPredicted  No Yes\n      No  125  60\n      Yes   7   8\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n* In-sample misclassification rate\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean(pima_pred_class != pima$type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.335\n```\n\n\n:::\n:::\n\n\n\n\n. . .\n\n* [Brier score](https://en.wikipedia.org/wiki/Brier_score)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmean((pima_pred_binary - pima_pred_prob)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1140525\n```\n\n\n:::\n:::\n\n\n\n\n## Calibration\n\nIf we have a model that tells us the probability of rain in a given time period (i.e. a week) is 50%, it had better rain on half of the days, or the model is just wrong about the probability of rain\n\n. . .\n\n* A classifier is calibrated if in the given time period, the percentage of days it actually rained when the forecast predicted $x \\%$ percent chance of rain is $x \\%$\n\n* In short, for a model to be calibrated, the actual probabilities should match the predicted probabilities\n\n. . .\n\n* Calibration should be used together with other model evaluation and diagnostic tools\n\n. . .\n\n* A model is well-calibrated does not mean it is good at making predictions (example: a constant classifier)\n\n. . .\n\nSee also: [Hosmer-Lemeshow test](https://en.wikipedia.org/wiki/Hosmer-Lemeshow_test)\n\n## Calibration plot: smoothing\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nPlot the observed outcome against the fitted probability, and apply a smoother\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npima |> \n  mutate(pred = predict(pima_logit, type = \"response\"),\n         obs = ifelse(type == \"Yes\", 1, 0)) |> \n  ggplot(aes(pred, obs)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  geom_abline(slope = 1, intercept = 0, \n              linetype = \"dashed\") \n```\n:::\n\n\n\n\nLooks good for the most part\n\n*   odd behavior for high probability values\n\n*   since there are only a few observations with high predicted probability\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-logistic_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n:::\n\n## Calibration plot: binning\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npima |> \n  mutate(\n    pred_prob = predict(pima_logit, type = \"response\"),\n    bin_pred_prob = cut(pred_prob, breaks = seq(0, 1, .1))\n  ) |> \n  group_by(bin_pred_prob) |> \n  summarize(n = n(),\n            bin_actual_prob = mean(type == \"Yes\")) |> \n  mutate(mid_point = seq(0.15, 0.65, 0.1)) |> \n  ggplot(aes(x = mid_point, y = bin_actual_prob)) +\n  geom_point(aes(size = n)) +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, \n              color = \"black\", linetype = \"dashed\") +\n  # expand_limits(x = c(0, 1), y = c(0, 1)) +\n  coord_fixed()\n```\n:::\n\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-logistic_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n:::\n\n## Other calibration methods\n\n*   [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling)\n\n*   [Isotonic regression](https://en.wikipedia.org/wiki/Isotonic_regression)\n\n*   [Beta calibration](https://doi.org/10.1214/17-EJS1338SI)\n\n*   [`probably`](https://www.tidyverse.org/blog/2022/11/model-calibration/) package in R\n\n*   Formally, a classifier is calibrated (or well-calibrated) when $P(Y = 1 \\mid\\hat p(X) = p) = p$\n",
    "supporting": [
      "14-logistic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}