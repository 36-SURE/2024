{
  "hash": "b9792f6ef955cd7ed81f8b5449c112d8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Supervised learning: nonparametric regression\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n# Background\n\n## Model flexibility vs interpretability\n\nFrom [ISLR](https://www.statlearning.com/)\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/flexibility.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n## Model flexibility vs interpretability\n\n- __Tradeoff__ between model's **flexibility** (i.e. how wiggly/curvy it is) and its **interpretability**\n\n- The simpler, parametric form of the model, the the easier it is to interpret\n\n. . .\n\n- __Parametric__ models, for which we can write down a mathematical expression for $f(X)$ __before observing the data__, _a priori_ (e.g. linear regression), __are inherently less flexible__\n\n- __Nonparametric__ models, in which $f(X)$ is __estimated from the data__, are more flexible, but harder to interpret\n\n## $k$-nearest neighbors ($k$-NN)\n\n- Find the $k$ data points __closest__ to an observation $x$, use these to make predictions\n\n  - Need to use some measure of distance, e.g., Euclidean distance\n  \n. . .  \n\n- Take the average value of the response over the $k$ nearest neighbors\n\n  -   $k$-NN classification: most common class among the $k$ nearest neighbors (\"majority vote\")\n  \n  -   $k$-NN regression: average of the values of $k$ nearest neighbors\n\n. . .\n\n- __The number of neighbors $k$ is a tuning parameter__ (like $\\lambda$ is for ridge/lasso)\n\n## Finding the optimal number of neighbors $k$\n\nRecall: **bias-variance tradeoff**\n\n. . .\n\n- If $k$ is too small, the resulting model is too flexible: low bias, high variance\n\n. . .\n\n- If $k$ is too large, the resulting model is *not flexible enough*: high bias, low variance\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/Fig_2.16.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n## Moving beyond linearity \n\n*   The truth is (almost) never linear\n\n. . .\n\n*   But often the linearity assumption is good enough\n\n. . .\n\n*   What if it's not linear?\n\n    *   splines\n    \n    *   local regression\n    \n    *   generalized additive models\n    \n. . .\n    \n*   The above methods can offer a lot of flexibility, without losing the ease and interpretability of linear models\n\n\n## Generalized additive models (GAMs)\n\n*   Previously: generalized linear models (GLMs) (e.g., linear regression, logistic regression, Poisson regression)\n\n. . .\n\n*   GLM generalizes linear regression to permit non-normal distributions and link functions of the mean $$g(E(Y)) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p$$\n\n. . .\n\n*   What if we replace the linear predictor by additive smooth functions of the explanatory variables?\n\n*   Entering generalized additive models (GAMs) \n\n    *   relax the restriction that the relationship must be a simple weighted sum\n    \n    *   instead assume that the response can be modeled by a sum of arbitrary functions of each predictor\n\n## Generalized additive models (GAMs)\n\n*   Allows for flexible nonlinearities in several variables\n\n*   But retains the additive structure of linear models\n\n. . .\n\n$$g(E(Y)) = \\beta_0 + s_1(x_1) + s_2(x_2) + \\dots + s_p(x_p)$$\n\n*   Like GLMs, a GAM specifies a link function $g()$ and a probability distribution for the response $Y$\n\n*   $s_j$ is some smooth function of predictor $j$\n\n*   GLM is the special case of GAM in which each $s_j$ is a linear function\n\n## Generalized additive models (GAMs)\n\n*   Relationships between individual predictors and the response are smooth\n\n. . .\n\n*   Estimate the smooth relationships simultaneously to predict the response by adding them up\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://multithreaded.stitchfix.com/assets/images/blog/fig1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n\n. . .\n\n*   GAMs have the advantage over GLMs of greater flexibility\n\n. . .\n\n*   A disadvantage of GAMs and other smoothing methods, compared with GLMs, is the loss of simple interpretability\n\n    *   How do we interpret the effect of a predictor on the response?\n    \n    *   How do we obtain confidence intervals for those effects?\n    \n## Generalized additive models (GAMs)\n\n<center>\n\n![](/images/gam.png){width=\"750\"}\n\n</center>\n    \n## Splines\n\n*   A common way to smooth and learn nonlinear functions is to use **splines**\n\n*   A spline is a piecewise polynomial having continuous derivatives of all orders lower than the degree\nof the polynomial\n\n*   There are __knots__ (boundary points for functions) placed at every point\n\n*   Splines are functions that are constructed from simpler basis functions\n\n*   Splines can be used to approximate other, more complex functions\n\n## Splines\n\n* A $d^{\\text{th}}$-order spline is a piecewise polynomial function of degree $d$ that is continuous and has continuous derivatives of orders $1, \\dots, d âˆ’ 1$ at its knot points\n\n<!-- . . . -->\n\n<!-- * There are $t_1 < \\dots <t_p$ such that $f$ is a polynomial of degree $k$ on each of the intervals $(-\\infty, t_1], [t_1, t_2], \\dots, [t_p, \\infty)$ and $f^{(j)}$ is continuous at knots $t_1, \\dots, t_p$ for each $j=0,1,\\dots,k-1$ -->\n\n. . .\n\n* This requires us to choose the knots (fixed points between which the function is polynomial)\n\n. . .\n\n* We can eliminate the need to choose knots by using a **smoothing spline**\n\n\n## Smoothing splines\n\nUse __smooth function__ $s(x)$ to predict $y$, control smoothness directly by minimizing the __spline objective function__\n\n$$\\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int s''(x)^2dx$$\n\n. . .\n\n$$= \\text{fit data} + \\text{impose smoothness}$$\n\n. . .\n\n$$\\longrightarrow \\text{model fit} = \\text{likelihood} - \\lambda \\cdot \\text{wiggliness}$$\n\n## Smoothing splines\n\nThe most commonly considered case: cubic smoothing splines\n\n$$\\text{minimize }\\sum_{i=1}^n (y_i - s(x_i))^2 + \\lambda \\int s''(x)^2dx$$\n\n. . .\n\nFirst term: RSS, tries to make $s(x)$ fit the data at each $x_i$\n\n. . .\n\n<br>\n\nSecond term: roughness penalty, controls how wiggly $s(x)$ is via tuning parameter $\\lambda \\ge 0$\n\n. . .\n\n*   Balances the accuracy of the fit and the flexibility of the function\n\n. . .\n\n*   The smaller $\\lambda$, the more wiggly the function\n\n. . .\n    \n*   As $\\lambda \\rightarrow \\infty$, $s(x)$ becomes linear\n\n## Smoothing splines\n\nGoal: Estimate the __smoothing spline__ $\\hat{s}(x)$ that __balances the tradeoff between the model fit and wiggliness__\n\n. . .\n\nRemember: [Goldilocks principle](https://en.wikipedia.org/wiki/Goldilocks_principle)\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://github.com/noamross/gams-in-r-course/blob/master/images/diffsmooth-1.png?raw=true){fig-align='center' width=90%}\n:::\n:::\n\n\n\n<!-- --- -->\n\n<!-- ## Basis functions -->\n\n<!-- * Splines are _piecewise cubic polynomials_ with __knots__ (boundary points for functions) at every point -->\n\n<!-- . . . -->\n\n<!-- * Practical alternative: linear combination of set of __basis functions__ -->\n\n<!-- . . . -->\n\n<!-- * Example: cubic polynomial  -->\n\n<!--   * 4 basis functions: $B_1(x) = 1$, $B_2(x) = x$, $B_3(x) = x^2$, $B_4(x) = x^3$ -->\n\n<!--   * $\\displaystyle r(x) = \\sum_j^4 \\beta_j B_j(x)$ is the regression function -->\n\n<!--   *  __linear in the transformed variables__ $B_1(x), \\dots, B_4(x)$ but it is __nonlinear in $x$__ -->\n\n<!-- . . . -->\n\n<!-- *   Extend this idea for splines _piecewise_ using indicator functions so the spline is a weighted sum $$s(x) = \\sum_j^m \\beta_j B_j(x)$$ -->\n\n<!-- ## Number of basis functions -->\n\n<!-- <br> -->\n\n<!-- ```{r out.width='60%', echo = FALSE, fig.align='center'} -->\n<!-- knitr::include_graphics(\"https://github.com/noamross/gams-in-r-course/blob/master/images/diffbasis-1.png?raw=true\") -->\n<!-- ``` -->\n\n<!-- Think of this as a tuning parameter -->\n\n\n# Examples\n\n## Predicting MLB HR probability\n\n* Data available via the [`pybaseball`](https://github.com/jldbc/pybaseball) library in `python`\n\n* [Statcast data](https://baseballsavant.mlb.com/csv-docs) include pitch-level information, pulled from [baseballsavant.com](https://baseballsavant.mlb.com/)\n\n* Example data collected for the entire month of May 2024\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nsavant <- read_csv(\"https://raw.githubusercontent.com/36-SURE/2024/main/data/savant.csv\")\nbatted_balls <- savant |> \n  filter(type == \"X\") |> \n  mutate(is_hr = as.numeric(events == \"home_run\")) |> \n  filter(!is.na(launch_angle), !is.na(launch_speed), !is.na(is_hr))\n# head(batted_balls)\n```\n:::\n\n\n\n## Predicting HRs with launch angle and exit velocity\n\nHRs are relatively rare and confined to one area of this plot\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbatted_balls |> \n  ggplot(aes(x = launch_speed, \n             y = launch_angle, \n             color = as.factor(is_hr))) +\n  geom_point(alpha = 0.2)\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-nonparametric_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n## Predicting HRs with launch angle and exit velocity\n\nThere is a sweet spot of `launch_angle` (mid-way ish) and `launch_speed` (relatively high) where almost all HRs occur\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbatted_balls |> \n  group_by(\n    launch_angle_bucket = round(launch_angle * 2, -1) / 2,\n    launch_speed_bucket = round(launch_speed * 2, -1) / 2\n  ) |> \n  summarize(hr = sum(is_hr == 1),\n            n = n()) |> \n  ungroup() |> \n  mutate(pct_hr = hr / n) |> \n  ggplot(aes(x = launch_speed_bucket, \n             y = launch_angle_bucket, \n             fill = pct_hr)) +\n  geom_tile() +\n  scale_fill_gradient2(labels = scales::percent_format(),\n                       low = \"blue\", \n                       high = \"red\", \n                       midpoint = 0.2)\n```\n:::\n\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-nonparametric_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Fitting GAMs with [mgcv](https://cran.r-project.org/web/packages/mgcv/index.html)\n\n*   Set up training data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\ntrain <- batted_balls |> \n  slice_sample(prop = 0.5)\ntest <- batted_balls |> \n  anti_join(train)\n```\n:::\n\n\n\n. . .\n\n*   Modeling the log-odds of a home run using non-linear functions of `launch_speed` and `launch_angle` $$\n\\log \\left( \\frac{p_{\\texttt{is_hr}}}{1 - p_\\texttt{is_hr}} \\right) = \\beta_0 + s_1 (\\texttt{launch_speed}) + s_2 (\\texttt{launch_angle})$$ where $p_\\texttt{is_hr}$ is the probability of a home run\n\n. . .\n\n*   Fit the model with smoothing spline functions `s()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mgcv)\nhr_gam <- gam(is_hr ~ s(launch_speed) + s(launch_angle),\n              family = binomial,\n              method = \"REML\", # more stable solution than default\n              data = train)\n```\n:::\n\n\n\n## View model summary\n\n* `mgcv` performs hypothesis tests for the smooth terms --- these are roughly the equivalent of an $F$-test for dropping each term\n\n* Effective degrees of freedom (edf): basically the number of free parameters required to represent the function\n\n* In a smoothing spline, different choices of $\\lambda$ correspond to different values of the edf, representing different amounts of smoothness\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# summary(hr_gam)\nlibrary(broom)\n# glance(hr_gam)\ntidy(hr_gam)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 5\n  term              edf ref.df statistic p.value\n  <chr>           <dbl>  <dbl>     <dbl>   <dbl>\n1 s(launch_speed)  1.00   1.00      388.       0\n2 s(launch_angle)  3.92   4.24      251.       0\n```\n\n\n:::\n\n```{.r .cell-code}\ntidy(hr_gam, parametric = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    -34.8      11.8     -2.94 0.00329\n```\n\n\n:::\n:::\n\n\n\n## Visualize partial response functions\n\nDisplay the partial effect of each term in the model $\\longrightarrow$ add up to the overall prediction\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(gratia)\ndraw(hr_gam)\n```\n\n::: {.cell-output-display}\n![](18-nonparametric_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=1440}\n:::\n:::\n\n\n\n## Convert to probability scale\n\nCentered on average value of 0.5 because it's the partial effect without the intercept\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndraw(hr_gam, fun = plogis)\n```\n\n::: {.cell-output-display}\n![](18-nonparametric_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=1440}\n:::\n:::\n\n\n\n## Include intercept in plot\n\nIntercept reflects relatively rare occurence of HRs\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndraw(hr_gam, fun = plogis, constant = coef(hr_gam)[1])\n```\n\n::: {.cell-output-display}\n![](18-nonparametric_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Model diagnostics\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nappraise(hr_gam)\n```\n\n::: {.cell-output-display}\n![](18-nonparametric_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n\n## Model checking\n\nCheck whether more basis functions are needed with `gam.check()` based on an approximate test\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngam.check(hr_gam)\n```\n\n::: {.cell-output-display}\n![](18-nonparametric_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMethod: REML   Optimizer: outer newton\nfull convergence after 9 iterations.\nGradient range [-1.611342e-05,4.419327e-06]\n(score 627.7034 & scale 1).\nHessian positive definite, eigenvalue range [1.611218e-05,1.116216].\nModel rank =  19 / 19 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n                  k'  edf k-index p-value  \ns(launch_speed) 9.00 1.00    0.97   0.095 .\ns(launch_angle) 9.00 3.92    0.99   0.590  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n## Evaluate prediction accuracy\n\n*   In-sample performance (training set)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_gam |> \n  augment(type.predict = \"response\") |> \n  mutate(newdata = train, pred_class = round(.fitted)) |> \n  summarize(correct = mean(is_hr == pred_class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  correct\n    <dbl>\n1   0.974\n```\n\n\n:::\n:::\n\n\n\n*   Out-of-sample performance (test set)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_gam |> \n  augment(newdata = test, type.predict = \"response\") |> \n  mutate(pred_class = round(.fitted)) |> \n  summarize(correct = mean(is_hr == pred_class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  correct\n    <dbl>\n1   0.974\n```\n\n\n:::\n:::\n\n\n\n## Comparison with a GLM\n\n*   Very few situations in reality where GLMs (with a linear predictor) perform better than an additive model using smooth functions\n\n*   Especially since smooth functions can just capture linear models\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# note the warning\nhr_logit <- glm(is_hr ~ launch_speed + launch_angle, family = binomial, data = train)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_logit |> \n  augment(newdata = train, type.predict = \"response\") |> \n  mutate(pred_class = round(.fitted)) |> \n  summarize(correct = mean(is_hr == pred_class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  correct\n    <dbl>\n1   0.958\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_logit |> \n  augment(newdata = test, type.predict = \"response\") |> \n  mutate(pred_class = round(.fitted)) |> \n  summarize(correct = mean(is_hr == pred_class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  correct\n    <dbl>\n1   0.960\n```\n\n\n:::\n:::\n\n\n\n## Continuous interactions as a smooth surface\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_gam_mult <- gam(is_hr ~ s(launch_speed, launch_angle), family = binomial, data = train)\n```\n:::\n\n\n\nPlot the predicted heatmap (response on the log-odds scale)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# draw(hr_gam_mult)\nhr_gam_mult |> \n  smooth_estimates() |> \n  ggplot(aes(launch_speed, launch_angle, z = .estimate)) +\n  geom_contour_filled()\n```\n\n::: {.cell-output-display}\n![](18-nonparametric_files/figure-revealjs/unnamed-chunk-25-1.png){fig-align='center' width=1440}\n:::\n:::\n\n\n\n## Continuous interactions as a smooth surface\n\nPlot the predicted heatmap (response on the probability scale)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_gam_mult |> \n  smooth_estimates() |> \n  mutate(prob = plogis(.estimate)) |> \n  ggplot(aes(launch_speed, launch_angle, z = prob)) +\n  geom_contour_filled()\n```\n\n::: {.cell-output-display}\n![](18-nonparametric_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=1440}\n:::\n:::\n\n\n\n## Evaluate predictions\n\n*   This has one smoothing parameter for the 2D smooth\n\n*   But prediction accuracy does not improve\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_gam_mult |> \n  augment(newdata = train, type.predict = \"response\") |> \n  mutate(pred_class = round(.fitted)) |> \n  summarize(correct = mean(is_hr == pred_class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  correct\n    <dbl>\n1   0.974\n```\n\n\n:::\n\n```{.r .cell-code}\nhr_gam_mult |> \n  augment(newdata = test, type.predict = \"response\") |> \n  mutate(pred_class = round(.fitted)) |> \n  summarize(correct = mean(is_hr == pred_class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  correct\n    <dbl>\n1   0.974\n```\n\n\n:::\n:::\n\n\n\n## Separate interactions from individual terms with tensor smooths\n\n*   Decompose the smooth into main effects and an interaction with `ti()`\n\n*   Another option: `te()`, representing a tensor product smooth\n\nMore complicated model but yet does not improve prediction accuracy\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_gam_tensor <- gam(is_hr ~ s(launch_speed) + s(launch_angle) + ti(launch_speed, launch_angle),\n                     family = binomial,\n                     data = train)\n```\n:::\n\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_gam_tensor |> \n  augment(newdata = train, type.predict = \"response\") |> \n  mutate(pred_class = round(.fitted)) |> \n  summarize(correct = mean(is_hr == pred_class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  correct\n    <dbl>\n1   0.974\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhr_gam_tensor |> \n  augment(newdata = test, type.predict = \"response\") |> \n  mutate(pred_class = round(.fitted)) |> \n  summarize(correct = mean(is_hr == pred_class))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 1\n  correct\n    <dbl>\n1   0.974\n```\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n## Resources\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">140 char vrsn<br><br>1 GAMs are just GLMs<br>2 GAMs fit wiggly terms<br>3 use + s(foo) not foo in frmla<br>4 use method = &quot;REML&quot;<br>5 gam.check()</p>&mdash; ðŸ‡ºðŸ‡¦ Dr Gavin Simpson ðŸ˜·ðŸ‡ªðŸ‡ºðŸ‡©ðŸ‡° (@ucfagls) <a href=\"https://twitter.com/ucfagls/status/842444686513991680?ref_src=twsrc%5Etfw\">March 16, 2017</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n- Book: [Generalized Additive Models](https://reseau-mexico.fr/sites/reseau-mexico.fr/files/igam.pdf)\n\n- [GAMs in R by Noam Ross](https://noamross.github.io/gams-in-r-course/)\n\n- [`mgcv` course](https://eric-pedersen.github.io/mgcv-esa-workshop/)\n\n- [GAM: The Predictive Modeling Silver Bullet](https://multithreaded.stitchfix.com/blog/2015/07/30/gam/)\n\n- Chapters 7 & 8 of [Advanced Data Analysis from an Elementary Point of View by Cosma Shalizi](https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf)\n\n## Appendix: code to build dataset\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# pip install pybaseball\n# dictionary: https://baseballsavant.mlb.com/csv-docs\nimport pandas as pd\nfrom pybaseball import statcast\nsavant = statcast(start_dt='2024-05-01', end_dt='2024-05-31')\nsavant.to_csv('savant.csv', index=False)\n```\n:::",
    "supporting": [
      "18-nonparametric_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}