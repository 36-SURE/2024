{
  "hash": "91697b62afba93ade291372c98196b07",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Supervised learning: linear regression\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n# Background\n\n## Resources\n\n*   [Advanced Data Analysis from an Elementary Point of View](https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf) (aka the greatest book that will never be published)\n\n*   [The Truth about Linear Regression](https://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf)\n\n> \"Regression\", in statistical jargon, is the problem of guessing the average level of some quantitative response variable from various predictor variables. - [Note GOAT](http://bactra.org/notebooks/regression.html)\n\n## Simple linear regression\n\n**Linear regression is used when the response variable is quantitative**\n\n. . .\n\nAssume a __linear relationship__ for $Y = f(X)$ $$Y_{i}=\\beta_{0}+\\beta_{1} X_{i}+\\epsilon_{i}, \\quad \\text { for } i=1,2, \\dots, n$$\n\n- $Y_i$ is the $i$th value for the __response__ variable\n  \n- $X_i$ is the $i$th value for the __predictor__ variable\n\n. . .\n  \n- $\\beta_0$ is an _unknown_, constant __intercept__\n\n  -   average value for $Y$ if $X = 0$ (be careful sometimes...)\n  \n- $\\beta_1$ is an _unknown_, constant __slope__\n\n  -   change in average value for $Y$ for each one-unit increase in $X$\n  \n. . .\n  \n- $\\epsilon_i$ is the __random__ noise\n\n    -   assume __independent, identically distributed__ (_iid_) from a normal distribution \n\n    -   $\\epsilon_i \\overset{iid}{\\sim}N(0, \\sigma^2)$ with constant variance $\\sigma^2$\n\n\n\n## Simple linear regression estimation\n\nWe are estimating the __conditional expection (mean)__ for $Y$:\n\n$$\\mathbb{E}[Y_i \\mid X_i] = \\beta_0 + \\beta_1X_i$$\n\n- average value for $Y$ given the value for $X$\n\n. . .\n\nHow do we estimate the __best fitted__ line?\n\n. . .\n\n__Ordinary least squares (OLS)__ - by minimizing the __residual sum of squares (RSS)__\n\n$$\\text{RSS} \\left(\\beta_{0}, \\beta_{1}\\right)=\\sum_{i=1}^{n}\\left[Y_{i}-\\left(\\beta_{0}+\\beta_{1} X_{i}\\right)\\right]^{2}=\\sum_{i=1}^{n}\\left(Y_{i}-\\beta_{0}-\\beta_{1} X_{i}\\right)^{2}$$\n\n. . .\n\n$$\\widehat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2}} \\quad \\text{ and } \\quad \\widehat{\\beta}_{0}=\\bar{Y}-\\widehat{\\beta}_{1} \\bar{X}$$\n\nwhere $\\displaystyle \\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$ and $\\displaystyle \\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$\n\n\n\n## Connection to covariance and correlation\n\n[__Covariance__](https://en.wikipedia.org/wiki/Covariance) describes the __joint variability of two variables__ $$\\text{Cov}(X, Y) = \\sigma_{X,Y} = \\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]$$\n\n. . .\n\n__Sample covariance__ (use $n - 1$ since the means are used and we want [__unbiased estimates__](https://lazyprogrammer.me/covariance-matrix-divide-by-n-or-n-1/)) $$\\hat{\\sigma}_{X,Y} = \\frac{1}{n-1} \\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)$$\n\n## Connection to covariance and correlation\n\n__Correlation__ is a _normalized_ form of covariance, ranges from -1 to 1 $$\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\cdot \\sigma_Y}$$\n\n__Sample correlation__ uses the sample covariance and standard deviations, e.g. $\\displaystyle s_X^2 = \\frac{1}{n-1} \\sum_i (X_i - \\bar{X})^2$ $$r_{X,Y} = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)^{2} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}}$$\n\n\n\n## Connection to covariance and correlation\n\nWe have $$\\widehat{\\beta}_{1}=\\frac{\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}} \\quad \\text{ and } \\quad r_{X,Y} = \\frac{\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} \\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}}}$$\n\n. . .\n\nWe can rewrite $\\hat{\\beta}_1$ as $$\\widehat{\\beta}_{1} = r_{X,Y} \\cdot \\frac{s_Y}{s_X}$$\n\nWe can rewrite $\\displaystyle r_{X,Y}$ as $$r_{X,Y} = \\widehat{\\beta}_{1} \\cdot \\frac{s_X}{s_Y}$$\n\n. . .\n\nCan think of $\\widehat{\\beta}_{1}$ weighting the ratio of variance between $X$ and $Y$...\n\n\n# Linear regression in `R`\n\n## Gapminder data\n\nHealth and income outcomes for 184 countries from 1960 to 2016 from the famous [Gapminder project](https://www.gapminder.org/data)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nlibrary(dslabs)\nclean_gapminder <- gapminder |>\n  filter(year == 2011, !is.na(gdp)) |>\n  mutate(log_gdp = log(gdp))\nglimpse(clean_gapminder)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 168\nColumns: 10\n$ country          <fct> \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             <int> 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011,…\n$ infant_mortality <dbl> 14.3, 22.8, 106.8, 7.2, 12.7, 15.3, 3.8, 3.4, 32.5, 1…\n$ life_expectancy  <dbl> 77.4, 76.1, 58.1, 75.9, 76.0, 73.5, 82.2, 80.7, 70.8,…\n$ fertility        <dbl> 1.75, 2.83, 6.10, 2.12, 2.20, 1.50, 1.88, 1.44, 1.96,…\n$ population       <dbl> 2886010, 36717132, 21942296, 88152, 41655616, 2967984…\n$ gdp              <dbl> 6321690864, 81143448101, 27013935821, 801787943, 4729…\n$ continent        <fct> Europe, Africa, Africa, Americas, Americas, Asia, Oce…\n$ region           <fct> Southern Europe, Northern Africa, Middle Africa, Cari…\n$ log_gdp          <dbl> 22.56725, 25.11948, 24.01962, 20.50235, 26.88222, 22.…\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## Modeling life expectancy\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nInterested in modeling a country's __life expectancy__\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder |>\n  ggplot(aes(x = life_expectancy)) +\n  geom_histogram(color = \"black\", fill = \"gray\")\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-linear_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n## Relationship between life expectancy and log GDP\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngdp_plot <- clean_gapminder |>\n  ggplot(aes(x = log_gdp, y = life_expectancy)) +\n  geom_point(size = 3, alpha = 0.5)\ngdp_plot\n```\n:::\n\n\n\nWe fit linear regression models using `lm()`, formula is input as: `response ~ predictor`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsimple_lm <- lm(life_expectancy ~ log_gdp, \n                data = clean_gapminder) \n```\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-linear_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n## Summarize the model using  `summary()`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(simple_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = life_expectancy ~ log_gdp, data = clean_gapminder)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.901  -4.781   1.879   5.335  13.962 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   24.174      5.758   4.198 4.38e-05 ***\nlog_gdp        1.975      0.242   8.161 7.87e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.216 on 166 degrees of freedom\nMultiple R-squared:  0.2864,\tAdjusted R-squared:  0.2821 \nF-statistic: 66.61 on 1 and 166 DF,  p-value: 7.865e-14\n```\n\n\n:::\n:::\n\n\n\n## Summarize the model using the [`broom`](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) package\n\nThe 3 `broom` functions (Note: the output is always a tibble)\n\n*   `tidy()`: coefficients table in a tidy format\n\n*   `glance()`: produces summary metrics of a model fit\n\n*   `augment()`: adds/\"augments\" columns to the original data (e.g., adding predictions)\n\nNote: the output of `tidy()`, `augment()` and `glance()` are always a tibble.\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom)\ntidy(simple_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    24.2      5.76       4.20 4.38e- 5\n2 log_gdp         1.97     0.242      8.16 7.87e-14\n```\n\n\n:::\n\n```{.r .cell-code}\nglance(simple_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.286         0.282  7.22      66.6 7.87e-14     1  -569. 1145. 1154.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n\n## Inference with OLS\n\nIntercept and coefficient estimates $$\\quad \\hat{\\beta}_0 = 24.174 \\quad \\text{and} \\quad \\hat{\\beta}_1 = 1.975$$\n\n. . .\n\nEstimates of uncertainty for $\\beta$'s via __standard errors__ $$\\quad \\widehat{SE}(\\hat{\\beta}_0) = 5.758 \\quad \\text{and} \\quad \\widehat{SE}(\\hat{\\beta}_1) = 0.242$$\n\n. . .\n\n$t$-statistics:  `Estimates` / `Std. Error`, i.e., number of standard deviations from 0\n\n  - $p$-values (i.e., `Pr(>|t|)`): estimated probability observing value as extreme as |`t value`| __given the null hypothesis__ $\\beta_1 = 0$\n  \n  - $p$-value $< \\alpha = 0.05$ (conventional threshold): __sufficient evidence to reject the null hypothesis that the coefficient is zero__\n\n  - i.e., there is a __significant__ association between `life_expectancy` and `log_gdp`\n\n\n## Coefficient of determination\n\n* In linear regression, the square of the correlation coefficient happens to be exactly the coefficient of\ndetermination\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor(clean_gapminder$log_gdp, clean_gapminder$life_expectancy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5351189\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(clean_gapminder$log_gdp, clean_gapminder$life_expectancy) ^ 2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2863522\n```\n\n\n:::\n:::\n\n\n\n* $R^2$ estimates the __proportion of the variance__ of $Y$ explained by $X$ \n\n* More generally: variance of model predictions / variance of $Y$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvar(predict(simple_lm)) / var(clean_gapminder$life_expectancy) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2863522\n```\n\n\n:::\n:::\n\n\n\n\n## Generating predictions\n\nWe can use the `predict()` or `fitted()` function to either get the fitted values of the regression:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain_preds <- predict(simple_lm)\nhead(train_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6 \n68.74401 73.78465 71.61243 64.66585 77.26605 67.97876 \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(fitted(simple_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6 \n68.74401 73.78465 71.61243 64.66585 77.26605 67.97876 \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(simple_lm$fitted.values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6 \n68.74401 73.78465 71.61243 64.66585 77.26605 67.97876 \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsimple_lm |> \n  pluck(\"fitted.values\") |> \n  head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6 \n68.74401 73.78465 71.61243 64.66585 77.26605 67.97876 \n```\n\n\n:::\n:::\n\n\n\n\n## Predictions for new data\n\nOr we can provide it `newdata` which __must contain the explanatory variables__:\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nus_data <- clean_gapminder |> \n  filter(country == \"United States\")\n\nnew_us_data <- us_data |>\n  select(country, gdp) |>\n  slice(rep(1, 3)) |> \n  mutate(adj_factor = c(0.25, 0.5, 0.75),\n         log_gdp = log(gdp * adj_factor))\nnew_us_data$pred_life_exp <- \n  predict(simple_lm, newdata = new_us_data) \ngdp_plot +\n  geom_point(data = new_us_data,\n             aes(x = log_gdp, y = pred_life_exp),\n             color = \"darkred\", size = 3)\n```\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-linear_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n\n:::\n:::\n\n\n\n## Plot observed values against predictions\n\nUseful diagnostic (for __any type of model__, not just linear regression!) \n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder |>\n  mutate(pred_vals = predict(simple_lm)) |> \n  ggplot(aes(x = pred_vals, y = life_expectancy)) +\n  geom_point(alpha = 0.5, size = 3) +\n  geom_abline(slope = 1, intercept = 0, \n              linetype = \"dashed\",\n              color = \"red\",\n              linewidth = 2)\n```\n:::\n\n\n\n- \"Perfect\" model will follow __diagonal__\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-linear_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n## Plot observed values against predictions\n\nAugment the data with model output using the [`broom` package](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder <- simple_lm |> \n  augment(clean_gapminder) \nclean_gapminder |>\n  ggplot(aes(x = .fitted, y = life_expectancy)) + \n  geom_point(alpha = 0.5, size = 3) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", \n              linetype = \"dashed\", linewidth = 2)\n```\n:::\n\n\n\n- Adds various columns from model fit we can use in plotting for model diagnostics\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-linear_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n:::\n:::\n\n## Assessing assumptions of linear regression\n\nSimple linear regression assumes $Y_i \\overset{iid}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)$\n\n- If this is true, then $Y_i - \\hat{Y}_i \\overset{iid}{\\sim} N(0, \\sigma^2)$\n\n. . .\n\nPlot residuals against $\\hat{Y}_i$, __residuals vs fit__ plot\n\n- Used to assess linearity, any divergence from mean 0\n\n- Used to assess equal variance, i.e., if $\\sigma^2$ is homogenous across predictions/fits $\\hat{Y}_i$\n\n. . .\n\nMore difficult to assess the independence and fixed $X$ assumptions\n\n- Make these assumptions based on subject-matter knowledge\n\n\n\n## Plot residuals against predicted values\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n- Residuals = observed - predicted\n\n- Interpretation of residuals in context?\n\n- Conditional on the predicted values, the __residuals should have a mean of zero__\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder |>\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point(alpha = 0.5, size = 3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", \n             color = \"red\", linewidth = 2) +\n  # plot the residual mean\n  geom_smooth(se = FALSE)\n```\n:::\n\n\n\n- Residuals __should NOT display any pattern__\n\n- Two things to look for:\n\n  -   Any trend around horizontal reference line?\n\n  -   Equal vertical spread?\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-linear_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n:::\n:::\n\n\n\n## Multiple linear regression \n\nWe can include as many variables as we want (assuming $n > p$)\n\n$$Y=\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\cdots+\\beta_{p} X_{p}+\\epsilon$$\n\nOLS estimates in matrix notation ( $\\boldsymbol{X}$ is a $n \\times p$ matrix):\n  \n$$\\hat{\\boldsymbol{\\beta}} = (\\boldsymbol{X} ^T \\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$$\n\n. . .\n\nCan just add more variables to the formula in `R`\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmultiple_lm <- lm(life_expectancy ~ log_gdp + fertility, data = clean_gapminder)\n```\n:::\n\n\n\n- Use adjusted $R^2$  when including multiple variables $\\displaystyle 1 - \\frac{(1 - R^2)(n - 1)}{(n - p - 1)}$\n\n  - Adjusts for the number of variables in the model $p$  \n  \n  - Adding more variables __will always increase__ the model's (unadjusted) $R^2$\n  \nREAD THIS: [$R^2$ myths](https://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf#page=181) \n\n[Get your $R^2$ out of here](https://www.tiktok.com/@fatgreggy/video/7186499956058950958)\n\n\n## What about the Normal distribution assumption?\n\n* Model: $Y=\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\cdots+\\beta_{p} X_{p}+\\epsilon$\n\n- $\\epsilon_i$ is the __random__ noise: assume __independent and identically distributed__ (_iid_) from a Normal distribution $\\epsilon_i \\overset{iid}{\\sim}N(0, \\sigma^2)$ with constant variance $\\sigma^2$\n\n. . .\n\n* __OLS doesn't care about this assumption__, it's just estimating coefficients!\n\n. . .\n\n* In order to perform inference, __we need to impose additional assumptions__\n\n* By assuming $\\epsilon_i \\overset{iid}{\\sim}N(0, \\sigma^2)$, what we really mean is $Y \\overset{iid}{\\sim}N(\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\cdots+\\beta_{p} X_{p}, \\sigma^2)$\n\n. . .\n\n* So we're estimating the mean $\\mu$ of this conditional distribution, but what about $\\sigma^2$?\n  \n. . .\n\n* [Unbiased estimate](https://bradleyboehmke.github.io/HOML/linear-regression.html#simple-linear-regression) $\\displaystyle \\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n - (p + 1)}$\n\n* __Degrees of freedom__:  $n - (p + 1)$, data supplies us with $n$ \"degrees of freedom\" and we used up $p + 1$\n\n\n\n## Check the assumptions about normality with [`ggfortify`](https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_lm.html)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggfortify)\nautoplot(multiple_lm, ncol = 4)\n```\n\n::: {.cell-output-display}\n![](11-linear_files/figure-revealjs/more-resid-plots-1.png){fig-align='center' width=1440}\n:::\n:::\n\n\n\n- Standardized residuals = residuals `/ sd(`residuals`)` (see also `.std.resid` from `augment`)\n\n\n## Regression confidence intervals vs prediction intervals\n\nDo we want to predict the mean response for a particular value $x^*$ of the explanatory variable or do we want to predict the response for an individual case?\n\nExample: \n\n*   if we would like to know the average price for **all** Tesla with 50 thousand miles, then we would use an interval for the **mean response**\n\n*   if we want an interval to contain the price of **a particular** Tesla with 50 thousand miles, then we need the interval for **a single prediction**\n\n\n## Regression confidence intervals\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n- `geom_smooth()` displays __confidence intervals__ for the regression line\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# predict(simple_lm, interval = \"confidence\")\nlm_plot <- clean_gapminder |>\n  ggplot(aes(x = log_gdp,\n             y = life_expectancy)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  theme_bw() +\n  labs(x = \"log(GDP)\",\n       y = \"Life expectancy\")\nlm_plot\n```\n:::\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-linear_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n:::\n:::\n\n\n---\n\n## Regression confidence intervals\n\n*   Interval estimate for the MEAN response at a given observation (i.e. for the predicted AVERAGE $\\hat{\\beta}_0 + \\hat{\\beta}_1 x^*$)\n\n*   Based on standard errors for the estimated regression line at $x^*$ $$SE_{\\text{line}}\\left(x^{*}\\right)=\\hat{\\sigma} \\cdot \\sqrt{\\frac{1}{n}+\\frac{\\left(x^{*}-\\bar{x}\\right)^{2}}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}}$$\n\n*   Variation only comes from uncertainty in the parameter \n\n## Regression prediction intervals\n\n*   interval estimate for an INDIVIDUAL response at a given (new) observation \n\n*   add the variance $\\sigma^2$ of a __single predicted value__ $$SE_{\\text{pred}}\\left(x^{*}\\right)=\\hat{\\sigma} \\cdot \\sqrt{1 + \\frac{1}{n}+\\frac{\\left(x^{*}-\\bar{x}\\right)^{2}}{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}}}$$\n\n*   variation comes from uncertainty in parameter estimates and error variance \n\n## Confidence intervals versus prediction intervals\n\nGenerate 95% intervals\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# predict(simple_lm, interval = \"prediction\")\nlm_plot +\n  geom_ribbon(\n    data = augment(simple_lm, interval = \"prediction\"),\n    aes(ymin = .lower, ymax = .upper),\n    color = \"red\", fill = NA\n  )\n```\n:::\n\n\n\n*   The standard error for predicting an individual response will always be larger than for predicting a mean response\n\n*   Prediction intervals will always be wider than confidence intervals\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-linear_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n:::\n:::\n\n## Appendix: Interpret a linear regression model with log transformations\n\n* Log-transformed predictor: $Y = \\beta_0 + \\beta_1 \\log(X)$\n\n  *   A 1% increase in $X$ is associated with an average change of $\\beta_1/100$ units in $Y$\n\n* Log-transformed outcome: $\\log(Y) = \\beta_0 + \\beta_1 X$\n\n  *   A 1 unit increase in $X$ is associated with an average change of $(100 \\times \\beta) \\%$ in Y\n\n* Log-transformed predictor and outcome: $\\log(Y) = \\beta_0 + \\beta_1 \\log(X)$\n\n  *   A 1% increase in $X$ is associated with an average change of $\\beta_1 \\%$ in Y\n  \n[Useful reading](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/transf/transf.pdf)\n",
    "supporting": [
      "11-linear_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}