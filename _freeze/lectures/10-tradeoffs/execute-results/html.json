{
  "hash": "37ddd95820723f6956854a575b4602fa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Supervised learning: the tradeoffs\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n# Background\n\n## Concepts that hopefully you'll be able to distinguish\n\n* Supervised vs. unsupervised learning\n* Classification vs. regression\n* Classification vs. clustering\n* Explanatory vs. response variable\n* Inference vs. prediction\n* Flexibility-interpretability tradeoff\n* Bias-variance tradeoff\n* Model assessment vs. model selection\n* Parametric vs. nonparametric models\n\n## Statistical learning\n\n> Statistical learning refers to a set of tools for making sense of complex\ndatasets. --- [Preface of ISLR](https://www.statlearning.com/)\n\n. . .\n\nGeneral setup: Given a dataset of $p$ variables (columns) and $n$ observations (rows) $x_1,\\dots,x_n$.\n\nFor observation $i$, $$x_{i1},x_{i2},\\ldots,x_{ip} \\sim P \\,,$$ where $P$ is a $p$-dimensional distribution that we might not know much about *a priori*\n\n## Supervised learning\n\n* Response variable $Y$ in one of the $p$ variables (columns)\n\n* The remaining $p-1$ variables are predictor measurements $X$\n\n  *   Regression: $Y$ is quantitative\n  \n  *   Classification: $Y$ is categorical\n  \n. . .\n\n\n__Goal__: uncover associations between a set of __predictor__ (__independent__ / __explanatory__) variables / features and a single __response__ (or __dependent__) variable\n\n*   Accurately predict unseen test cases\n\n*   Understand which features  affect the response (and how)\n\n*   Assess the quality of our predictions and inferences\n\n## They're all the same\n\n<center>\n\n![](/images/covariates.png){width=\"600\"}\n\n</center>\n\n\n<!-- . . . -->\n\n<!-- * If we are using one or more variables to help us understand or predict values of another variable, -->\n\n<!--   * The variable that is the outcome of interest, the one weâ€™re trying to predict, is called the response variable (also called dependent variable) -->\n\n<!--   *   The variable whose relationship to the response is being studied is called the explanatory variable (or independent -->\n<!-- variable) -->\n\n<!-- Note: each of the explanatory and response variables can either be quantitative or categorical -->\n\n## Supervised learning\n\nExamples\n\n* Identify the risk factors for prostate cancer\n\n. . .\n\n* Predict whether someone will have a heart attack based on demographic, diet, and clinical measurements\n\n. . .\n\n* Predict a player's batting average in year $t+1$ using their batting average in year $t$ and uncover meaningful relationships between other measurements and batting average in year $t+1$\n\n. . .\n\n* Given NFL player tracking data which contain 2D coordinates of every player on the field at every tenth of the second, predict how far a ball-carrier will go at any given moment within a play\n\n\n\n## Examples of statistical learning methods / algorithms\n\n__You are probably already familiar with statistical learning__ - even if you did not know what the phrase meant before\n\n\n. . .\n\nExamples of statistical learning algorithms include:\n\n- Generalized linear models (GLMs) and penalized versions (lasso, ridge, elastic net)\n\n- Smoothing splines, generalized additive models (GAMs)\n\n- Decision trees and its variants (e.g., random forests, boosting)\n\n- Neural networks\n\n. . .\n\nTwo main types of problems\n\n- __Regression__ models: estimate _average_ value of response (i.e. the response is quantitative)\n\n- __Classification__ models: determine _the most likely_ class of a set of discrete response variable classes (i.e. the response is categorical)\n\n\n## Which method should I use in my analysis?\n\n. . .\n\n__IT DEPENDS__ - the big picture: __inference__ vs __prediction__\n\n\nLet $Y$ be the response variable, and $X$ be the predictors, then the __learned__ model will take the form:\n\n$$\n\\hat{Y}=\\hat{f}(X)\n$$\n\n. . .\n\n- Care about the details of $\\hat{f}(X)$? $\\longrightarrow$ inference\n\n- Fine with treating $\\hat{f}(X)$ as a obscure/mystical machine? $\\longrightarrow$ prediction\n\n\n. . .\n\nAny algorithm can be used for prediction, however options are limited for inference\n\n\n_Active area of research on using more mystical models for statistical inference_\n\n\n# The tradeoffs\n\n## Some tradeoffs\n\n* Prediction accuracy vs interpretability\n\n  * Linear models are easy to interpret; boosted trees are not\n  \n. . .\n  \n* Good fit vs overfit or underfit\n\n  * How do we know when the fit is just right?\n  \n. . .\n  \n* Parsimony versus black-box\n\n  * We often prefer a simpler model involving fewer variables over a black-box involving more (or all) predictors\n\n## Model flexibility vs interpretability\n\nGenerally speaking: __tradeoff__ between a model's _flexibility_ (i.e. how \"wiggly\" it is) and how __interpretable__ it is\n\n- The simpler parametric form of the model, the easier it is to interpret\n\n- Hence why __linear regression__ is popular in practice\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/flexibility.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n::: aside\n[[ISLR Figure 2.7](https://www.statlearning.com/)]\n:::\n\n\n\n## Model flexibility vs interpretability\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/flexibility.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n- __Parametric__ models, for which we can write down a mathematical expression for $f(X)$ __before observing the data__, _a priori_ (e.g. linear regression), __are inherently less flexible__\n\n\n. . .\n\n- __Nonparametric__ models, in which $f(X)$ is __estimated from the data__ (e.g. kernel regression)\n\n\n\n## Model flexibility vs interpretability\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/flexibility.png){fig-align='center' width=50%}\n:::\n:::\n\n\n\n- If your goal is prediction, your model can be as arbitrarily flexible as it needs to be\n\n- We'll discuss how to estimate the optimal amount of flexibility shortly...\n\n\n## Looks about right...\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://66.media.tumblr.com/c4886c7b12f2a9a7d81cba3e8d8f1d00/bc9f1aa7fb6adf6d-7c/s1280x1920/a01569c35bebdac425baf4ed3360f1481580d4d6.jpg){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n## Model assessment vs selection, what's the difference?\n\n. . .\n\n__Model assessment__:\n\n- __evaluating how well a learned model performs__, via the use of a single-number metric\n\n\n. . .\n\n__Model selection__:\n\n- selecting the best model from a suite of learned models (e.g., linear regression, random forest, etc.)\n\n\n\n## Model __flexibility__ ([ISLR Figure 2.9](https://www.statlearning.com/))\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/Flexibility.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n- Left panel: intuitive notion of the meaning of model flexibility\n\n- Data are generated from a smoothly varying non-linear model (shown in black), with random noise added:\n$$\nY = f(X) + \\epsilon\n$$\n\n\n\n## Model __flexibility__\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/Flexibility.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\nOrange line: an inflexible, fully parametrized model (simple linear regression)\n\n\n. . .\n\n- __Cannot__ provide a good estimate of $f(X)$\n\n\n. . .\n\n- Cannot __overfit__ by modeling the noisy deviations of the data from $f(X)$\n\n\n\n\n## Model __flexibility__\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/Flexibility.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\nGreen line: an overly flexible, nonparametric model \n\n\n. . .\n\n- __It can__ provide a good estimate of $f(X)$ \n\n. . .\n\n-  ... __BUT__ it goes too far and overfits by modeling the noise\n\n\n. . .\n\n__This is NOT generalizable__: bad job of predicting response given new data NOT used in learning the model\n\n\n\n## So... how do we deal with flexibility?\n\n__GOAL__: We want to learn a statistical model that provides a good estimate of $f(X)$ __without overfitting__\n\n\n. . .\n\nThere are two common approaches:\n\n- We can __split the data into two groups__:\n\n  - __training__ data: data used to train models, \n  \n  - __test__ data: data used to test them\n  \n  - By assessing models using \"held-out\" test set data, we act to ensure that we get a __generalizable(!)__ estimate of $f(X)$\n\n\n. . .\n\n- We can __repeat data splitting $k$ times__:\n\n  - Each observation is placed in the \"held-out\" / test data exactly once\n  \n  - This is called __k-fold cross validation__ (typically set $k$ to 5 or 10)\n\n\n. . .\n\n$k$-fold cross validation is the preferred approach, but the tradeoff is that CV analyses take ${\\sim}k$ times longer than analyses that utilize data splitting\n\n\n\n## Model assessment\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/Flexibility.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n- Right panel shows __a metric of model assessment__, the mean squared error (MSE) as a function of flexibility for both a training and test datasets\n\n\n. . .\n\n- Training error (gray line) __decreases as  flexibility increases__\n\n. . .\n\n- Test error (red line) decreases while flexibility increases __until__ the point a good estimate of $f(X)$ is reached, and then it __increases as it overfits to the training data__\n\n\n\n## Model assessment metrics\n\n__Loss function__ (aka _objective_ or _cost_ function) is a metric that represents __the quality of fit of a model__\n\n\n. . .\n\nFor regression we typically use __mean squared error (MSE)__\n- quadratic loss: squared differences between model predictions $\\hat{f}(X)$ and observed data $Y$\n\n$$\\text{MSE} = \\frac{1}{n} \\sum_i^n (Y_i - \\hat{f}(X_i))^2$$\n\n\n. . .\n\nFor classification, the situation is not quite so clear-cut\n\n- __misclassification rate__: percentage of predictions that are wrong\n\n- __area under the ROC curve (AUC)__\n\n. . .\n\n- interpretation can be affected by __class imbalance__: \n\n  - if two classes are equally represented in a dataset, a misclassification rate of 2% is good\n  \n  - but if one class comprises 99% of the data, that 2% is no longer such a good result...\n\n\n\n## Back to model selection\n\n__Model selection__: picking the best model from a suite of possible models \n\n- Example: Picking the best regression model based on __MSE__, or best classification model based on __misclassification rate__\n\n\n. . .\n\nTwo things to keep in mind:\n\n1. __Ensure an apples-to-apples comparison of metrics__\n\n  - every model should be learned using __the same training and test data__\n  \n  - Do not resample the data between the time when you, e.g., perform linear regression and vs you perform random forest\n  \n. . .\n\n2. __An assessment metric is a random variable__, i.e., if you choose different data to be in your training set, the metric will be different.\n\n. . .\n\nFor regression, a third point should be kept in mind: __a metric like the MSE is unit-dependent__\n\n- an MSE of 0.001 in one analysis context is not necessarily better or worse than an MSE of 100 in another context\n\n\n\n## An example __true__ model\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-tradeoffs_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n\n## The repeated experiments...\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-tradeoffs_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=1344}\n:::\n:::\n\n\n\n\n\n## The linear regression fits\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-tradeoffs_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=1344}\n:::\n:::\n\n\n\nLook at the plots. For any given value of $x$:\n\n- The *average* estimated $y$ value is offset from the truth (__high bias__)\n- The dispersion (variance) in the estimated $y$ values is relatively small (__low variance__)\n\n\n\n## The spline fits\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-tradeoffs_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=1344}\n:::\n:::\n\n\n\nLook at the plots. For any given value of $x$:\n\n. . .\n\n- The *average* estimated $y$ value approximately matches the truth (__low bias__)\n- The dispersion (variance) in the estimated $y$ values is relatively large (__high variance__)\n\n\n\n## Bias-variance tradeoff\n\n\"Best\" model minimizes the test-set MSE, where the __true__ MSE can be decomposed into ${\\rm MSE} = {\\rm (Bias)}^2 + {\\rm Variance}$\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](http://www.stat.cmu.edu/~pfreeman/Flexibility.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\nTowards the left: high bias, low variance. Towards the right: low bias, high variance. \n\n__Optimal amount of flexibility lies somewhere in the middle__ (\"just the right amount\" --- [*Goldilocks principle*](https://en.wikipedia.org/wiki/Goldilocks_principle))\n\n## \"The sweet spot\"\n\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Remember, when building predictive models, try to find the sweet spot between bias and variance. <a href=\"https://twitter.com/hashtag/babybear?src=hash&amp;ref_src=twsrc%5Etfw\">#babybear</a></p>&mdash; Dr. G. J. Matthews (@StatsClass) <a href=\"https://twitter.com/StatsClass/status/1037808217076760576?ref_src=twsrc%5Etfw\">September 6, 2018</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n## Principle of parsimony (Occam's razor)\n\n\"Numquam ponenda est pluralitas sine necessitate\" (plurality must never be posited without necessity)\n\nFrom ISLR:\n\n> When faced with new data modeling and prediction problems, it's tempting to always go for the trendy new methods. Often they give extremely impressive results, especially when the datasets are very large and can support the fitting of high-dimensional nonlinear models. However, if we can produce models with the simpler tools that perform as well, they are likely to be easier to fit and understand, and potentially less fragile than the more complex approaches. Wherever possible, it makes sense to try the simpler models as well, and then make a choice based on the performance/complexity tradeoff.\n\nIn short: \"When faced with several methods that give roughly equivalent performance, pick the simplest.\"\n\n## The curse of dimensionality\n\nThe more features, the merrier?... Not quite\n\n*   Adding additional **signal** features that are truly associated with the response will improve the ftted model\n\n    *   Reduction in test set error\n    \n*   Adding noise features that are not truly associated with the response will lead to a deterioration in the model\n\n    *   Increase in test set error\n    \n**Noise features increase the dimensionality of the problem, increasing the risk of overfitting**\n\n## Check out this song\n\n\n\n{{< video https://youtu.be/pZTLFu79UbY width=\"800\" height=\"600\" >}}\n\n",
    "supporting": [
      "10-tradeoffs_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}