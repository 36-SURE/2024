{
  "hash": "d02b74ecdfed7d4b009f33f6f9f5345f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unsupervised learning: Gaussian mixture models\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n# Background\n\n## Previously: clustering\n\n* Goal: partition $n$ data points into $K$ subgroups, where roughly points within group are more \"similar\" than points between groups\n\n* Previous methods: __$k$-means clustering __ and __hierarchical clustering__\n\n. . .\n\n* **Classification vs clustering**\n\n**Classification**\n\n*   We are given labeled (categorical) data\n\n*   Focus on generalization (i.e. we want to have a classifier that performs well on predicting new (test) data)\n\n. . .\n\n**Clustering**\n\n*   We are only given the points (with no labels)\n\n*   We want to find interesting subgroups/structure in the data\n\n\n## Previously: clustering\n\n- Previous methods: __$k$-means clustering __ and __hierarchical clustering__\n\n- Output __hard__ assignments, strictly assigning observations to only one cluster\n\n. . .\n\n- What about __soft__ assignments and __uncertainty__ in the clustering results?\n\n  - Assigns each observation a probability of belonging to a cluster\n\n  - Incorporate statistical modeling into clustering\n  \n. . .\n\n- We want to estimate the density of the observations in some way that allows us to extract clusters\n  \n## Motivating figure\n\n<br>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*uQTamSp8hAcnJPl0.){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## Model-based clustering\n\nKey idea: data are considered as coming from a mixture of underlying probability distributions\n\n. . .\n\nMost popular method: Gaussian mixture model (GMM)\n\n*   Each observation is assumed to be distributed as one of $k$ multivariate normal distributions\n\n*   $k$ is the number of clusters (components)\n\n## Previously: kernel density estimation\n\n\n\nKernel density estimator: $\\displaystyle \\hat{f}(x) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h} K_h(x - x_i)$\n\n. . .\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n* Smooth each data point into a small density bumps\n\n* Sum all these small bumps together to obtain the final density estimate\n\nUse __every observation__ when estimating the density for new points\n\nWe want to estimate the density of the points in some way that allows us to extract clusters\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](../images/kde-bumps.png){fig-align='center' width=678}\n:::\n:::\n\n\n\n\n:::\n:::\n\n## Mixture models\n\n*   Model the density as a mixture of distributions\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://angusturner.github.io/assets/images/mixture.png){fig-align='center'}\n:::\n:::\n\n\n\n\n## Mixture models\n\nFormally, $$\\displaystyle f(x) = \\sum_{k=1}^K \\tau_k f_k(x)$$\n\nwhere \n\n* $f_k$ are some distributions\n\n* $\\tau_k \\ge 0$ and $\\sum_{k=1}^K \\tau_k = 1$ are the mixture weights\n\n* $K$ is the number of mixture components (i.e. the number of clusters)\n\n\n## Mixture models\n\nThis is a __data generating process__\n\nImagine that each cluster (component) has a different distribution\n\n. . .\n\nTask: Generate a new observation (i.e. sample) from a mixture model\n\n. . .\n\n*   Choose a cluster by drawing $Z \\in  \\{1, \\dots, K\\}$, where $P(Z = k) = \\tau_k$ \n\n    *   i.e. $Z$ is a categorical latent variable indicating which cluster the new observation is from\n\n*   Generate an observation from the distribution $f_Z$ (corresponding to cluster $Z$)\n\n. . .\n\nHence $\\displaystyle f(x) = \\sum_{k=1}^K P(Z = k) p (x \\mid Z = k) = \\sum_{k=1}^K \\tau_k f_Z(x)$\n\n<!-- . . . -->\n\n<!-- 1. __Pick a distribution/component__ among our $K$ options by introducing a new variable $$z \\sim \\text{Multinomial} (\\tau_1, \\tau_2, \\dots, \\tau_k)$$ i.e. $z$ is a categorical latent variable indicating which component the new point is from -->\n\n<!-- . . . -->\n\n<!-- 2. __Generate an observation with that distribution/component__, i.e. $x \\mid z \\sim f_{z}$ -->\n\n\n<!-- . . . -->\n\n<!-- _So what do we use for each $f_k$?_ -->\n\n\n## Gaussian mixture models (GMMs)\n\n* Assume a __parametric mixture model__, with __parameters__ $\\theta_k$ for the $k$th component $$f(x) = \\sum_{k=1}^K \\tau_k f_k(x; \\theta_k)$$\n\n* Gaussian mixture models (GMMs) are perhaps the most popular mixture models\n\n. . .\n\n* Assume each component is [Gaussian (normal)](https://en.wikipedia.org/wiki/Normal_distribution), where the 1D case is\n\n$$f_k(x; \\theta_k) = N(x; \\mu_k, \\sigma_k^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma_k^2}}\\text{exp} \\left( -\\frac{(x - \\mu_k)^2}{2 \\sigma_k^2} \\right)$$\n\nwith mean $\\mu_k$ and variance $\\sigma_k ^2$\n\n. . .\n\n* We need to estimate each $\\{\\tau_1, \\dots, \\tau_k\\}$, $\\{\\mu_1, \\dots, \\mu_k\\}$, $\\{\\sigma_1, \\dots, \\sigma_k\\}$\n\n## Gaussian mixture models (GMMs)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://miro.medium.com/v2/resize:fit:1100/format:webp/0*uQTamSp8hAcnJPl0.){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\nModel $\\tau_1, \\tau_2, \\tau_3$ and \n\n$f_{\\text{red}}(x) = N(\\mu_{\\text{red}}, \\sigma^2_{\\text{red}})$\n\n$f_{\\text{blue}}(x) = N(\\mu_{\\text{blue}}, \\sigma^2_{\\text{blue}})$\n\n$f_{\\text{green}}(x) = N(\\mu_{\\text{green}}, \\sigma^2_{\\text{green}})$\n\n## Estimating a mixture model\n\nWhat can we do to soft-cluster our data?\n\n. . .\n\nFor an observation $x$, compute the weight for $x$ to belong to cluster $k$ for $k \\in \\{1, \\dots, K\\}$ $$ P(Z = k \\mid X = x) = \\frac{P(X = x \\mid Z = k) P(Z = k)}{\\sum_{j}P(X = x \\mid Z = j) P(Z = j)} =\\frac{\\tau_{k} \\ N(\\mu_{k}, \\sigma_{k}^{2})}{\\sum_{j=1}^{K} \\tau_{j} \\ N(\\mu_{j}, \\sigma^2_{j})}$$\n\n(recall that $\\displaystyle f(x) = \\sum_{k=1}^K \\tau_k \\ N(\\mu_k, \\sigma^2_k)$)\n\n. . .\n\nHow do we estimate the parameters of the mixture model (i.e. the $\\tau_k, \\mu_k, \\sigma_k$)?\n\n. . .\n\n*   We don't know which component (i.e. the true labels) an observation actually belongs to \n\n*   $Z$ is a latent variable (since the true cluster labels are not observed)\n\n*   This is known as the missing data or latent variable problems\n\n\n## Estimating a mixture model: maximum likelihood\n\nLikelihood function: how likely to observe data for given parameter values\n\n. . .\n\nSetup: Given observations $X_1, \\dots, X_n$\n\nTask: Estimate the parameters of a mixture model\n\n. . .\n\n*   What if we know the true cluster labels $Z_1, \\dots, Z_n$? We can just use maximum likelihood estimation and maximize $$L\\{\\left(\\tau_k, \\mu_k, \\sigma^2_k \\right)_{k=1}^K\\} = \\sum_{i=1}^n \\log f(X_i, Z_i)$$ (just take data from each group, compute the fraction of points $\\tau_k$, the mean $\\mu_k$, and the variance $\\sigma^2$)\n\n. . .\n\n*   In our case, since we do not know the cluster labels $Z_i$, we should try to maximize the (marginal) likelihood of the observed data\n\n$$\\mathcal L\\{\\left(\\tau_k, \\mu_k, \\sigma^2_k \\right)_{k=1}^K\\} = \\sum_{i=1}^n \\log f(X_i) = \\sum_{i=1}^n \\log \\left[\\sum_{k=1}^K f(X_i, k)\\right]$$\n\n\n## Estimating a mixture model: EM algorithm\n\n[Expectation–maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) is a method for (approximately) maximizing the (marginal) likelihood in the presence of missing data\n\nFor a GMM:\n\n*   First, initialize the model parameters randomly\n\n*   Then, alternate between the following two steps (keep repeating until nothing changes)\n\n    *   **E-step**: compute the cluster memberships for each point\n    \n    *   **M-step**: recompute/update the parameters\n\n<!-- ## Let's pretend we only have one component... -->\n\n<!-- Set up: Given $n$ observations from a single normal distribution -->\n\n<!-- Task: Estimate the distribution parameters using the __likelihood function__ - the probability/density of observing the data given the parameters $$\\mathcal{L}(\\mu, \\sigma \\mid x_1, \\dots, x_n) = f( x_1, \\dots, x_n \\mid \\mu, \\sigma) =  \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\tau \\sigma^2}}\\text{exp } \\left\\{-\\frac{(x_i - \\mu)^2}{2 \\sigma^2}\\right\\}$$ -->\n\n<!-- . . . -->\n\n<!-- Compute the __maximum likelihood estimates (MLEs)__ for $\\mu$ and $\\sigma$ -->\n\n<!-- - $\\displaystyle \\hat{\\mu}_{\\rm MLE} = \\frac{1}{n} \\sum_i^n x_i$, sample mean -->\n\n<!-- - $\\displaystyle \\hat{\\sigma}_{\\rm MLE} = \\sqrt{\\frac{1}{n}\\sum_i^n (x_i - \\mu)^2}$, sample standard deviation (plug in $\\hat{\\mu}_{\\rm MLE}$) -->\n\n\n<!-- ## The problem with more than one component -->\n\n<!-- ::: columns -->\n<!-- ::: {.column width=\"50%\" style=\"text-align: left;\"} -->\n\n<!-- - __We don't know which component an observation belongs to__ -->\n\n<!-- - __IF WE DID KNOW__, then we could compute each component's MLEs as before -->\n\n<!-- - But we don't know because $z$ is a __latent variable__! So what about its distribution given the data? -->\n\n<!-- $\\displaystyle P(z_i = k \\mid x_i) = \\frac{P(x_i \\mid z_i = k) P(z_i = k)}{P(x_i)}$ -->\n\n<!-- $\\displaystyle =\\frac{\\tau_{k} N\\left(\\mu_{k}, \\sigma_{k}^{2}\\right)}{\\sum_{k=1}^{K} \\tau_{k} N\\left(\\mu_{k}, \\sigma_{k}\\right)}$ -->\n\n<!-- - __But we do NOT know these parameters!__ -->\n\n<!-- ::: -->\n\n<!-- ::: {.column width=\"50%\" style=\"text-align: left;\"} -->\n<!-- ```{r init-sim-data, echo = FALSE, fig.align='center', fig.height=9} -->\n<!-- library(tidyverse) -->\n<!-- # mixture components -->\n<!-- mu_true <- c(5, 13) -->\n<!-- sigma_true <- c(1.5, 2) -->\n<!-- # determine Z_i -->\n<!-- z <- rbinom(500, 1, 0.75) -->\n<!-- # sample from mixture model -->\n<!-- x <- rnorm(10000, mean = mu_true[z + 1],  -->\n<!--            sd = sigma_true[z + 1]) -->\n\n<!-- tibble(xvar = x) |> -->\n<!--   ggplot(aes(x = xvar)) + -->\n<!--   geom_histogram(color = \"black\", -->\n<!--                  fill = \"darkblue\", -->\n<!--                  alpha = 0.3) + -->\n<!--   labs(x = \"Simulated variable\", -->\n<!--        y = \"Count\") -->\n<!-- ``` -->\n\n<!-- ::: -->\n<!-- ::: -->\n\n\n## Expectation-maximization (EM) algorithm\n\nAlternate between the following:\n\n- _pretend_ to know the probability each observation belongs to each group, to estimate the parameters of the components\n\n. . .\n\n- _pretend_ to know the parameters of the components, to estimate the probability each observation belong to each group\n\n. . .\n\n1. Start with initial guesses about $\\tau_1, \\dots, \\tau_k$, $\\mu_1, \\dots, \\mu_k$, $\\sigma_1, \\dots, \\sigma_k$\n\n2. Repeat until nothing changes:\n\n\n. . .\n\n- **E-step**: calculate $\\hat{z}_{ik}$, the weight for observation $i$ belonging to cluster $k$\n\n- **M-step**: update parameter estimates with __weighted__ MLE using $\\hat{z}_{ik}$\n\n## Is this familiar?\n\n. . .\n\n*   In GMMs, we're essentially guessing the latent variables $Z_i$ and pretending to know the parameters to perform maximum likelihood estimation\n\n. . .\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n*   This resembles the $k$-means algorithm!\n\n    *   The cluster centroids are chosen at random, and then recomputed/updated\n    \n    *   This is repeated until the cluster assignments stop changing\n    \n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n<center>    \n<iframe src=\"https://giphy.com/embed/kd9BlRovbPOykLBMqX\" width=\"270\" height=\"270\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n</center>\n\n:::\n:::\n    \n. . .\n\n*   Instead of assigning each point to a single cluster, we “softly” assign them so they contribute fractionally to each cluster\n\n\n## How does this relate back to clustering?\n\n. . .\n\nFrom the EM algorithm:  $\\hat{z}_{ik}$ is the estimated weight for observation $i$ belonging to cluster $k$ (i.e. soft membership)\n\n. . .\n\n  - assign observation $i$ to a cluster with the largest $\\hat{z}_{ik}$\n  \n  - measure cluster assignment __uncertainty__ of $\\displaystyle 1 - \\max_k \\hat{z}_{ik}$\n\n\n. . .\n\n__Our parameters determine the type of clusters__\n\n\n. . .\n\nIn the 1D case, there are two options:\n\n\n. . .\n\n1. each cluster __is assumed to have equal variance__ (spread): $\\sigma_1^2 = \\sigma_2^2 = \\dots = \\sigma_k^2$\n\n\n. . .\n\n2. each cluster __is allowed to have a different variance__\n\n\n. . .\n\n_But that is only 1D... what happens in multiple dimensions?_\n\n\n## Multivariate GMMs\n\n$$f(x) = \\sum_{k=1}^K \\tau_k f_k(x; \\theta_k) \\qquad \\text{where }f_k(x; \\theta_k) \\sim N(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n\n\nEach component is a __multivariate normal distribution__:\n\n\n. . .\n\n- $\\boldsymbol{\\mu}_k$ is a _vector_ of means in $p$ dimensions\n\n\n. . .\n\n- $\\boldsymbol{\\Sigma}_k$ is the $p \\times p$ __covariance__ matrix - describes the joint variability between pairs of variables\n\n$$\\sum=\\left[\\begin{array}{cccc}\n\\sigma_{1}^{2} & \\sigma_{1,2} & \\cdots & \\sigma_{1, p} \\\\\n\\sigma_{2,1} & \\sigma_{2}^{2} & \\cdots & \\sigma_{2, p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p, 1} & \\sigma_{p, 2}^{2} & \\cdots & \\sigma_{p}^{2}\n\\end{array}\\right]$$\n\n\n\n## Covariance constraints\n\n$$\\sum=\\left[\\begin{array}{cccc}\n\\sigma_{1}^{2} & \\sigma_{1,2} & \\cdots & \\sigma_{1, p} \\\\\n\\sigma_{2,1} & \\sigma_{2}^{2} & \\cdots & \\sigma_{2, p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{p, 1} & \\sigma_{p, 2}^{2} & \\cdots & \\sigma_{p}^{2}\n\\end{array}\\right]$$\n\n. . .\n\nAs the number of dimensions increases, model fitting and estimation become increasingly difficult\n\n. . .\n\nWe can use __constraints__ on multiple aspects of the $k$ covariance matrices:\n\n\n. . .\n\n- __volume__: size of the clusters, i.e., number of observations, \n\n- __shape__: direction of variance, i.e. which variables display more variance\n\n- __orientation__: aligned with axes (low covariance) versus tilted (due to relationships between variables)\n\n\n## Covariance constraints\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5096736/bin/nihms793803f2.jpg){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n- The three letters in the model name denote, in order, the volume, shape, and orientation across clusters\n\n- __E__: equal and __V__: varying (__VVV__ is the most flexible, but has the most parameters)\n\n- Two II: __spherical__, one I: __diagonal__, the remaining are __general__\n\n. . .\n\nHow do we know which one to choose?\n\n## Bayesian information criterion (BIC)\n\n__This is a statistical model__\n\n$$f(x) = \\sum_{k=1}^K \\tau_k f_k(x; \\theta_k) \\qquad \\text{where }f_k(x; \\theta_k) \\sim N(\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n\n\n. . .\n\nUse a __model selection__ procedure for determining which best characterizes the data\n\n\n. . .\n\nSpecifically, use a __penalized likelihood__ measure $$\\text{BIC} = 2\\log \\mathcal{L} - m\\log n$$\n\n- $\\log \\mathcal{L}$: log-likelihood of the considered model\n\n- with $m$ parameters (_VVV_ has the most parameters) and $n$ observations\n\n. . .\n\n- __penalizes__ large models with __many clusters without constraints__\n\n- __use BIC to choose the covariance constraints AND number of clusters__\n\n:::aside\nThe above $\\text{BIC}$ is really the $- \\text{BIC}$ of what you typically see, this sign flip is just for ease\n:::\n\n# Example\n\n## Data: NBA player statistics per 100 possessions (2023-24 regular season)\n\nObtained via [`ballr` package](https://github.com/rtelmore/ballr)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nnba_players <- read_csv(\"https://raw.githubusercontent.com/36-SURE/2024/main/data/nba_players.csv\")\nhead(nba_players)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 33\n     rk player   pos     age tm        g    gs    mp    fg   fga fgpercent   x3p\n  <dbl> <chr>    <chr> <dbl> <chr> <dbl> <dbl> <dbl> <dbl> <dbl>     <dbl> <dbl>\n1     1 Preciou… PF-C     24 TOT      74    18  1624   7.2  14.4     0.501   0.8\n2     2 Bam Ade… C        26 MIA      71    71  2416  10.9  21       0.521   0.3\n3     3 Ochai A… SG       23 TOT      78    28  1641   5.2  12.7     0.411   1.8\n4     4 Santi A… PF       23 MEM      61    35  1618   7.5  17.2     0.435   3.2\n5     5 Nickeil… SG       25 MIN      82    20  1921   6.1  13.8     0.439   3.4\n6     6 Grayson… SG       28 PHO      75    74  2513   6.6  13.3     0.499   4  \n# ℹ 21 more variables: x3pa <dbl>, x3ppercent <dbl>, x2p <dbl>, x2pa <dbl>,\n#   x2ppercent <dbl>, ft <dbl>, fta <dbl>, ftpercent <dbl>, orb <dbl>,\n#   drb <dbl>, trb <dbl>, ast <dbl>, stl <dbl>, blk <dbl>, tov <dbl>, pf <dbl>,\n#   pts <dbl>, x <lgl>, ortg <dbl>, drtg <dbl>, link <chr>\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n## Implementation with [`mclust`](https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html) package\n\nSelect the model and number of clusters\n\nUse `Mclust()` function to search over 1 to 9 clusters (default) and the different covariance constraints (i.e. models) \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mclust)\n# x3pa: 3pt attempts per 100 possessions\n# trb: total rebounds per 100 possessions\nnba_mclust <- nba_players |> \n  select(x3pa, trb) |> \n  Mclust()\nsummary(nba_mclust)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n---------------------------------------------------- \nGaussian finite mixture model fitted by EM algorithm \n---------------------------------------------------- \n\nMclust VVV (ellipsoidal, varying volume, shape, and orientation) model with 3\ncomponents: \n\n log-likelihood   n df      BIC       ICL\n      -2246.359 452 17 -4596.65 -4739.919\n\nClustering table:\n  1   2   3 \n 40 280 132 \n```\n\n\n:::\n:::\n\n\n\n\n## View clustering summary\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom)\nnba_mclust |> \n  tidy()  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  component  size proportion mean.x3pa mean.trb\n      <int> <int>      <dbl>     <dbl>    <dbl>\n1         1    40     0.0822     0.101    15.5 \n2         2   280     0.535      8.12      6.38\n3         3   132     0.382      6.04     11.0 \n```\n\n\n:::\n:::\n\n\n\n\n\n## View clustering assignments\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_mclust |> \n  augment() |> \n  ggplot(aes(x = x3pa, y = trb, color = .class, size = .uncertainty)) +\n  geom_point(alpha = 0.6) +\n  ggthemes::scale_color_colorblind()\n```\n\n::: {.cell-output-display}\n![](17-mixture_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n\n\n\n## Display the BIC for each model and number of clusters\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_mclust |> \n  plot(what = \"BIC\", \n       legendArgs = list(x = \"bottomright\", ncol = 4))\n```\n\n::: {.cell-output-display}\n![](17-mixture_files/figure-revealjs/nba-bic-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_mclust |> \n  plot(what = \"classification\")\n```\n\n::: {.cell-output-display}\n![](17-mixture_files/figure-revealjs/nba-cluster-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n:::\n\n## How do the cluster assignments compare to the positions?\n\nTwo-way table to compare the clustering assignments with player positions\n\n(What's the way to visually compare the two labels?)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntable(\"Clusters\" = nba_mclust$classification, \"Positions\" = nba_players$pos)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Positions\nClusters  C C-PF PF PF-C PF-SF PG PG-SG SF SF-PF SF-SG SG SG-PG\n       1 34    2  3    0     0  1     0  0     0     0  0     0\n       2  3    0 33    0     1 78     4 70     1     1 88     1\n       3 43    1 53    1     0  3     0 22     1     0  8     0\n```\n\n\n:::\n:::\n\n\n\n\nTakeaway: positions tend to fall within particular clusters\n\n\n## What about the cluster probabilities?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_player_probs <- nba_mclust$z\ncolnames(nba_player_probs) <- c(\"cluster1\", \"cluster2\", \"cluster3\")\n\nnba_player_probs <- nba_player_probs |>\n  as_tibble() |>\n  mutate(player = nba_players$player) |>\n  pivot_longer(!player, names_to = \"cluster\", values_to = \"prob\")\n\nnba_player_probs |>\n  ggplot(aes(prob)) +\n  geom_histogram() +\n  facet_wrap(~ cluster)\n```\n\n::: {.cell-output-display}\n![](17-mixture_files/figure-revealjs/nba-probs-1.png){fig-align='center' width=1728}\n:::\n:::\n\n\n\n\n\n\n## Which players have the highest uncertainty?\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n<br>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnba_players |>\n  mutate(cluster = nba_mclust$classification,\n         uncertainty = nba_mclust$uncertainty) |> \n  group_by(cluster) |>\n  slice_max(uncertainty, n = 5) |> \n  mutate(player = fct_reorder(player, uncertainty)) |> \n  ggplot(aes(x = uncertainty, y = player)) +\n  geom_point(size = 3) +\n  facet_wrap(~ cluster, scales = \"free_y\", nrow = 3)\n```\n:::\n\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-mixture_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n:::\n\n## Challenges and resources\n\n*   What if the data are not normal (ish) and instead are skewed?\n\n    *   Apply a transformation, then perform GMM\n    \n    *   GMM on principal components of data\n    \n    *   [Multivariate $t$ mixture model](https://link.springer.com/article/10.1007/s11222-011-9272-x)\n    \n*   [Review paper](https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-033121-115326): Model-Based Clustering\n\n*   [Book](https://math.univ-cotedazur.fr/~cbouveyr/MBCbook/): Model-Based Clustering and Classification for Data Science\n    \n*   [Adrian Raftery's website](https://sites.stat.washington.edu/raftery/Research/mbc.html)\n    \n## Appendix: code to build dataset\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ballr)\nnba_players <- NBAPerGameStatisticsPer100Poss(season = 2024)\nnba_players <- nba_players |> \n  group_by(player) |> \n  slice_max(g) |> \n  ungroup() |> \n  filter(mp >= 150) # keep players with at least 150 minutes played\n```\n:::\n",
    "supporting": [
      "17-mixture_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}