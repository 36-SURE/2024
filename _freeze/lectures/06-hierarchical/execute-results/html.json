{
  "hash": "512eec621aa3990cb6e895429b23015f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unsupervised learning: hierarchical clustering\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io](https://36-sure.github.io)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n# Background\n\n## The big picture\n\n* $k$-means clustering: partition the observations into a pre-specified number of clusters\n\n. . .\n\n* Hierarchical clustering: does not require commitment to a particular choice of clusters\n\n  *   In fact, we end up with a tree-like visual representation of the observations, called a dendrogram\n  \n  *   This allows us to view at once the clusterings obtained for each possible number of clusters\n  \n  *   Common approach: agglomerative (bottom-up) hierarchical clustering: build a dendrogram starting from the leaves and combining clusters up to the trunk\n  \n  *   There's also divisive (top-down) hierarchical clustering: start with one large cluster and then break the cluster recursively into smaller and smaller pieces\n\n## Data: County-level health indicators for Utah in 2014\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nutah_health <- read_csv(\"https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/utah_health.csv\")\nglimpse(utah_health)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 30\nColumns: 20\n$ County                 <chr> \"Tooele\", \"Uintah\", \"Wasatch\", \"Statewide\", \"Sa…\n$ Population             <dbl> 59870, 34524, 25273, 2855287, 27906, 1524, 7221…\n$ PercentUnder18         <dbl> 35.3, 33.6, 33.1, 31.1, 29.4, 27.8, 23.4, 33.6,…\n$ PercentOver65          <dbl> 7.9, 9.1, 9.1, 9.5, 12.3, 23.7, 20.5, 11.0, 10.…\n$ DiabeticRate           <dbl> 9, 8, 6, 7, 8, 10, 10, 8, 8, 7, 7, 9, 6, 4, 9, …\n$ HIVRate                <dbl> 37, 21, 28, 111, 22, NA, NA, NA, 72, 36, NA, NA…\n$ PrematureMortalityRate <dbl> 347.6, 396.2, 227.2, 286.7, 314.3, 445.8, 293.6…\n$ InfantMortalityRate    <dbl> 5.0, 5.2, NA, 5.0, NA, NA, NA, NA, 5.8, 7.1, NA…\n$ ChildMortalityRate     <dbl> 47.7, 57.4, 69.0, 52.9, 61.2, NA, NA, 70.4, 58.…\n$ LimitedAccessToFood    <dbl> 14, 14, 14, 17, 17, 15, 14, 14, 16, 19, 11, 18,…\n$ FoodInsecure           <dbl> 7, 5, 1, 5, 4, 41, 16, 8, 6, 12, 0, 14, 4, 4, 8…\n$ MotorDeathRate         <dbl> 18, 23, 16, 11, 18, NA, 21, 36, 10, 13, NA, NA,…\n$ DrugDeathRate          <dbl> 18, 13, 17, 17, 16, NA, 26, 18, 20, 18, NA, NA,…\n$ Uninsured              <dbl> 17, 24, 24, 20, 24, 26, 19, 23, 20, 26, 14, 26,…\n$ UninsuredChildren      <dbl> 10, 16, 16, 11, 15, 17, 12, 14, 12, 15, 10, 20,…\n$ HealthCareCosts        <dbl> 9095, 7086, 8327, 8925, 8942, 7824, 8121, 8527,…\n$ CouldNotSeeDr          <dbl> 13, 12, 13, 13, 13, NA, NA, 13, 11, 13, 13, NA,…\n$ MedianIncome           <dbl> 61927, 60419, 62014, 57067, 43921, 36403, 43128…\n$ ChildrenFreeLunch      <dbl> 34, 36, 30, 31, 38, 58, 31, 34, 37, 40, 12, 33,…\n$ HomicideRate           <dbl> NA, NA, NA, 2, NA, NA, NA, NA, 2, NA, NA, NA, 1…\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## General setup\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n-   Given a dataset with $p$ variables (columns) and $n$ observations (rows) $x_1,\\dots,x_n$\n\n-   Compute the **distance/dissimilarity** between observations\n\n-   e.g. **Euclidean distance** between observations $i$ and $j$\n\n$$d(x_i, x_j) = \\sqrt{(x_{i1}-x_{j1})^2 + \\cdots + (x_{ip}-x_{jp})^2}$$\n\n**What are the distances between these counties using `PercentOver65` (percent of county population that is 65 and over) and `DiabeticRate` (prevalence of diabetes)?**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n<br>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nutah_health |> \n  ggplot(aes(x = PercentOver65, y = DiabeticRate)) +\n  geom_point(size = 4)\n```\n\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Remember to standardize!\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nutah_health <- utah_health |> \n  mutate(\n    std_pct_over65 = as.numeric(scale(PercentOver65)),\n    std_diabetic_rate = as.numeric(scale(DiabeticRate))\n  )\n\nutah_health |> \n  ggplot(aes(x = std_pct_over65, y = std_diabetic_rate)) +\n  geom_point(size = 4) +\n  coord_fixed()\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Compute the distance matrix using `dist()`\n\n-   Compute pairwise Euclidean distance\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncounty_dist <- utah_health |> \n  select(std_pct_over65, std_diabetic_rate) |> \n  dist()\n```\n:::\n\n\n\n\n-   Returns an object of `dist` class... but not a `matrix`\n\n-   Convert to a matrix, then set the row and column names:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncounty_dist_matrix <- as.matrix(county_dist)\nrownames(county_dist_matrix) <- utah_health$County\ncolnames(county_dist_matrix) <- utah_health$County\ncounty_dist_matrix[1:4, 1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Tooele    Uintah   Wasatch Statewide\nTooele    0.0000000 0.7474104 2.1010897 1.4367342\nUintah    0.7474104 0.0000000 1.3885164 0.7003632\nWasatch   2.1010897 1.3885164 0.0000000 0.7003632\nStatewide 1.4367342 0.7003632 0.7003632 0.0000000\n```\n\n\n:::\n:::\n\n\n\n\n-   Convert to a long table with `pivot_longer` for plotting purpose\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlong_dist_matrix <- county_dist_matrix |> \n  as_tibble() |> \n  mutate(county1 = rownames(county_dist_matrix)) |> \n  pivot_longer(cols = !county1, names_to = \"county2\", values_to = \"distance\")\n```\n:::\n\n\n\n\n## This heatmap is useless...\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n<br>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlong_dist_matrix |> \n  ggplot(aes(x = county1, y = county2, fill = distance)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"darkorange\", \n                      high = \"darkblue\") +\n  coord_fixed() +\n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank())\n```\n:::\n\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n## Arrange heatmap with `seriation`\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n<br>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(seriation)\ncounty_dist_seriate <- seriate(county_dist)\ncounty_order <- get_order(county_dist_seriate)\ncounty_names_order <- utah_health$County[county_order]\nlong_dist_matrix |> \n  mutate(\n    county1 = fct_relevel(county1, county_names_order),\n    county2 = fct_relevel(county2, county_names_order)\n  ) |> \n  ggplot(aes(x = county1, y = county2, fill = distance)) +\n  scale_fill_gradient(low = \"darkorange\", \n                      high = \"darkblue\") +\n  geom_tile() +\n  coord_fixed() +\n  theme(axis.text = element_blank(), \n        axis.ticks = element_blank())\n```\n:::\n\n\n\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n:::\n\n:::\n\n# Hierarchical clustering\n\n## (Agglomerative) [Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n\nLet's pretend all $n$ observations are in their own cluster\n\n. . .\n\n-   Step 1: Compute the pairwise dissimilarities between each cluster\n\n    -   e.g., distance matrix on previous slides\n\n. . .\n\n-   Step 2: Identify the pair of clusters that are **least dissimilar**\n\n. . .\n\n-   Step 3: Fuse these two clusters into a new cluster!\n\n. . .\n\n-   **Repeat Steps 1 to 3 until all observations are in the same cluster**\n\n. . .\n\n**\"Bottom-up\"**, agglomerative clustering that forms a **tree/hierarchy** of merging\n\nNo mention of any randomness. And no mention of the number of clusters $k$.\n\n## (Agglomerative) [Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nStart with all observations in their own cluster\n\n-   Step 1: Compute the pairwise dissimilarities between each cluster\n\n-   Step 2: Identify the pair of clusters that are **least dissimilar**\n\n-   Step 3: Fuse these two clusters into a new cluster!\n\n-   **Repeat Steps 1 to 3 until all observations are in the same cluster**\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Clusters.svg/250px-Clusters.svg.png){fig-align='center' width=70%}\n:::\n:::\n\n\n\n:::\n:::\n\n## (Agglomerative) [Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nStart with all observations in their own cluster\n\n-   Step 1: Compute the pairwise dissimilarities between each cluster\n\n-   Step 2: Identify the pair of clusters that are **least dissimilar**\n\n-   Step 3: Fuse these two clusters into a new cluster!\n\n-   **Repeat Steps 1 to 3 until all observations are in the same cluster**\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ad/Hierarchical_clustering_simple_diagram.svg/418px-Hierarchical_clustering_simple_diagram.svg.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n\nForms a **dendrogram** (typically displayed from bottom-up)\n:::\n:::\n\n## Dissimilarity between clusters\n\n* We know how to compute distance/dissimilarity between two observations\n\n* **But how do we handle clusters?**\n\n  *   Dissimilarity between a cluster and an observation, or between two clusters\n\n. . .\n\nWe need to choose a **linkage function**. Clusters are built up by **linking them together**\n\n\n## Types of linkage\n\nFirst, compute all pairwise dissimilarities between the observations in the two clusters\n\ni.e., compute the distance matrix between observations, $d(x_i, x_j)$ for $i \\in C_1$ and $j \\in C_2$\n\n. . .\n\n-   **Complete linkage**: use the **maximum** (largest) value of these dissimilarities \\hfill $\\underset{i \\in C_1, j \\in C_2}{\\text{max}} d(x_i, x_j)$ (**maximal** inter-cluster dissimilarity)\n\n. . .\n\n-   **Single linkage**: use the **minimum** (smallest) value of these dissimilarities \\hfill $\\underset{i \\in C_1, j \\in C_2}{\\text{min}} d(x_i, x_j)$ (**minimal** inter-cluster dissimilarity)\n\n. . .\n\n-   **Average linkage**: use the **average** value of these dissimilarities \\hfill $\\displaystyle \\frac{1}{|C_1||C_2|} \\sum_{i \\in C_1} \\sum_{j \\in C_2} d(x_i, x_j)$ (**mean** inter-cluster dissimilarity)\n\n. . .\n\n\n## Complete linkage example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n-   Use `hclust()` with a `dist()` objsect\n\n-   Use `complete` linkage by default\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nutah_complete <- county_dist |> \n  hclust(method = \"complete\")\n```\n:::\n\n\n\n\n-   Use `cutree()` to return cluster labels\n\n-   Returns compact clusters (similar to $k$-means)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nutah_health |> \n  mutate(\n    cluster = as.factor(cutree(utah_complete, k = 3))\n  ) |>\n  ggplot(aes(x = std_pct_over65, y = std_diabetic_rate,\n             color = cluster)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## What are we cutting? Dendrograms\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nUse the [`ggdendro`](https://cran.r-project.org/web/packages/ggdendro/index.html) package (instead of `plot()`)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggdendro)\nutah_complete |> \n  ggdendrogram(labels = FALSE, \n               leaf_labels = FALSE,\n               theme_dendro = FALSE) +  \n  labs(y = \"Dissimilarity between clusters\") +\n  theme(axis.text.x = element_blank(), \n        axis.title.x = element_blank(),\n        panel.grid = element_blank())\n```\n:::\n\n\n\n\n-   Each **leaf** is one observation\n\n-   **Height of branch indicates dissimilarity between clusters**\n\n    -   (After first step) Horizontal position along x-axis means nothing\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n------------------------------------------------------------------------\n\n## [Textbook example](https://bradleyboehmke.github.io/HOML/hierarchical.html)\n\n<br>\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://bradleyboehmke.github.io/HOML/19-hierarchical_files/figure-html/comparing-dendrogram-to-distances-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n------------------------------------------------------------------------\n\n## Cut dendrograms to obtain cluster labels\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nSpecify the height to cut with `h` (instead of `k`)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/complete-dendro-cut-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nFor example, `cutree(utah_complete, h = 4)`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/health-complete-cut-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Single linkage example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nChange the `method` argument to `single`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/single-dendro-cut-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nResults in a **chaining** effect\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/health-single-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Average linkage example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nChange the `method` argument to `average`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/average-dendro-cut-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nCloser to `complete` but varies in compactness\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/health-average-plot-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## More linkage functions\n\n-   **Centroid linkage**: Computes the dissimilarity between the centroid for cluster 1 and the centroid for cluster 2\n\n    -   i.e. distance between the averages of the two clusters\n\n    -   use `method = centroid`\n\n. . .\n\n-   **Ward's linkage**: Merges a pair of clusters to minimize the within-cluster variance\n\n    -   i.e. aim is to minimize the objection function from $K$-means\n\n    -   can use `ward.D` or `ward.D2` (different algorithms)\n\n. . .\n\n-   There's another one...\n\n## [Minimax linkage](https://faculty.marshall.usc.edu/jacob-bien/papers/jasa2011minimax.pdf)\n\n<!-- -   **Identify the point whose farthest point is closest** (hence the minimax) -->\n\n-   Each cluster is defined by a **prototype** observation (most representative)\n\n-   The prototype is \"minimally dissimilar\" from every point in the cluster (hence the \"minimax\")\n\n-   Dendrogram interpretation: each point is $\\leq h$ in dissimilarity to the **prototype** of cluster\n\n-   **Cluster centers are chosen among the observations themselves - hence prototype**\n\n## Minimax linkage \n\n-   For each point belonging to either cluster, find the maximum distance between it and all the other points in the two clusters. \n\n-   The smallest of these maximum distances (\"minimal-maximum\" distance) is defined as the distance between the two clusters\n\n-   The distance to prototype is measured by the maximum minimax radius\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://europepmc.org/articles/PMC4527350/bin/nihms637357f2.jpg){fig-align='center' width=20%}\n:::\n:::\n\n\n\n\n\n## Minimax linkage example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n-   Easily done in `R` via the [`protoclust`](https://github.com/jacobbien/protoclust) package\n\n-   Use the `protoclust()` function to apply the clustering to the `dist()` object\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(protoclust)\nutah_minimax <- protoclust(county_dist_matrix)\n\nutah_minimax |> \n  as.hclust() |>\n  as.dendrogram() |> \n  ggdendrogram(labels = FALSE, \n               leaf_labels = FALSE,\n               theme_dendro = FALSE) +  \n  labs(y = \"Dissimilarity between clusters\") +\n  theme(axis.text.x = element_blank(), \n        axis.title.x = element_blank(),\n        panel.grid = element_blank())\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\nconverting to dist (note: ignores above diagonal)\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-31-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Minimax linkage example\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n-   Use the `protocut()` function to make the cut\n\n-   But then access the cluster labels `cl`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nminimax_county_clusters <- utah_minimax |> \n  protocut(k = 3)\nutah_health |> \n  mutate(cluster = as.factor(minimax_county_clusters$cl)) |>\n  ggplot(aes(x = std_pct_over65, y = std_diabetic_rate,\n             color = cluster)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-33-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n\n## Minimax linkage example\n\n-   Want to check out the prototypes for the three clusters\n\n-   `protocut` returns the indices of the prototypes (in order of the cluster labels)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nminimax_county_clusters$protos\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18 13  7\n```\n\n\n:::\n:::\n\n\n\n\n-   Subset the rows for these counties using `slice()`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nutah_health |>\n  select(County, std_pct_over65, std_diabetic_rate) |>\n  slice(minimax_county_clusters$protos)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  County std_pct_over65 std_diabetic_rate\n  <chr>           <dbl>             <dbl>\n1 Emery          0.0600             0.116\n2 Davis         -0.978             -1.27 \n3 Kane           1.74               1.50 \n```\n\n\n:::\n:::\n\n\n\n\n## Post-clustering analysis\n\n* For context, how does population relate to our clustering results?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nutah_health |> \n  mutate(cluster = as.factor(minimax_county_clusters$cl)) |> \n  ggplot(aes(x = log(Population), fill = cluster)) +\n  geom_density(alpha = 0.1) +\n  scale_fill_manual(values = c(\"darkblue\", \"purple\", \"gold\", \"orange\"))\n```\n\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-36-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Post-clustering analysis\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# utah_health |> \n#   arrange(Population)\nlibrary(dendextend)\nutah_minimax |> \n  as.hclust() |>\n  as.dendrogram() |> \n  set(\"branches_k_color\", k = 3) |> \n  set(\"labels_col\", k = 3) |> \n  ggplot(horiz = TRUE)\n```\n:::\n\n\n\n\n* Different population levels tend to fall within particular clusters...\n\n* It's easy to **include more variables** - just change the distance matrix\n\n:::\n\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](06-hierarchical_files/figure-revealjs/unnamed-chunk-38-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n## Practical issues\n\n* What dissimilarity measure should be used?\n\n* What type of linkage should be used?\n\n* How many clusters to choose?\n\n* Which features should we use to drive the clustering?\n  \n  *   Categorical variables?\n\n* Hard clustering vs. soft clustering \n\n  *   Hard clustering ($k$-means, hierachical): assigns each observation to exactly one cluster\n  \n  *   Soft (fuzzy) clustering: assigns each observation a probability of belonging to a cluster\n\n<center>\n\n<font size=\"+4\">\n\nIT DEPENDS...\n\n</font>\n\n</center>",
    "supporting": [
      "06-hierarchical_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}