{
  "hash": "0cf7d0a5368e45f3e0a9fc008a17767e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Supervised learning: multinomial classification\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n# Binary classification\n\n## Review: binary classification\n\n*   Classification: predicting a categorical response for an observation (since it involves assigning the observation to a category/class)\n\n*   Binary classification: response is binary\n\n*   Often, we are more interested in estimating the probability for each category of the response\n\n*   Methods: logistic regression, GAMs, tree-based methods, to name a few\n\n## Evaluating the prediction threshold\n\n*   For each observation, obtain predicted class by comparing the predicted probability to some cutoff $c$ (typically, $c=0.5$) $$\\hat{Y} =\\left\\{\\begin{array}{ll} 1, & \\hat{p}(x)>c \\\\ 0, & \\hat{p}(x) \\leq c \\end{array}\\right.$$\n\n. . .\n\n*   Given the classifications, we can form a confusion matrix:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://www.researchgate.net/publication/358927129/figure/fig1/AS:11431281104485755@1670091775265/Gambar-5-Confusion-matrix-model37-Dari-gambar-5-menggambarkan-True-Positive-TP-yang.png){fig-align='center' width=30%}\n:::\n:::\n\n\n\n\n## Confusion matrix\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://www.researchgate.net/publication/358927129/figure/fig1/AS:11431281104485755@1670091775265/Gambar-5-Confusion-matrix-model37-Dari-gambar-5-menggambarkan-True-Positive-TP-yang.png){fig-align='center' width=120%}\n:::\n:::\n\n\n\n\nWe want to __maximize__ all of the following (positive means 1, negative means 0):\n\n- __Accuracy__: How often is the classifier correct? $(\\text{TP} + \\text{TN}) \\ / \\ {\\text{total}}$\n- __Precision__: How often is it right for predicted positives? $\\text{TP} \\ / \\  (\\text{TP} + \\text{FP})$\n- __Sensitivity__ (or true positive rate (TPR) or power): How often does it detect positives? $\\text{TP} \\ / \\  (\\text{TP} + \\text{FN})$\n- __Specificity__ (or true negative rate (TNR), or 1 - false positive rate (FPR)): How often does it detect negatives? $\\text{TN}  \\ / \\ (\\text{TN} + \\text{FP})$\n\n## Receiver operating characteristic (ROC) curve\n\nQuestion: How do we balance between high power and low false positive rate?\n\n. . .\n\n* Check all possible values for the cutoff $c$, plot the power against false positive rate\n\n* Want to maximize the __area under the curve (AUC)__\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://bradleyboehmke.github.io/HOML/02-modeling-process_files/figure-html/modeling-process-roc-1.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n## Plotting an ROC curve\n\n*   Example: MLB home run probability\n\n*   Model: logistic regression with exit velocity and launch angle as predictors\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nbatted_balls <- read_csv(\"https://raw.githubusercontent.com/36-SURE/2024/main/data/batted_balls.csv\")\nhr_logit <- glm(is_hr ~ launch_speed + launch_angle, family = binomial, data = batted_balls)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(pROC)\nhr_roc <- batted_balls |> \n  mutate(pred_hr = predict(hr_logit, type = \"response\")) |> \n  roc(is_hr, pred_hr)\n# str(hr_roc)\nhr_roc$auc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nArea under the curve: 0.9406\n```\n\n\n:::\n:::\n\n\n\n\n## Plotting an ROC curve\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(threshold = hr_roc$thresholds,\n       specificity = hr_roc$specificities,\n       sensitivity = hr_roc$sensitivities) |> \n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](21-multinomial_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n\n# Multinomial classification\n\n## Motivating example: expected points in American football\n\nFootball is complicated --- **not all yards are created equal**\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-multinomial_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=1920}\n:::\n:::\n\n\n\n\n## Play evaluation in American football\n\n* How do we overcome the limitations of yards gained?\n\n. . .\n\n* Expected points: how many points have teams scored in similar situations?\n\n. . .\n\n* Win probability: have teams in similar situations won the game?\n\n## NFL play-by-play data\n\nEach row is a play with contextual information:\n\n* Possession team: team with the ball, on offense (opposing team is on defense)\n\n. . .\n\n* Down: 4 downs to advance the ball 10 (or more) yards\n\n  *   New set of downs, else turnover to defense\n  \n. . .  \n\n* Yards to go: distance in yards to go for a first down\n\n. . .\n\n\n* Yard line: distance in yards away from opponent's endzone (100 to 0) - the field position\n\n. . .\n\n* Time remaining: seconds remaining in game, each game is 3600 seconds long\n\n  *   4 quarters, halftime in between, followed by a potential overtime\n  \n## NFL play-by-play data  \n\nDrive: a series of plays, changes with possession and the types of scoring events:\n\n*   No Score: 0 points - turnover the ball or half/game ends\n\n*   Field Goal: 3 points - kick through opponent's goal post\n\n*   Touchdown: 7 points - enter opponent's end zone\n\n*   Safety: 2 points for opponent - tackled in own endzone\n\n. . .\n\nNext score: type of next score with respect to possession team\n\n*   For: Touchdown (7), Field Goal (3), Safety (2)\n\n*   Against: -Touchdown (-7), -Field Goal (-3), -Safety (-2)\n\n*   No Score (0)\n\n## Expected points\n\n*   __Expected points:__ Measure the value of play in terms of $\\mathbb{E}[\\text{points of next scoring play}]$\n\n    * - i.e., historically, how many points have teams scored when in similar situations?\n    \n. . .\n\n*   __Response__: $Y \\in$ {touchdown, field goal, safety, no score, -safety, -field goal, -touchdown} \n\n*   __Predictors__: $\\mathbf{X} =$ {down, yards to go, yard line, ...}\n\n. . .\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: center;\"}\n\n| $Y$         | value | probability  |\n|-------------|-------|--------------|\n| touchdown   | 7     |              |\n| field goal  | 3     |              |\n| safety      | 2     |              |\n| no score    | 0     |              |\n| -safety     | -2    |              |\n| -field goal | -3    |              |\n| -touchdown  | -7    |              |\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n*   Task: __estimate the probabilities__ of each scoring event to compute expected points\n\n- Outcome probabilities $$P(Y = y \\mid  \\mathbf{X})$$\n\n- Expected points $$E(Y \\mid \\mathbf{X}) = \\sum_{y \\in Y} y \\cdot P(Y=y \\mid\\mathbf{X})$$\n\n:::\n:::\n\n. . .\n\n**Important question: Which model do we use when a categorical response variable has more than two classes?**\n\n## Review: logistic regression \n\nResponse variable $Y$ is binary categorical with two possible values: 1 or 0<br>(i.e. binary classification problem)\n\nEstimate the probability\n\n$$\np(x) = P(Y = 1 \\mid X = x)\n$$\n\nAssuming that we are dealing with two classes, the possible observed values for $Y$ are 0 and 1, \n$$\nY \\mid x \\sim {\\rm Binomial}(n=1,p=\\mathbb{E}[Y \\mid x])\n$$\n\n. . .\n\nTo limit the regression between $[0, 1]$, use the __logit__ function (i.e., the __log-odds ratio__)\n\n$$\n\\log \\left[ \\frac{p(x)}{1 - p(x)} \\right] = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p\n$$\n\n. . .\n\nmeaning\n\n$$p(x) = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p}}$$\n\n\n## [Multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression)\n\nExtend logistic regression to $K$ classes (via the [softmax function](https://en.wikipedia.org/wiki/Softmax_function)):\n\n\n$$P(Y=k^* \\mid X=x)=\\frac{e^{\\beta_{0 k^*}+\\beta_{1 k^*} x_{1}+\\cdots+\\beta_{p k^*} x_{p}}}{\\sum_{k=1}^{K} e^{\\beta_{0 k}+\\beta_{1 k} x_{1}+\\cdots+\\beta_{p k} x_{p}}}$$\n\n. . .\n\nEstimate coefficients for $K - 1$ classes __relative to reference class__\n\nFor example, let $K$ be the reference then we use $K - 1$ logit transformations\n  \nNotation: $\\boldsymbol{\\beta}$ for vector of coefficients and $\\mathbf{X}$ for matrix of predictors\n  \n$$\\begin{array}{c}\n\\log \\Big( \\frac{P(Y = 1 \\ \\mid \\ \\mathbf{X})}{P(Y=K \\ \\mid \\ \\mathbf{X})} \\Big) = \\boldsymbol{\\beta}_{1} \\cdot \\mathbf{X}  \\\\\n\\log \\Big( \\frac{P(Y=2 \\ \\mid \\ \\mathbf{X})}{P(Y=K \\ \\mid \\ \\mathbf{X})} \\Big) =\\boldsymbol{\\beta}_{2} \\cdot \\mathbf{X} \\\\\n\\vdots \\\\\n\\log \\Big( \\frac{P(Y=K-1 \\ \\mid \\ \\mathbf{X})}{P(Y=K \\ \\mid \\ \\mathbf{X})} \\Big) =\\boldsymbol{\\beta}_{K-1} \\cdot \\mathbf{X}\n\\end{array}$$\n\n---\n\n## [Multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) for the next scoring event\n\n$Y \\in$ {touchdown, field goal, safety, no score, -safety, -field goal, -touchdown}\n\n$\\mathbf{X} =$ {down, yards to go, yard line, ...} \n\n. . .\n\nModel is specified with __six logit transformations__ relative to __no score__:\n\n$$\\begin{array}{c}\n\\log \\left(\\frac{P(Y = \\text { touchdown } \\mid \\ \\mathbf{X})}{P(Y = \\text { no score } \\mid \\  \\mathbf{X})}\\right)=\\mathbf{X} \\cdot \\boldsymbol{\\beta}_{\\text {touchdown }} \\\\\n\\log \\left(\\frac{P(Y = \\text { field goal } \\mid \\ \\mathbf{X})}{P(Y=\\text { no score } \\mid \\ \\mathbf{X})}\\right)=\\mathbf{X} \\cdot \\boldsymbol{\\beta}_{\\text {field goal }}, \\\\\n\\vdots & \\\\ \\log \\left(\\frac{P(Y = -\\text {touchdown } \\mid \\ \\mathbf{X})}{P(Y = \\text { no score } \\mid \\  \\mathbf{X})}\\right)=\\mathbf{X} \\cdot \\boldsymbol{\\beta}_{-\\text {touchdown }},\n\\end{array}$$\n\n. . .\n\nOutput: predicted probability associated with each of the next scoring events for each play\n\n## Example\n\nNFL play-by-play dataset (2012-2022) with the next score (in a half) for each play\n\nFollowing steps provided by [`nflfastR`](https://github.com/nflverse/nflverse-pbp/blob/master/models/model_data.R), which follows the OG [`nflscrapR`](https://github.com/ryurko/nflscrapR-models/blob/master/R/init_models/init_ep_fg_models.R)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnfl_pbp <- read_csv(\"https://github.com/36-SURE/2024/raw/main/data/nfl_pbp.csv.gz\")\n# glimpse(nfl_pbp)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# additional data prep\nnfl_pbp <- nfl_pbp |> \n  # make No_Score the reference level\n  mutate(next_score_half = fct_relevel(next_score_half, \"No_Score\"),\n         log_ydstogo = log(ydstogo),\n         down = factor(down))\n```\n:::\n\n\n\n\n\n## Fitting a multinomial logistic regression model\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(nnet)\nep_model <- multinom(next_score_half ~ half_seconds_remaining + yardline_100 + down + log_ydstogo + \n                       log_ydstogo * down + yardline_100 * down, \n                     data = nfl_pbp, maxit = 300)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# weights:  98 (78 variable)\ninitial  value 820568.904843 \niter  10 value 644396.481292\niter  20 value 628421.265697\niter  30 value 614354.879384\niter  40 value 601877.827175\niter  50 value 599094.485735\niter  60 value 582578.628173\niter  70 value 563639.418072\niter  80 value 555744.384886\niter  90 value 553787.681372\niter 100 value 553756.085367\nfinal  value 553755.982007 \nconverged\n```\n\n\n:::\n\n```{.r .cell-code}\n# maxit = 300: provide a sufficient number of steps for model fitting\n# summary(ep_model)\n```\n:::\n\n\n\n\nIn the summary, notice the usual type of output, including coefficient estimates for every next scoring event, except for the reference level `No_Score`\n\n## Obtain probabilities for each categories\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(broom)\nevent_prob <- ep_model |> \n  predict(newdata = nfl_pbp, type = \"probs\") |> \n  as_tibble() |> \n  mutate(ep = Touchdown * 7 + Field_Goal * 3 + Safety * 2 + Opp_Touchdown * -7 + Opp_Field_Goal * -3 + Opp_Safety * -2)\nevent_prob\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 421,689 × 8\n   No_Score Field_Goal Opp_Field_Goal Opp_Safety Opp_Touchdown  Safety Touchdown\n      <dbl>      <dbl>          <dbl>      <dbl>         <dbl>   <dbl>     <dbl>\n 1 0.000944      0.191         0.169   0.00630          0.302  0.00415     0.326\n 2 0.00102       0.188         0.164   0.00433          0.284  0.00434     0.354\n 3 0.00116       0.153         0.180   0.00459          0.301  0.00490     0.356\n 4 0.00128       0.232         0.136   0.00259          0.234  0.00415     0.391\n 5 0.000787      0.297         0.0709  0.000295         0.118  0.00378     0.509\n 6 0.000871      0.317         0.0844  0.000513         0.144  0.00439     0.449\n 7 0.00110       0.350         0.0990  0.000686         0.169  0.00483     0.375\n 8 0.000780      0.321         0.0476  0.0000898        0.0768 0.00334     0.550\n 9 0.000875      0.355         0.0563  0.000175         0.0941 0.00397     0.489\n10 0.000570      0.338         0.0291  0.0000221        0.0459 0.00284     0.583\n# ℹ 421,679 more rows\n# ℹ 1 more variable: ep <dbl>\n```\n\n\n:::\n:::\n\n\n\n\n## Models in production\n\n*   It’s naive to assess the model on the training data it was learned on \n\n. . .\n\n*   How is our model going to be used in practice?\n\n. . .\n\n*   The model itself will be a functional tool that can be applied to new observations without the need to retrain (for a considerable amount of time)\n\n    *   e.g., built an expected points model and want to use it new season\n    \n. . .    \n    \n*   Constant retraining is unnecessary and will become computationally burdensome\n\n. . .\n\n*   Ultimate goal: have a model that generalizes well to new data\n\n## Review: $k$-fold cross-validation\n\n* Randomly split data into $K$ equal-sized folds (i.e., subsets of data)\n\n. . .\n\n* Then for each fold $k$ in 1 to $K$:\n\n  *   Train the model on data excluding observations in fold $k$\n\n  *   Generate predictions on holdout data in fold k\n\n  *   (Optional) Compute some performance measure for fold $k$ predictions (e.g., RMSE, accuracy)\n  \n. . .\n\n* Aggregate holdout predictions to evaluate model performance\n\n. . .\n\n* (Optional) Aggregate performance measure across $K$ folds\n\n  *   e.g., compute average RMSE, including standard error for CV estimate\n  \n. . .  \n\nTakeaway: We can assess our model performance using anything we want with cross-validation\n\n## Review: calibration (for classification)\n\nKey idea: For a classifier to be calibrated, the actual probabilities should match the predicted probabilities\n\n. . .\n\nProcess\n\n*   Bin the data according to predicted probabilities from model\n\n    *   e.g., [0, 0.1], (0.1, 0.2], ..., (0.9, 1]\n\n. . .    \n\n*   For each bin, compute proportion of observations whose class is the positive class\n\n. . .\n\n*   Plot observed proportions against the predicted probabilities bin midpoints    \n\n. . .\n\nVisualization: calibration curves (or reliability diagrams)\n    \n\n## Cross-validation calibration\n\n*   Randomly split data into $K$ equal-sized folds (i.e., subsets of data)\n\n. . .\n\n*   Then for each fold $k$ in 1 to $K$:\n\n    *   Train a classifier on data excluding observations in fold $k$\n\n    *   Generate predicted probabilities on holdout data in fold $k$\n    \n. . .    \n\n*   Aggregate holdout probabilities to create calibration plot\n\n. . .\n\n*   Alternatively: Create calibration plot for each holdout fold separately\n\n    *   Could average across bin calibration estimates<br>(though, standard errors might be messed up)\n\n\n## Cross-validation for next scoring event model\n\n* Can we randomly split data into $K$ equal-sized folds?\n\n. . .\n\n* Why not? Observations are correlated (at the play-level)\n\n. . .\n\n* For model evaluation purposes, we cannot randomly assign plays to different training and test folds\n\n. . .\n\n* Alternative ideas? Randomly assign game-drives, game-halves, games, weeks\n\n. . .\n\n* Another idea that avoids randomness: leave-one-season-out cross validation\n\n## Leave-one-season-out calibration\n\n* Since the model outputs probability estimates, use out-of-sample calibration to evaluate the model\n\n* Assess how well the model is calibrated for each scoring event\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nep_cv <- function(s) {\n  \n  test <- nfl_pbp |> filter(season == s)\n  train <- nfl_pbp |> filter(season != s)\n\n  ep_model <- multinom(next_score_half ~ half_seconds_remaining + yardline_100 + down + \n                         log_ydstogo + log_ydstogo * down + yardline_100 * down, \n                       data = train, maxit = 300)\n  ep_out <- ep_model |> \n    predict(newdata = test, type = \"probs\") |> \n    as_tibble() |> \n    mutate(next_score_half = test$next_score_half,\n           season = s)\n  return(ep_out)\n}\n\nseasons <- unique(nfl_pbp$season)\nep_cv_calibration <- seasons |> \n  map(ep_cv) |> \n  bind_rows()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# weights:  98 (78 variable)\ninitial  value 745900.440604 \niter  10 value 582572.386661\niter  20 value 567612.722519\niter  30 value 553128.934420\niter  40 value 539573.367519\niter  50 value 537142.707480\niter  60 value 530470.755017\niter  70 value 514487.037163\niter  80 value 505049.356679\niter  90 value 503664.462255\niter 100 value 503645.015371\nfinal  value 503644.994544 \nconverged\n# weights:  98 (78 variable)\ninitial  value 745384.774415 \niter  10 value 579676.888404\niter  20 value 565801.534090\niter  30 value 551821.702311\niter  40 value 538083.862911\niter  50 value 535516.763693\niter  60 value 522268.990541\niter  70 value 510214.212169\niter  80 value 503279.229245\niter  90 value 502419.020138\niter 100 value 502408.109824\nfinal  value 502408.057516 \nconverged\n# weights:  98 (78 variable)\ninitial  value 746102.815260 \niter  10 value 581128.133082\niter  20 value 566363.466914\niter  30 value 553159.787960\niter  40 value 538887.792660\niter  50 value 536394.121640\niter  60 value 517227.685617\niter  70 value 508543.818882\niter  80 value 504334.318742\niter  90 value 503192.902124\niter 100 value 503174.592780\nfinal  value 503174.526755 \nconverged\n# weights:  98 (78 variable)\ninitial  value 745509.312664 \niter  10 value 579618.006957\niter  20 value 564306.403454\niter  30 value 551208.139857\niter  40 value 538465.941964\niter  50 value 536710.560522\niter  60 value 528973.377193\niter  70 value 511260.471224\niter  80 value 504532.493456\niter  90 value 502789.244226\niter 100 value 502772.289562\nfinal  value 502772.144039 \nconverged\n# weights:  98 (78 variable)\ninitial  value 746161.192564 \niter  10 value 583318.098071\niter  20 value 565452.660017\niter  30 value 553004.511199\niter  40 value 535661.314128\niter  50 value 533835.120716\niter  60 value 526651.621249\niter  70 value 512879.814523\niter  80 value 505125.732253\niter  90 value 503780.650158\niter 100 value 503768.498849\niter 100 value 503768.495637\niter 100 value 503768.495615\nfinal  value 503768.495615 \nconverged\n# weights:  98 (78 variable)\ninitial  value 746803.342913 \niter  10 value 583690.720571\niter  20 value 566306.979616\niter  30 value 551774.114094\niter  40 value 537259.662266\niter  50 value 535113.563551\niter  60 value 526452.935962\niter  70 value 512964.056547\niter  80 value 504964.169296\niter  90 value 503268.251731\niter 100 value 503249.111787\nfinal  value 503248.815177 \nconverged\n# weights:  98 (78 variable)\ninitial  value 747715.974773 \niter  10 value 583786.077390\niter  20 value 567105.715518\niter  30 value 552434.743020\niter  40 value 537595.742939\niter  50 value 535193.954596\niter  60 value 521167.329143\niter  70 value 510112.965931\niter  80 value 506456.019829\niter  90 value 505635.711866\niter 100 value 505619.618820\nfinal  value 505619.547336 \nconverged\n# weights:  98 (78 variable)\ninitial  value 746970.691186 \niter  10 value 584955.081885\niter  20 value 567314.200251\niter  30 value 550787.724286\niter  40 value 535225.530029\niter  50 value 532916.786298\niter  60 value 527702.038408\niter  70 value 510892.228628\niter  80 value 505225.792024\niter  90 value 503907.572475\niter 100 value 503898.506729\nfinal  value 503898.421803 \nconverged\n# weights:  98 (78 variable)\ninitial  value 747488.303286 \niter  10 value 583119.603702\niter  20 value 566841.648280\niter  30 value 551376.979315\niter  40 value 535103.066727\niter  50 value 532514.969609\niter  60 value 520124.869874\niter  70 value 509228.087974\niter  80 value 505351.542571\niter  90 value 504685.805585\niter 100 value 504672.226391\nfinal  value 504672.212695 \nconverged\n# weights:  98 (78 variable)\ninitial  value 743503.079301 \niter  10 value 580780.250261\niter  20 value 563224.055560\niter  30 value 548344.717654\niter  40 value 535963.961916\niter  50 value 533709.672202\niter  60 value 514506.546758\niter  70 value 506529.898117\niter  80 value 503369.501382\niter  90 value 502499.936997\niter 100 value 502491.333787\nfinal  value 502491.323111 \nconverged\n# weights:  98 (78 variable)\ninitial  value 744149.121470 \niter  10 value 584360.429636\niter  20 value 565852.095368\niter  30 value 554113.251762\niter  40 value 541601.089529\niter  50 value 539204.094419\niter  60 value 520343.899251\niter  70 value 510733.738542\niter  80 value 503381.508098\niter  90 value 501755.600539\niter 100 value 501730.994412\nfinal  value 501730.752384 \nconverged\n```\n\n\n:::\n:::\n\n\n\n\n## Leave-one-season-out calibration\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nep_cv_calibration |> \n  pivot_longer(No_Score:Touchdown,\n               names_to = \"event\",\n               values_to = \"pred_prob\") |>\n  mutate(bin_pred_prob = round(pred_prob / 0.05) * 0.05) |> \n  group_by(event, bin_pred_prob) |> \n  summarize(n_plays = n(), \n            n_events = length(which(next_score_half == event)),\n            bin_actual_prob = n_events / n_plays,\n            bin_se = sqrt((bin_actual_prob * (1 - bin_actual_prob)) / n_plays)) |>\n  ungroup() |> \n  mutate(bin_upper = pmin(bin_actual_prob + 2 * bin_se, 1),\n         bin_lower = pmax(bin_actual_prob - 2 * bin_se, 0)) |> \n  mutate(event = fct_relevel(event, \"Opp_Safety\", \"Opp_Field_Goal\", \"Opp_Touchdown\", \n                             \"No_Score\", \"Safety\", \"Field_Goal\", \"Touchdown\")) |> \n  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_point() +\n  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper)) +\n  expand_limits(x = c(0, 1), y = c(0, 1)) +\n  facet_wrap(~ event, ncol = 4)\n```\n:::\n\n\n\n\n## Leave-one-season-out calibration\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-multinomial_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=1920}\n:::\n:::\n\n\n\n\n\n## Player and team evaluations\n\n*   Expected points added (EPA): $EP_{end} - EP_{start}$\n\n*   Other common measures\n\n    *   Total EPA (for both offense and defense)\n    \n    *   EPA per play\n    \n    *   Success rate: fraction of plays with positive EPA\n    \n    \n## Multinomial classification with XGBoost\n\nNote: XGBoost requires the multinomial categories to be numeric starting at 0\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(xgboost)  \nnfl_pbp <- read_csv(\"https://github.com/36-SURE/2024/raw/main/data/nfl_pbp.csv.gz\") |> \n  mutate(next_score_half = fct_relevel(next_score_half, \n                                       \"No_Score\", \"Safety\", \"Field_Goal\", \"Touchdown\",\n                                       \"Opp_Safety\", \"Opp_Field_Goal\", \"Opp_Touchdown\"),\n         next_score_label = as.numeric(next_score_half) - 1) \n```\n:::\n\n\n\n\n## Leave-one-season-out cross-validation\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nep_xg_cv <- function(s) {\n  test <- nfl_pbp |> filter(season == s)\n  train <- nfl_pbp |> filter(season != s)\n  x_test <- test |> select(half_seconds_remaining, yardline_100, down, ydstogo) |> as.matrix()\n  x_train <- train |> select(half_seconds_remaining, yardline_100, down, ydstogo) |> as.matrix()\n  ep_xg <- xgboost(data = x_train, label = train$next_score_label, \n                   nrounds = 100, max_depth = 3, eta = 0.3, gamma = 0, \n                   colsample_bytree = 1, min_child_weight = 1, subsample = 1, nthread = 1, \n                   objective = 'multi:softprob', num_class = 7, eval_metric = 'mlogloss', verbose = 0)\n  ep_xg_pred <- ep_xg |> \n      predict(x_test) |> \n      matrix(ncol = 7, byrow = TRUE) |> \n      as_tibble()\n  colnames(ep_xg_pred) <- c(\"No_Score\", \"Safety\", \"Field_Goal\", \"Touchdown\", \n                            \"Opp_Safety\", \"Opp_Field_Goal\", \"Opp_Touchdown\")\n  ep_xg_pred <- ep_xg_pred |> \n    mutate(next_score_half = test$next_score_half, \n           season = s)\n  return(ep_xg_pred)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nseasons <- unique(nfl_pbp$season)\nep_xg_cv_pred <- seasons |> \n  map(ep_xg_cv) |> \n  bind_rows()\n```\n:::\n\n\n\n\n## Model calibration\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nep_xg_cv_pred |>\n  pivot_longer(No_Score:Opp_Touchdown,\n               names_to = \"event\",\n               values_to = \"pred_prob\") |>\n  mutate(bin_pred_prob = round(pred_prob / 0.05) * 0.05) |>\n  group_by(event, bin_pred_prob) |>\n  summarize(n_plays = n(),\n            n_events = length(which(next_score_half == event)),\n            bin_actual_prob = n_events / n_plays,\n            bin_se = sqrt((bin_actual_prob * (1 - bin_actual_prob)) / n_plays)) |>\n  ungroup() |>\n  mutate(bin_upper = pmin(bin_actual_prob + 2 * bin_se, 1),\n         bin_lower = pmax(bin_actual_prob - 2 * bin_se, 0)) |>\n  mutate(event = fct_relevel(event, \"Opp_Safety\", \"Opp_Field_Goal\", \"Opp_Touchdown\",\n                             \"No_Score\", \"Safety\", \"Field_Goal\", \"Touchdown\")) |>\n  ggplot(aes(x = bin_pred_prob, y = bin_actual_prob)) +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_point() +\n  geom_errorbar(aes(ymin = bin_lower, ymax = bin_upper)) +\n  expand_limits(x = c(0, 1), y = c(0, 1)) +\n  facet_wrap(~ event, ncol = 4)\n```\n:::\n\n\n\n\n## Model calibration\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-multinomial_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=1920}\n:::\n:::\n",
    "supporting": [
      "21-multinomial_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}