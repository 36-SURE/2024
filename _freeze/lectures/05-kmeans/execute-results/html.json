{
  "hash": "31f62924582f2fdfece09978eddc1fcb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Unsupervised learning: $k$-means clustering\"\nauthor: \"<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University\"\nfooter:  \"[36-SURE.github.io/2024](https://36-sure.github.io/2024)\"\nformat:\n  revealjs:\n    theme: theme.scss\n    chalkboard: true\n    smaller: true\n    slide-number: c/t\n    code-line-numbers: false\n    linestretch: 1.25\n    html-math-method:\n      method: mathjax\n      url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\n---\n\n\n\n\n\n\n# Background\n\n## Statistical learning\n\n> Statistical learning refers to a set of tools for making sense of complex\ndatasets. --- [Preface of ISLR](https://www.statlearning.com/)\n\n. . .\n\nGeneral setup: Given a dataset of $p$ variables (columns) and $n$ observations (rows) $x_1,\\dots,x_n$.\n\nFor observation $i$, $$x_{i1},x_{i2},\\ldots,x_{ip} \\sim P \\,,$$ where $P$ is a $p$-dimensional distribution that we might not know much about *a priori*\n\n## Statistical learning vs. machine learning???\n\n* Plenty of overlap - both fields focus on supervised and unsupervised problems\n\n  *   Machine learning has a greater emphasis on large scale applications and prediction accuracy\n  *   Statistical learning emphasizes models and their interpretability, and the assessment of uncertainty\n\n* The distinction has become more and more blurred, and there is a great deal of \"cross-fertilization\"\n\n* \"Statistical machine learning\"???\n\n<!-- ??? -->\n\n<!-- - Statistical learning is the process of ascertaining (discovering) associations between groups of variables -->\n\n<!-- - unsupervised learning - where the goal is to discover interesting things about the data -->\n\n## Supervised learning\n\n* Response variable $Y$ in one of the $p$ variables (columns)\n\n* The remaining $p-1$ variables are predictor measurements $X$\n\n  *   Regression: $Y$ is quantitative\n  \n  *   Classification: $Y$ is categorical\n  \n. . .\n\nObjective: Given training data $(x_1, y_1), ..., (x_n, y_n)$\n\n*   Accurately predict unseen test cases\n\n*   Understand which features  affect the response (and how)\n\n*   Assess the quality of our predictions and inferences\n  \n## Unsupervised learning\n\n* No response variable (i.e. data are not labeled)\n\n* Only given a set of features measured on a set of observations\n\n* Objective: understand the variation and grouping structure of a set of unlabeled data\n\n  *   e.g., discover subgroups among the variables or among the observations\n\n* Difficult to know how \"good\" you are doing\n\n## Unsupervised learning\n\n* It is often easier to obtain unlabeled data than labeled data\n\n* Unsupervised learning is more subjective than supervised learning\n\n* There is no simple goal for the analysis, such as prediction of a response\n\n* Unsupervised learning can be useful as a pre-processing step for supervised learning\n\nThink of unsupervised learning as **an extension of EDA** - **there's no unique right answer!**\n\n## Clustering (cluster analysis)\n\n. . .\n\n>  Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set --- ISLR\n\n. . .\n\n**Goals**: partition of the observations into distinct clusters so that\n\n-   observations **within** clusters are **more similar** to each other\n\n-   observations **in different** clusters are **more different** from each other\n\n. . .\n\n* This often involves domain-specific considerations based on knowledge of the data being studied\n\n\n## Distance between observations\n\n* What does it means for two or more observations to be similar or different?\n\n. . .\n\n* This require characterizing the __distance__ between observations\n\n  *   Clusters: groups of observations that are \"close\" together\n\n. . .\n\n*   This is easy to do for 2 quantitative variables: just make a scatterplot\n\n. . .\n\n**But how do we define \"distance\" beyond 2D data?**\n\n. . .\n\nLet $\\boldsymbol{x}_i = (x_{i1}, \\dots, x_{ip})$ be a vector of $p$ features for observation $i$\n\nQuestion of interest: How \"far away\" is $\\boldsymbol{x}_i$ from $\\boldsymbol{x}_j$?\n\n. . .\n\nWhen looking at a scatterplot, we're using __Euclidean distance__ $$d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\sqrt{(x_{i1} - x_{j1})^2 + \\dots + (x_{ip} - x_{jp})^2}$$\n\n---\n\n## Distances in general\n\n* There's a variety of different types of distance metrics: [Manhattan](https://en.wikipedia.org/wiki/Taxicab_geometry), [Mahalanobis](https://en.wikipedia.org/wiki/Mahalanobis_distance), [Cosine](https://en.wikipedia.org/wiki/Cosine_similarity), [Kullback-Leibler](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), [Hellinger](https://en.wikipedia.org/wiki/Hellinger_distance),\n[Wasserstein](https://en.wikipedia.org/wiki/Wasserstein_metric)\n\n* We're just going to focus on [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)\n\n. . .\n\n* Let $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$ denote the pairwise distance between two observations $i$ and $j$\n\n1. __Identity__: $\\boldsymbol{x}_i = \\boldsymbol{x}_j \\Leftrightarrow d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = 0$\n\n2. __Non-negativity__: $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\geq 0$\n\n3. __Symmetry__: $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = d(\\boldsymbol{x}_j, \\boldsymbol{x}_i)$\n\n4. __Triangle inequality__: $d(\\boldsymbol{x}_i, \\boldsymbol{x}_j) \\leq d(\\boldsymbol{x}_i, \\boldsymbol{x}_k) + d(\\boldsymbol{x}_k, \\boldsymbol{x}_j)$\n\n. . .\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n__Distance Matrix__: matrix $D$ of all pairwise distances\n\n- $D_{ij} = d(\\boldsymbol{x}_i, \\boldsymbol{x}_j)$\n\n- where $D_{ii} = 0$ and $D_{ij} = D_{ji}$\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n$$D = \\begin{pmatrix}\n                0 & D_{12} & \\cdots & D_{1n} \\\\\n                D_{21} & 0 & \\cdots & D_{2n} \\\\\n                \\vdots & \\vdots & \\ddots & \\vdots \\\\\n                D_{n1} & \\cdots & \\cdots & 0\n            \\end{pmatrix}$$\n\n:::\n:::\n\n---\n\n## Units matter in clustering\n\n-   Variables are typically measured in different units\n\n-   One variable may *dominate* others when computing Euclidean distance because its range is much larger\n\n-   Scaling of the variables matters!\n\n-   Standardize each variable in the dataset to have mean 0 and standard deviation 1 with `scale()`\n\n<!-- ??? -->\n\n<!-- It is the partitioning of data into homogeneous subgroups -->\n\n<!-- Goal define clusters for which the within-cluster variation is relatively small, i.e. observations within clusters are similar to each other -->\n\n# $k$-means clustering\n\n## $k$-means clustering\n\n-   Goal: partition the observations into a pre-specified number of clusters\n\n. . .\n\n-   Let $C_1, \\dots, C_K$ denote sets containing indices of observations in each of the $k$ clusters\n\n    -   if observation $i$ is in cluster $k$, then $i \\in C_k$\n\n. . .\n\n-   We want to minimize the **within-cluster variation** $W(C_k)$ for each cluster $C_k$ (i.e. the amount by which the observations within a cluster differ from each other)\n\n-   This is equivalent to solving $$\\underset{C_1, \\dots, C_K}{\\text{minimize }} \\Big\\{ \\sum_{k=1}^K W(C_k) \\Big\\}$$\n\n-   In other words, we want to partition the observations into $K$ clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible\n\n## $k$-means clustering\n\nHow do we define within-cluster variation?\n\n-   Use the **(squared) Euclidean distance** $$W(C_k) = \\frac{1}{|C_k|}\\sum_{i,j \\in C_k} d(x_i, x_j)^2 \\,,$$ where $|C_k|$ denote the number of observations in cluster $k$\n\n-   Commonly referred to as the within-cluster sum of squares (WSS)\n\n. . .\n\n**So how do we solve this?**\n\n## [Lloyd's algorithm](https://en.wikipedia.org/wiki/K-means_clustering)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n1)  Choose $k$ random centers, aka **centroids**\n\n2)  Assign each observation closest center (using Euclidean distance)\n\n3)  Repeat until cluster assignment stop changing:\n\n-   Compute new centroids as the averages of the updated groups\n\n-   Reassign each observations to closest center\n\n**Converges to a local optimum**, not the global\n\n**Results will change from run to run** (set the seed!)\n\n**Takes** $k$ as an input!\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif){fig-align='center' width=80%}\n:::\n:::\n\n\n\n:::\n:::\n\n## Gapminder data\n\nHealth and income outcomes for 184 countries from 1960 to 2016 from the famous [Gapminder project](https://www.gapminder.org/data)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_light())\nlibrary(dslabs)\nglimpse(gapminder)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10,545\nColumns: 9\n$ country          <fct> \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"…\n$ year             <int> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,…\n$ infant_mortality <dbl> 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.…\n$ life_expectancy  <dbl> 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8…\n$ fertility        <dbl> 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,…\n$ population       <dbl> 1636054, 11124892, 5270844, 54681, 20619075, 1867396,…\n$ gdp              <dbl> NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778…\n$ continent        <fct> Europe, Africa, Africa, Americas, Americas, Asia, Ame…\n$ region           <fct> Southern Europe, Northern Africa, Middle Africa, Cari…\n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n## GDP is severely skewed right...\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngapminder |> \n  ggplot(aes(x = gdp)) + \n  geom_histogram() \n```\n\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Some initial cleaning...\n\n-   Each row is at the `country`-`year` level\n\n-   Focus on data for 2011 where `gdp` is not missing\n\n-   Log-transform `gdp`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder <- gapminder |>\n  filter(year == 2011, !is.na(gdp)) |>\n  mutate(log_gdp = log(gdp))\n```\n:::\n\n\n\n\n## $k$-means clustering example (`gdp` and `life_expectancy`)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n-   Use the `kmeans()` function, **but must provide number of clusters** $k$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninit_kmeans <- clean_gapminder |> \n  select(log_gdp, life_expectancy) |> \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(init_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") \n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Careful with units...\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n-   Use `coord_fixed()` so that the axes match with unit scales\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(init_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Standardize the variables!\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n-   Use the `scale()` function to first **standardize the variables**, $\\frac{\\text{value} - \\text{mean}}{\\text{sd}}$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder <- clean_gapminder |>\n  mutate(\n    std_log_gdp = as.numeric(scale(log_gdp, center = TRUE, scale = TRUE)),\n    std_life_exp = as.numeric(scale(life_expectancy, center = TRUE, scale = TRUE))\n  )\n\nstd_kmeans <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(std_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Standardize the variables!\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(std_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = std_log_gdp, y = std_life_exp,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## And if we run it again?\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nWe get different clustering results!\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanother_kmeans <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 1)\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(another_kmeans$cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\n\n**Results depend on initialization**\n\nKeep in mind: **the labels / colors are arbitrary**\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Fix randomness issue with `nstart`\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nRun the algorithm `nstart` times, then **pick the results with lowest total within-cluster variation** $$\\text{total WSS} = \\sum_{k=1}^K W(C_k)$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnstart_kmeans <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kmeans(algorithm = \"Lloyd\", centers = 4, nstart = 30)\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(nstart_kmeans$cluster)\n  ) |> \n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## By default `R` uses [Hartigan–Wong method](https://en.wikipedia.org/wiki/K-means_clustering#Hartigan%E2%80%93Wong_method)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\nUpdates based on changing a single observation\n\n**Computational advantages over re-computing distances for every observation**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndefault_kmeans <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kmeans(algorithm = \"Hartigan-Wong\",\n         centers = 4, nstart = 30) \n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(default_kmeans$cluster)\n  ) |> \n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\n\nVery little differences for our purposes...\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## Better alternative to `nstart`: $k$-means++\n\nObjective: initialize the cluster centers before proceeding with the standard $k$-means clustering algorithm\n\n. . .\n\nIntuition: \n\n* randomly choose a data point the first cluster center\n\n* each subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point's closest existing cluster center\n\n## $k$-means++\n\nPick a random observation to be the center $c_1$ of the first cluster $C_1$\n\n-   This initializes a set $\\text{Centers} = \\{c_1 \\}$\n\n. . .\n\nThen for each remaining cluster $c^* \\in 2, \\dots, K$:\n\n-   For each observation (that is not a center), compute $D(x_i) = \\underset{c \\in \\text{Centers}}{\\text{min}} d(x_i, c)$\n\n    -   Distance between observation and its closest center $c \\in \\text{Centers}$\n\n. . .\n\n-   Randomly pick a point $x_i$ with probability: $\\displaystyle p_i = \\frac{D^2(x_i)}{\\sum_{j=1}^n D^2(x_j)}$\n\n. . .\n\n-   As distance to closest center increases, the probability of selection increases\n\n-   Call this randomly selected observation $c^*$, update $\\text{Centers} = \\text{Centers} \\cup c^*$\n\n. . .\n\nThen run $k$-means using these $\\text{Centers}$ as the starting points\n\n## $k$-means++ in `R` using [`flexclust`](https://cran.r-project.org/web/packages/flexclust/flexclust.pdf)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(flexclust)\ninit_kmeanspp <- clean_gapminder |> \n  select(std_log_gdp, std_life_exp) |> \n  kcca(k = 4, control = list(initcent = \"kmeanspp\"))\n\nclean_gapminder |>\n  mutate(\n    country_clusters = as.factor(init_kmeanspp@cluster)\n  ) |>\n  ggplot(aes(x = log_gdp, y = life_expectancy,\n             color = country_clusters)) +\n  geom_point(size = 4) + \n  ggthemes::scale_color_colorblind() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n\n\n**Note the use of `@` instead of `$`...**\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n:::\n:::\n\n## So, how do we choose the number of clusters?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://i.pinimg.com/originals/86/90/6c/86906c4cb23094b8bfb851031509b9f4.gif){fig-align='center' width=60%}\n:::\n:::\n\n\n\n\n## So, how do we choose the number of clusters?\n\n**There is no universally accepted way to conclude that a particular choice of $k$ is optimal!**\n\nFrom [Cosma Shalizi's notes](https://www.stat.cmu.edu/~cshalizi/350/lectures/08/lecture-08.pdf)\n\n> One reason you should be intensely skeptical of clustering results --- including your own! --- is that there is currently very little theory about how to find the right number of clusters. It’s not even completely clear what \"the right number of clusters\" means!\n\nAdditional readings: [here](https://www.stat.cmu.edu/~larry/=sml/clustering.pdf) and [here](https://www.stat.cmu.edu/~larry/=sml/Clustering2.pdf)\n\n## Popular heuristic: elbow plot (use with caution)\n\nLook at the total within-cluster variation as a function of the number of clusters\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# function to perform clustering for each value of k\ngapminder_kmeans <- function(k) {\n  \n  kmeans_results <- clean_gapminder |>\n    select(std_log_gdp, std_life_exp) |>\n    kmeans(centers = k, nstart = 30)\n  \n  kmeans_out <- tibble(\n    clusters = k,\n    total_wss = kmeans_results$tot.withinss\n  )\n  return(kmeans_out)\n}\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# number of clusters to search over\nn_clusters_search <- 2:12\n\n# iterate over each k to compute total wss\nkmeans_search <- n_clusters_search |> \n  map(gapminder_kmeans) |> \n  bind_rows()\n\nkmeans_search |> \n  ggplot(aes(x = clusters, y = total_wss)) +\n  geom_line() + \n  geom_point(size = 4) +\n  scale_x_continuous(breaks = n_clusters_search)\n```\n:::\n\n\n\n:::\n:::\n\n## Popular heuristic: elbow plot (use with caution)\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\nChoose $k$ where marginal improvements is low at the bend (hence the elbow)\n\n**This is just a guideline and should not dictate your choice of** $k$\n\n[Gap statistic](https://web.stanford.edu/~hastie/Papers/gap.pdf) is a popular choice (see [`clusGap` function](https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clusGap.html) in [`cluster` package](https://cran.r-project.org/web/packages/cluster/cluster.pdf))\n\nLater on: model-based approach to choosing the number of clusters\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n:::\n\n## Appendix: elbow plot with `flexclust`\n\n::: columns\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngapminder_kmpp <- function(k) {\n  \n  kmeans_results <- clean_gapminder |>\n    select(std_log_gdp, std_life_exp) |>\n    kcca(k = k, control = list(initcent = \"kmeanspp\"))\n  \n  kmeans_out <- tibble(\n    clusters = k,\n    total_wss = sum(kmeans_results@clusinfo$size * \n                      kmeans_results@clusinfo$av_dist)\n  )\n  return(kmeans_out)\n}\n\nn_clusters_search <- 2:12\nkmpp_search <- n_clusters_search |> \n  map(gapminder_kmpp) |> \n  bind_rows()\nkmpp_search |> \n  ggplot(aes(x = clusters, y = total_wss)) +\n  geom_line() + \n  geom_point(size = 4) +\n  scale_x_continuous(breaks = n_clusters_search)\n```\n:::\n\n\n\n:::\n\n::: {.column width=\"50%\" style=\"text-align: left;\"}\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-28-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n:::\n:::\n\n## Appendix: $k$-means for image segmentation and compression\n\nGoal: partition an image into multiple segments, where each segment typically represents an object in the image\n\n* Treat each pixel in the image as a point in 3-dimensional space comprising the intensities of the (red, blue, green) channels\n\n* Treat each pixel in the image as a separate data point\n\n*  Apply $k$-means clustering and identify the clusters\n\n*  All the pixels belonging to a cluster are treated as a segment in the image\n\nThen for any $k$, reconstruct the image by replacing each pixel vector with the (red, blue, green) triplet given by the center to which that pixel has been assigned\n\n## Appendix: $k$-means for image segmentation and compression\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# https://raw.githubusercontent.com/36-SURE/2024/main/data/spongebob.jpeg\nset.seed(2)\nlibrary(jpeg)\nimg_raw <- readJPEG(\"../data/spongebob.jpeg\")\nimg_width <- dim(img_raw)[1]\nimg_height <- dim(img_raw)[2]\nimg_tbl <- tibble(x = rep(1:img_height, each = img_width),\n                  y = rep(img_width:1, img_height),\n                  r = as.vector(img_raw[, , 1]),\n                  g = as.vector(img_raw[, , 2]),\n                  b = as.vector(img_raw[, , 3]))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimg_kmpp <- function(k) {\n  km_fit <- img_tbl |>\n    select(r, g, b) |>\n    kcca(k = k, control = list(initcent = \"kmeanspp\"))\n  km_col <- rgb(km_fit@centers[km_fit@cluster,])\n  return(km_col)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimg_tbl <- img_tbl |>\n  mutate(original = rgb(r, g, b),\n         k2 = img_kmpp(2),\n         k4 = img_kmpp(4),\n         k8 = img_kmpp(8),\n         k11 = img_kmpp(11)) |>\n  pivot_longer(original:k11, names_to = \"k\", values_to = \"hex\") |>\n  mutate(k = factor(k, levels = c(\"original\", \"k2\", \"k4\", \"k8\", \"k11\")))\n```\n:::\n\n\n\n\n## Appendix: $k$-means for image segmentation and compression\n\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nimg_tbl |> \n  ggplot(aes(x, y, color = hex)) +\n  geom_point() +\n  scale_color_identity() +\n  facet_wrap(~ k, nrow = 1) +\n  coord_fixed() +\n  theme_classic()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](05-kmeans_files/figure-revealjs/unnamed-chunk-33-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n\n\n<!-- ::: columns -->\n\n<!-- ::: {.column width=\"50%\" style=\"text-align: left;\"} -->\n\n<!-- c1 -->\n\n<!-- ::: -->\n\n<!-- ::: {.column width=\"50%\" style=\"text-align: left;\"} -->\n\n<!-- c2 -->\n\n<!-- ::: -->\n\n<!-- ::: -->\n",
    "supporting": [
      "05-kmeans_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}