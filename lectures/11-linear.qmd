---
title: "Supervised learning: linear regression"
author: "<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University"
footer:  "[36-SURE.github.io](https://36-sure.github.io)"
format:
  revealjs:
    theme: theme.scss
    chalkboard: true
    smaller: true
    slide-number: c/t
    code-line-numbers: false
    linestretch: 1.25
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---

```{r}
#| include: false
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.height = 9
)
```

# Background

## Resources

*   [Advanced Data Analysis from an Elementary Point of View](https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf) (aka the greatest book that will never be published)

*   [The Truth about Linear Regression](https://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf)

> "Regression", in statistical jargon, is the problem of guessing the average level of some quantitative response variable from various predictor variables. - [Note GOAT](http://bactra.org/notebooks/regression.html)

## Simple linear regression

**Linear regression is used when the response variable is quantitative**

. . .

Assume a __linear relationship__ for $Y = f(X)$ $$Y_{i}=\beta_{0}+\beta_{1} X_{i}+\epsilon_{i}, \quad \text { for } i=1,2, \dots, n$$

- $Y_i$ is the $i$th value for the __response__ variable
  
- $X_i$ is the $i$th value for the __predictor__ variable

. . .
  
- $\beta_0$ is an _unknown_, constant __intercept__

  -   average value for $Y$ if $X = 0$ (be careful sometimes...)
  
- $\beta_1$ is an _unknown_, constant __slope__

  -   change in average value for $Y$ for each one-unit increase in $X$
  
. . .
  
- $\epsilon_i$ is the __random__ noise

    -   assume __independent, identically distributed__ (_iid_) from a normal distribution 

    -   $\epsilon_i \overset{iid}{\sim}N(0, \sigma^2)$ with constant variance $\sigma^2$



## Simple linear regression estimation

We are estimating the __conditional expection (mean)__ for $Y$:

$$\mathbb{E}[Y_i \mid X_i] = \beta_0 + \beta_1X_i$$

- average value for $Y$ given the value for $X$

. . .

How do we estimate the __best fitted__ line?

. . .

__Ordinary least squares (OLS)__ - by minimizing the __residual sum of squares (RSS)__

$$\text{RSS} \left(\beta_{0}, \beta_{1}\right)=\sum_{i=1}^{n}\left[Y_{i}-\left(\beta_{0}+\beta_{1} X_{i}\right)\right]^{2}=\sum_{i=1}^{n}\left(Y_{i}-\beta_{0}-\beta_{1} X_{i}\right)^{2}$$

. . .

$$\widehat{\beta}_{1}=\frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}} \quad \text{ and } \quad \widehat{\beta}_{0}=\bar{Y}-\widehat{\beta}_{1} \bar{X}$$

where $\displaystyle \bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$ and $\displaystyle \bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$



## Connection to covariance and correlation

[__Covariance__](https://en.wikipedia.org/wiki/Covariance) describes the __joint variability of two variables__ $$\text{Cov}(X, Y) = \sigma_{X,Y} = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])]$$

. . .

__Sample covariance__ (use $n - 1$ since the means are used and we want [__unbiased estimates__](https://lazyprogrammer.me/covariance-matrix-divide-by-n-or-n-1/)) $$\hat{\sigma}_{X,Y} = \frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)$$

## Connection to covariance and correlation

__Correlation__ is a _normalized_ form of covariance, ranges from -1 to 1 $$\rho_{X,Y} = \frac{\text{Cov}(X,Y)}{\sigma_X \cdot \sigma_Y}$$

__Sample correlation__ uses the sample covariance and standard deviations, e.g. $\displaystyle s_X^2 = \frac{1}{n-1} \sum_i (X_i - \bar{X})^2$ $$r_{X,Y} = \frac{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)\left(Y_{i}-\bar{Y}\right)}{\sqrt{\sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}}}$$



## Connection to covariance and correlation

We have $$\widehat{\beta}_{1}=\frac{\sum_{i=1}^{n}(X_{i}-\bar{X})(Y_{i}-\bar{Y})}{\sum_{i=1}^{n}(X_{i}-\bar{X})^{2}} \quad \text{ and } \quad r_{X,Y} = \frac{\sum_{i=1}^{n}(X_{i}-\bar{X})(Y_{i}-\bar{Y})}{\sqrt{\sum_{i=1}^{n}(X_{i}-\bar{X})^{2} \sum_{i=1}^{n}(Y_{i}-\bar{Y})^{2}}}$$

. . .

We can rewrite $\hat{\beta}_1$ as $$\widehat{\beta}_{1} = r_{X,Y} \cdot \frac{s_Y}{s_X}$$

We can rewrite $\displaystyle r_{X,Y}$ as $$r_{X,Y} = \widehat{\beta}_{1} \cdot \frac{s_X}{s_Y}$$

. . .

Can think of $\widehat{\beta}_{1}$ weighting the ratio of variance between $X$ and $Y$...


# Linear regression in `R`

## Gapminder data

Health and income outcomes for 184 countries from 1960 to 2016 from the famous [Gapminder project](https://www.gapminder.org/data)

```{r load-data, warning = FALSE, message = FALSE}
library(tidyverse)
theme_set(theme_light())
library(dslabs)
clean_gapminder <- gapminder |>
  filter(year == 2011, !is.na(gdp)) |>
  mutate(log_gdp = log(gdp))
glimpse(clean_gapminder)
```

```{r}
#| echo: false
ggplot2::theme_set(ggplot2::theme_light(base_size = 20))
```


## Modeling life expectancy

::: columns
::: {.column width="50%" style="text-align: left;"}

Interested in modeling a country's __life expectancy__

```{r life-exp, echo = TRUE, eval = FALSE}
clean_gapminder |>
  ggplot(aes(x = life_expectancy)) +
  geom_histogram(color = "black", fill = "gray")
```



:::

::: {.column width="50%" style="text-align: left;"}

```{r ref.label = 'life-exp', echo = FALSE}

```

:::
:::



## Relationship between life expectancy and log GDP

::: columns
::: {.column width="50%" style="text-align: left;"}

```{r life-gdp-diff-plot, eval = FALSE}
gdp_plot <- clean_gapminder |>
  ggplot(aes(x = log_gdp, y = life_expectancy)) +
  geom_point(size = 3, alpha = 0.5)
gdp_plot
```

We fit linear regression models using `lm()`, formula is input as: `response ~ predictor`

```{r life-exp-reg}
simple_lm <- lm(life_expectancy ~ log_gdp, 
                data = clean_gapminder) 
```

:::


::: {.column width="50%" style="text-align: left;"}

```{r ref.label = 'life-gdp-diff-plot', echo = FALSE}

```

:::
:::



## Summarize the model using  `summary()`

```{r summary-init-reg}
summary(simple_lm)
```

## Summarize the model using the [`broom`](https://cran.r-project.org/web/packages/broom/vignettes/broom.html) package

The 3 `broom` functions (Note: the output is always a tibble)

*   `tidy()`: coefficients table in a tidy format

*   `glance()`: produces summary metrics of a model fit

*   `augment()`: adds/"augments" columns to the original data (e.g., adding predictions)

Note: the output of `tidy()`, `augment()` and `glance()` are always a tibble.

. . .

```{r}
library(broom)
tidy(simple_lm)
glance(simple_lm)
```


## Inference with OLS

Intercept and coefficient estimates $$\quad \hat{\beta}_0 = 24.174 \quad \text{and} \quad \hat{\beta}_1 = 1.975$$

. . .

Estimates of uncertainty for $\beta$'s via __standard errors__ $$\quad \widehat{SE}(\hat{\beta}_0) = 5.758 \quad \text{and} \quad \widehat{SE}(\hat{\beta}_1) = 0.242$$

. . .

$t$-statistics:  `Estimates` / `Std. Error`, i.e., number of standard deviations from 0

  - $p$-values (i.e., `Pr(>|t|)`): estimated probability observing value as extreme as |`t value`| __given the null hypothesis__ $\beta_1 = 0$
  
  - $p$-value $< \alpha = 0.05$ (conventional threshold): __sufficient evidence to reject the null hypothesis that the coefficient is zero__

  - i.e., there is a __significant__ association between `life_expectancy` and `log_gdp`


## Coefficient of determination

* In linear regression, the square of the correlation coefficient happens to be exactly the coefficient of
determination

```{r example-cor}
cor(clean_gapminder$log_gdp, clean_gapminder$life_expectancy)
cor(clean_gapminder$log_gdp, clean_gapminder$life_expectancy) ^ 2
```

* $R^2$ estimates the __proportion of the variance__ of $Y$ explained by $X$ 

* More generally: variance of model predictions / variance of $Y$

```{r general-rsquared}
var(predict(simple_lm)) / var(clean_gapminder$life_expectancy) 
```


## Generating predictions

We can use the `predict()` or `fitted()` function to either get the fitted values of the regression:

```{r init-predict}
train_preds <- predict(simple_lm)
head(train_preds)
```

```{r}
head(fitted(simple_lm))
```

```{r}
head(simple_lm$fitted.values)
```

```{r}
simple_lm |> 
  pluck("fitted.values") |> 
  head()
```


## Predictions for new data

Or we can provide it `newdata` which __must contain the explanatory variables__:

::: columns
::: {.column width="50%" style="text-align: left;"}

```{r us-preds, eval = FALSE}
us_data <- clean_gapminder |> 
  filter(country == "United States")

new_us_data <- us_data |>
  select(country, gdp) |>
  slice(rep(1, 3)) |> 
  mutate(adj_factor = c(0.25, 0.5, 0.75),
         log_gdp = log(gdp * adj_factor))
new_us_data$pred_life_exp <- 
  predict(simple_lm, newdata = new_us_data) 
gdp_plot +
  geom_point(data = new_us_data,
             aes(x = log_gdp, y = pred_life_exp),
             color = "darkred", size = 3)
```

:::


::: {.column width="50%" style="text-align: left;"}

```{r ref.label='us-preds', echo = FALSE, fig.height=6,fig.width=6,fig.align="center"}

```


:::
:::



## Plot observed values against predictions

Useful diagnostic (for __any type of model__, not just linear regression!) 

::: columns
::: {.column width="50%" style="text-align: left;"}

```{r plot-pred-obs, eval = FALSE}
clean_gapminder |>
  mutate(pred_vals = predict(simple_lm)) |> 
  ggplot(aes(x = pred_vals, y = life_expectancy)) +
  geom_point(alpha = 0.5, size = 3) +
  geom_abline(slope = 1, intercept = 0, 
              linetype = "dashed",
              color = "red",
              linewidth = 2)
```

- "Perfect" model will follow __diagonal__

:::


::: {.column width="50%" style="text-align: left;"}

```{r ref.label='plot-pred-obs', echo = FALSE, fig.height=6,fig.width=6,fig.align="center"}

```

:::
:::



## Plot observed values against predictions

Augment the data with model output using the [`broom` package](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)

::: columns
::: {.column width="50%" style="text-align: left;"}

```{r plot-pred-obs-broom, eval = FALSE}
clean_gapminder <- simple_lm |> 
  augment(clean_gapminder) 
clean_gapminder |>
  ggplot(aes(x = .fitted, y = life_expectancy)) + 
  geom_point(alpha = 0.5, size = 3) +
  geom_abline(slope = 1, intercept = 0, color = "red", 
              linetype = "dashed", linewidth = 2)
```

- Adds various columns from model fit we can use in plotting for model diagnostics

:::

::: {.column width="50%" style="text-align: left;"}
```{r ref.label='plot-pred-obs-broom', echo = FALSE, fig.height=6,fig.width=6,fig.align="center"}

```

:::
:::

## Assessing assumptions of linear regression

Simple linear regression assumes $Y_i \overset{iid}{\sim} N(\beta_0 + \beta_1 X_i, \sigma^2)$

- If this is true, then $Y_i - \hat{Y}_i \overset{iid}{\sim} N(0, \sigma^2)$

. . .

Plot residuals against $\hat{Y}_i$, __residuals vs fit__ plot

- Used to assess linearity, any divergence from mean 0

- Used to assess equal variance, i.e., if $\sigma^2$ is homogenous across predictions/fits $\hat{Y}_i$

. . .

More difficult to assess the independence and fixed $X$ assumptions

- Make these assumptions based on subject-matter knowledge



## Plot residuals against predicted values

::: columns
::: {.column width="50%" style="text-align: left;"}

- Residuals = observed - predicted

- Interpretation of residuals in context?

- Conditional on the predicted values, the __residuals should have a mean of zero__

```{r plot-resid-fit-broom, eval = FALSE}
clean_gapminder |>
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_point(alpha = 0.5, size = 3) +
  geom_hline(yintercept = 0, linetype = "dashed", 
             color = "red", linewidth = 2) +
  # plot the residual mean
  geom_smooth(se = FALSE)
```

- Residuals __should NOT display any pattern__

- Two things to look for:

  -   Any trend around horizontal reference line?

  -   Equal vertical spread?

:::


::: {.column width="50%" style="text-align: left;"}

```{r ref.label='plot-resid-fit-broom', echo = FALSE, fig.height=6,fig.width=6,fig.align="center"}

```

:::
:::



## Multiple linear regression 

We can include as many variables as we want (assuming $n > p$)

$$Y=\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}+\epsilon$$

OLS estimates in matrix notation ( $\boldsymbol{X}$ is a $n \times p$ matrix):
  
$$\hat{\boldsymbol{\beta}} = (\boldsymbol{X} ^T \boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{Y}$$

. . .

Can just add more variables to the formula in `R`

```{r}
multiple_lm <- lm(life_expectancy ~ log_gdp + fertility, data = clean_gapminder)
```

- Use adjusted $R^2$  when including multiple variables $\displaystyle 1 - \frac{(1 - R^2)(n - 1)}{(n - p - 1)}$

  - Adjusts for the number of variables in the model $p$  
  
  - Adding more variables __will always increase__ the model's (unadjusted) $R^2$
  
READ THIS: [$R^2$ myths](https://www.stat.cmu.edu/~cshalizi/TALR/TALR.pdf#page=181) 

[Get your $R^2$ out of here](https://www.tiktok.com/@fatgreggy/video/7186499956058950958)


## What about the Normal distribution assumption?

* Model: $Y=\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}+\epsilon$

- $\epsilon_i$ is the __random__ noise: assume __independent and identically distributed__ (_iid_) from a Normal distribution $\epsilon_i \overset{iid}{\sim}N(0, \sigma^2)$ with constant variance $\sigma^2$

. . .

* __OLS doesn't care about this assumption__, it's just estimating coefficients!

. . .

* In order to perform inference, __we need to impose additional assumptions__

* By assuming $\epsilon_i \overset{iid}{\sim}N(0, \sigma^2)$, what we really mean is $Y \overset{iid}{\sim}N(\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}, \sigma^2)$

. . .

* So we're estimating the mean $\mu$ of this conditional distribution, but what about $\sigma^2$?
  
. . .

* [Unbiased estimate](https://bradleyboehmke.github.io/HOML/linear-regression.html#simple-linear-regression) $\displaystyle \hat{\sigma}^2 = \frac{\text{RSS}}{n - (p + 1)}$

* __Degrees of freedom__:  $n - (p + 1)$, data supplies us with $n$ "degrees of freedom" and we used up $p + 1$



## Check the assumptions about normality with [`ggfortify`](https://cran.r-project.org/web/packages/ggfortify/vignettes/plot_lm.html)

```{r}
#| echo: false
ggplot2::theme_set(ggplot2::theme_light(base_size = 18))
```

```{r more-resid-plots, fig.height=6,fig.width=15,fig.align="center"}
library(ggfortify)
autoplot(multiple_lm, ncol = 4)
```

- Standardized residuals = residuals `/ sd(`residuals`)` (see also `.std.resid` from `augment`)


## Regression confidence intervals vs prediction intervals

Do we want to predict the mean response for a particular value $x^*$ of the explanatory variable or do we want to predict the response for an individual case?

Example: 

*   if we would like to know the average price for **all** Tesla with 50 thousand miles, then we would use an interval for the **mean response**

*   if we want an interval to contain the price of **a particular** Tesla with 50 thousand miles, then we need the interval for **a single prediction**


## Regression confidence intervals

::: columns
::: {.column width="50%" style="text-align: left;"}

- `geom_smooth()` displays __confidence intervals__ for the regression line

```{r conf-int, eval = FALSE}
# predict(simple_lm, interval = "confidence")
lm_plot <- clean_gapminder |>
  ggplot(aes(x = log_gdp,
             y = life_expectancy)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") +
  theme_bw() +
  labs(x = "log(GDP)",
       y = "Life expectancy")
lm_plot
```

:::


::: {.column width="50%" style="text-align: left;"}

```{r, ref.label='conf-int', echo = FALSE, fig.height=5, fig.width=6,fig.align="center"}

```

:::
:::


---

## Regression confidence intervals

*   Interval estimate for the MEAN response at a given observation (i.e. for the predicted AVERAGE $\hat{\beta}_0 + \hat{\beta}_1 x^*$)

*   Based on standard errors for the estimated regression line at $x^*$ $$SE_{\text{line}}\left(x^{*}\right)=\hat{\sigma} \cdot \sqrt{\frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}}$$

*   Variation only comes from uncertainty in the parameter 

## Regression prediction intervals

*   interval estimate for an INDIVIDUAL response at a given (new) observation 

*   add the variance $\sigma^2$ of a __single predicted value__ $$SE_{\text{pred}}\left(x^{*}\right)=\hat{\sigma} \cdot \sqrt{1 + \frac{1}{n}+\frac{\left(x^{*}-\bar{x}\right)^{2}}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}}$$

*   variation comes from uncertainty in parameter estimates and error variance 

## Confidence intervals versus prediction intervals

Generate 95% intervals

::: columns
::: {.column width="50%" style="text-align: left;"}

```{r pred-interval, eval = FALSE}
# predict(simple_lm, interval = "prediction")
lm_plot +
  geom_ribbon(
    data = augment(simple_lm, interval = "prediction"),
    aes(ymin = .lower, ymax = .upper),
    color = "red", fill = NA
  )
```

*   The standard error for predicting an individual response will always be larger than for predicting a mean response

*   Prediction intervals will always be wider than confidence intervals

:::

::: {.column width="50%" style="text-align: left;"}

```{r, ref.label='pred-interval', echo = FALSE,  fig.height=6,fig.width=6,fig.align="center"}

```

:::
:::

## Appendix: Interpret a linear regression model with log transformations

* Log-transformed predictor: $Y = \beta_0 + \beta_1 \log(X)$

  *   A 1% increase in $X$ is associated with an average change of $\beta_1/100$ units in $Y$

* Log-transformed outcome: $\log(Y) = \beta_0 + \beta_1 X$

  *   A 1 unit increase in $X$ is associated with an average change of $(100 \times \beta) \%$ in Y

* Log-transformed predictor and outcome: $\log(Y) = \beta_0 + \beta_1 \log(X)$

  *   A 1% increase in $X$ is associated with an average change of $\beta_1 \%$ in Y
  
[Useful reading](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/transf/transf.pdf)
