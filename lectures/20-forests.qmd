---
title: "Supervised learning: random forests and boosting"
author: "<br>SURE 2024<br><br>Department of Statistics & Data Science<br>Carnegie Mellon University"
footer:  "[36-SURE.github.io](https://36-sure.github.io)"
format:
  revealjs:
    theme: theme.scss
    chalkboard: true
    smaller: true
    slide-number: c/t
    code-line-numbers: false
    linestretch: 1.25
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
---


```{r}
#| include: false
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.align = "center",
  fig.height = 9
)
library(tidyverse)
ggplot2::theme_set(ggplot2::theme_light(base_size = 20))
```

# Background

## Decision trees review

__Pros__

- Decision trees are __very easy to explain__ and more closely mirror human decision-making

- Easy to visualize and thus easy to interpret __without assuming a parametric form__

. . .

__Cons__

- High variance: small changes in the input data can result in very different tree structures

- Not often competitive in terms of predictive accuracy

. . .

We can combine multiple trees to improve accuracy (**ensemble methods**)

## Bagging

__Bootstrap aggregation__  (bagging) is a general approach for reducing the variance of a statistical learning method

. . .

- __Bootstrap__: take repeated samples **with replacement** from the training data

```{r out.width='40%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/images/bootstrap-scheme.png")
```

. . .

- __Aggregation__: combine the results from many trees together, each constructed with a different bootstrapped sample of the data


## Bagging algorithm

*   Construct $B$ trees (e.g., $B=1000$) using $B$ bootstrapped training sets, and average the resulting predictions

. . .

*   These trees are grown deep, and are not pruned (i.e. really overfitted)

. . .

*   Each individual tree has high variance, but low bias

. . .

*   Averaging these $B$ trees reduces the variance

## Bagging algorithm

To generate a prediction for a new observation:

- __Regression__: take the __average__ across the $B$ trees

. . .

- __Classification__: take the __majority vote__ (most commonly
occurring class) across the $B$ trees 

  -   (could also use probabilities instead...)

. . .

The tradeoff with using bagging 

*   improves prediction accuracy via __wisdom of the crowds__

*   but at the expense of interpretability (how do you read $B = 1000$ trees)

. . .

For model summary, use __variable importance__ and __partial dependence__

## Out-of-bag (OOB) error

*   In bagging, the trees are constructed via bootstrapped data (sampling with replacements)

. . .

*   Each sample is likely to have duplicate observations

    *   i.e. each tree is built only on a subset of the data
    
. . .
    
*   On average, each bagged tree makes use of around 2/3 of the observations

*   The remaining 1/3 not used to fit a given bagged tree are referred to as the out-of-bag observations

. . .

*   We can use the OOB samples to estimate predictive performance

## Random forests

TL;DR: Random forests are bagged trees except that we also choose random subsets of predictors for each tree

. . .

* Random forests provide an improvement over bagged trees with a small tweak that decorrelates the trees

  *   This reduces the variance when we average the trees
  
. . .
  
* As in bagging, we build a number of decision trees on bootstrapped training samples

. . .

* But we only consider a random subset of $m$ predictors (from the full set of $p$ predictors) at each potential split

  *   The split is allowed to use only one of those $m$ predictors

  *   This adds more randomness to make __each tree more independent of each other__ 
  
. . .

* $m$ is a tuning parameter --- typically use $m = p/3$ (regression) or $m = \sqrt{p}$ (classification)

## Random forests

See also: [this joke](https://www.reddit.com/r/datascience/comments/qhu09k/where_do_data_scientists_go_camping/)

<center>

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Thank you for never overfitting Random Forest <a href="https://t.co/fVerhCfSDo">pic.twitter.com/fVerhCfSDo</a></p>&mdash; Brian Burke (@bburkeESPN) <a href="https://twitter.com/bburkeESPN/status/1779712185360425249?ref_src=twsrc%5Etfw">April 15, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</center>

## Example: Tree frog embryo hatching data

See `?stacks::tree_frogs` for more details

Response: `latency`

Predictors: `treatment`, `reflex`, `t_o_d`, `age`

```{r load-data, warning = FALSE, message = FALSE}
# install.packages("stacks")
library(tidyverse)
theme_set(theme_light())
library(stacks)
frogs <- tree_frogs |> 
  filter(!is.na(latency)) |> 
  select(-clutch, -hatched)
glimpse(frogs)
```

```{r}
#| echo: false
ggplot2::theme_set(ggplot2::theme_light(base_size = 20))
```

## Example using [`ranger`](https://github.com/imbs-hl/ranger)

`ranger` package provides a fast implementation of random forests in `R`

Fitting a plain random forest model with no tuning

```{r}
library(ranger)
frogs_rf <- ranger(latency ~ treatment + reflex + t_o_d + age, 
                   num.trees = 500, importance = "impurity", data = frogs)
frogs_rf
```

## Variable importance

```{r fig.align='center', fig.height=6}
library(vip)
vip(frogs_rf)
```

## Partial depedence plot

```{r}
library(pdp)
frogs_rf |> 
  partial(pred.var = "age") |> 
  autoplot()
```


## Model evaluation

::: columns
::: {.column width="50%" style="text-align: left;"}

*   RMSE

```{r}
frogs |> 
  mutate(pred = frogs_rf$predictions) |> 
  summarize(rmse = sqrt(mean((latency - pred) ^ 2)))
```

*   Plot predictions versus original observed values

:::

::: {.column width="50%" style="text-align: left;"}

```{r}
frogs |>
  mutate(pred = frogs_rf$predictions) |>
  ggplot(aes(latency, pred)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed")
```

:::
:::

## Tuning random forests

*   Number of trees

    *   It is more efficient to just pick a large enough value (e.g., 1000 trees) instead of tuning this
    
    *   No risk of overfitting if we pick something too big
    
*   The important parameter to tune is `mtry` (number of predictors at each split)

    *   The default ($p/3$ (regression) or $\sqrt{p}$ (classification)) often does a good job

. . .

Check out Julia Slige's tutorials on using `tidymodels` to tune random forests

*   https://juliasilge.com/blog/sf-trees-random-tuning/

*   https://juliasilge.com/blog/water-sources/


## Boosting

**Bagging vs boosting**

Bagging (and random forests) is a parallel ensemble model

*   Create multiple copies of the original training data set using the bootstrap

*   Fit a separate decision tree to each copy

*   Combine all of the trees in order to create a single predictive model

*   Important takeway: each tree is built on a bootstrap data set, independent of the other trees

. . .

Boosting is a sequential ensemble model

*   Trees are grown sequentially

*   Each tree is grown using information from previously grown trees

## Boosting

Build ensemble models __sequentially__

. . .

- start with a __weak learner__, e.g. small decision tree with few splits


. . .

- each model in the sequence _slightly_ improves upon the predictions of the previous models __by focusing on the observations with the largest residuals__

```{r out.width='80%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/images/boosted-trees-process.png")
```

## Boosting algorithm

Write the prediction at step $t$ of the search as $\hat{y}_i^{(t)}$, start with $\hat{y}_i^{(0)} = 0$

- Fit the first decision tree $f_1$ to the data: $\hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)$


. . .

- Fit the next tree $f_2$ to the residuals of the previous: $y_i - \hat{y}_i^{(1)}$

- Add this to the prediction: $\hat{y}_i^{(2)} = \hat{y}_i^{(1)} + f_2(x_i) = f_1(x_i) + f_2(x_i)$


. . .

- Fit the next tree $f_3$ to the residuals of the previous: $y_i - \hat{y}_i^{(2)}$

- Add this to the prediction:  $\hat{y}_i^{(3)} = \hat{y}_i^{(2)} + f_3(x_i) = f_1(x_i) + f_2(x_i) + f_3(x_i)$


. . .

__Continue until some stopping criteria__ to reach final model as a __sum of trees__:

$$\hat{y_i} = f(x_i) = \sum_{b=1}^B f_b(x_i)$$

## Intuition behind boosting

*   Instead of fitting a single large decision tree, which could result in overfitting, boosting learns slowly

*   Given the current model, we fit a decision tree to the residuals from the model

*   We then add this new decision tree into the fitted function in order to update the residuals

*   Each of these trees can be rather small, with just a few terminal nodes (this is a tuning parameter)

*   The goal of fitting small trees to the residuals is to slowly improve the model fit in areas where it does not perform well

*   Incorporate a shrinkage parameter (i.e. learning rate) which slows the process down, which reduces the risk of overfitting but increases training time

## Visual example of boosting in action

```{r out.width='80%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/boosting-in-action-1.png")
```


## Gradient boosted trees

* Boosting can be generalized to other loss functions via __gradient descent__

* Entering gradient boosted trees, or __gradient boosting machines (GBMs)__

* Update the model parameters in the direction of the loss function's descending gradient 

```{r out.width='60%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/gradient-descent-fig-1.png")
```


## Tune the learning rate in gradient descent

We need to control how much we update by in each step - __the learning rate__

```{r out.width='100%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/learning-rate-fig-1.png")
```


## Stochastic gradient descent can help with complex loss functions

* Take random samples of the data when updating

* Speed up algorithm while adding randomness to get closer to global minimum (no guarantees!)

```{r out.width='100%', echo = FALSE, fig.align='center'}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/10-gradient-boosting_files/figure-html/stochastic-gradient-descent-fig-1.png")
```




## Extreme gradient boosting with [XGBoost](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html)

__XGBoost__ (eXtreme Gradient Boosting) is an efficient, flexible, and portable boosting library

In `R`: the [`xgboost`](https://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html) package

. . .

XGBoost hyperparameters for tuning

- Number of trees $B$ (`nrounds`)

- Learning rate (`eta`), i.e. how much we update in each step

  - Note: these two really have to be tuned together

. . .

- Complexity of the trees (depth, number of observations in nodes)

. . .

- Regularization (`gamma`) and early stopping

## Be responsible!

- XGBoost is more flexible, but requires more work to tune properly as compared to random forests

. . .

- Remember...

<center>

{{< video images/power-responsibility.mp4 width="650" height="400">}}

</center>


## XGBoost example: MLB home run probability

*   Data splitting

```{r}
batted_balls <- read_csv("https://raw.githubusercontent.com/36-SURE/36-SURE.github.io/main/data/batted_balls.csv")
set.seed(123)
train <- batted_balls |> 
  slice_sample(prop = 0.5)
test <- batted_balls |> 
  anti_join(train)
x_train <- train |> 
  select(-is_hr) |> 
  as.matrix()
x_test <- test |> 
  select(-is_hr) |> 
  as.matrix()
```

## Tuning XGBoost

```{r}
library(xgboost)
library(caret)

# create hyperparameter grid
xg_grid <- crossing(nrounds = seq(20, 150, 10),
                    eta = c(0.01, 0.05, 0.1), gamma = 0,
                    max_depth = c(2, 3, 4), colsample_bytree = 1,
                    min_child_weight = 1, subsample = 1)


# tuning
set.seed(1234)
xg_tune <- train(x = x_train,
                 y = train$is_hr, 
                 tuneGrid = xg_grid,
                 trControl = trainControl(method = "cv", number = 5),
                 objective = "binary:logistic", 
                 method = "xgbTree")
```

## Model evaluation

```{r}
# fit final model to training data
xg_fit <- xgboost(data = x_train,
                  label = train$is_hr,
                  objective = "binary:logistic",
                  nrounds = xg_tune$bestTune$nrounds,
                  params = as.list(select(xg_tune$bestTune, -nrounds)),
                  verbose = 0)
```

```{r}
train |> 
  mutate(pred = round(predict(xg_fit, newdata = x_train))) |> 
  summarize(correct = mean(is_hr == pred))

test |> 
  mutate(pred = round(predict(xg_fit, newdata = x_test))) |> 
  summarize(correct = mean(is_hr == pred))
```


## Variable importance

```{r}
library(vip)
xg_fit |> 
  vip()
```

## Partial dependence plot

Partial dependence of home run outcome on launch speed and launch angle

::: columns
::: {.column width="50%" style="text-align: left;"}

```{r}
library(pdp)
xg_fit |> 
  partial(train = x_train,
          pred.var = "launch_speed", 
          which.class = 1, 
          prob = TRUE) |> 
  autoplot()
```

:::

::: {.column width="50%" style="text-align: left;"}

```{r}
xg_fit |> 
  partial(train = x_train,
          pred.var = "launch_angle", 
          which.class = 1, 
          prob = TRUE) |> 
  autoplot()
```

:::
:::


## See also

*   [lightgbm](https://lightgbm.readthedocs.io/en/stable/)

*   [catboost](https://catboost.ai/en/docs/) (Quang's favorite)

*   ["The Ladder"](https://arxiv.org/abs/1502.04585)
